From jtmelton at gmail.com  Sun Jun  5 03:40:13 2016
From: jtmelton at gmail.com (John Melton)
Date: Sat, 4 Jun 2016 23:40:13 -0400
Subject: [Owasp-appsensor-project] AppSensor GSoC ML Weekly Summary
Message-ID: <CAM8z=y+wHtmBrLMYYEXq35NC4pG_n-+qWzF6u-_eN8p3WW40Zw@mail.gmail.com>

All,

I wanted to send out the initial update for GSoC. As I mentioned in another
thread, we'll have a bit more traffic on this list over the summer do to
frequent updates and discussions. If you have an interest, please jump in!

As you may or may not be aware, we have a student (Tim) who is working on
using machine learning (ML) to further the analysis possible in AppSensor.
Going forward, Tim will be sending out this weekly update to the list to
make sure everyone is aware of what's happening.

If you have any questions or comments, please reach out!

------------------------------------

*Report Date:* 2016/05/31

*Accomplishments in the past week:* Integrate Kafka and Spark Streaming,
converting data to Vectors and encountered some problems here, still
debugging.

*Goals for the upcoming week:* Continue work on trial algorithm run on
sample log files, Decide on what specific analysis should be used done for
complex and simple analysis.

*Blockers :* Blocked for now in the training step for k-means, Problem with
missing dependencies in Kafka when installed using brew in Mac OS.

*Adjustments to project plan, if any:* Allocated more time for
implementation of simple and complex analysis and swapped around the two
stretch goals in the plan, per discussion with mentors.

*Other:* None

------------------------------------

Thanks,
John
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.owasp.org/pipermail/owasp-appsensor-project/attachments/20160604/14d18cd1/attachment.html>

From timothy22000 at gmail.com  Wed Jun  8 16:39:11 2016
From: timothy22000 at gmail.com (Timothy Sum Hon Mun)
Date: Wed, 8 Jun 2016 17:39:11 +0100
Subject: [Owasp-appsensor-project] AppSensor GSoC ML Weekly Summary
Message-ID: <CAPQUyzw2Em0BPm3PFw0x6gP3GMJyzCEM-ww6BKAAgqbp_E0qzQ@mail.gmail.com>

Hi All,

I am Tim, the student working with AppSensor for this year's GSoC and have
been working closely with mentors on the project. Here is my weekly update
on my progress so far.

If you have any ideas, questions or comments, please do join in!

------------------------------------

Report 7/06/2016

Date:  7 June 2016

Accomplishments in the past week: Improve Kafka and Spark integration as
well as learned how to run it within IntelliJ, Found out problem with Kafka
Connect in brew, Created custom JSON2CSV converter to pre-process file
before converting it to Vectors for Spark MLib rather than dumping whole
json string,

Goals for the upcoming week: Use the JSON2CSV converter with Map Reduce in
Spark to be able to handle streaming JSON or use Spark SQL to generate
tables for this purpose, Run clustering algorithm with different settings
on the extracted features, Use interesting features from the result of
clustering and try to use at least one classification algorithm to do
analysis on the dataset.

Blockers (things preventing forward progress):

Adjustments to project plan, if any:


Other: (use this space to document anything you learned, what worked well,
what didn't work well, etc.):

   -

   Close kafka servers before closing zookeeper. Otherwise, it will remain
   open and occupy a port.
   -

   Brew install kafka for Mac - only installs and gradle builds core kafka
   - missing components like connect
   -

   Source download for Kafka - Need to run gradle then ./gradlew to build
   jars required to run kafka (brew does this step automatically but other
   components are missing).
   -

   Ensure any running application using the same consumer group is closed,
   otherwise it may seem like the consumer code in the app is not working but
   its actually being processed already (Can look at Zookeeper logs)
   -

   Can install a Spark plugin to run Spark apps in IntelliJ or Eclipse
   (will document in github repo).
   - Convert JSON coming from Kafka using custom JSON2CSV converter or
   Spark SQL to be able to use Word2Vec feature extraction in Spark.


------------------------------------

Kind Regards,
Tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.owasp.org/pipermail/owasp-appsensor-project/attachments/20160608/fd5a60ae/attachment.html>

From colin.watson at owasp.org  Thu Jun  9 12:01:19 2016
From: colin.watson at owasp.org (Colin Watson)
Date: Thu, 9 Jun 2016 13:01:19 +0100
Subject: [Owasp-appsensor-project] AppSensor GSoC ML Weekly Summary
In-Reply-To: <CAPQUyzw2Em0BPm3PFw0x6gP3GMJyzCEM-ww6BKAAgqbp_E0qzQ@mail.gmail.com>
References: <CAPQUyzw2Em0BPm3PFw0x6gP3GMJyzCEM-ww6BKAAgqbp_E0qzQ@mail.gmail.com>
Message-ID: <CAAxdBBkvMMjmvisXhmcUZ2vAiNUZd-P2XM1J3gSPDXQC31hChw@mail.gmail.com>

Welcome Tim. It all sounds great.

Regards

Colin



On 8 June 2016 at 17:39, Timothy Sum Hon Mun <timothy22000 at gmail.com> wrote:

> Hi All,
>
> I am Tim, the student working with AppSensor for this year's GSoC and have
> been working closely with mentors on the project. Here is my weekly update
> on my progress so far.
>
> If you have any ideas, questions or comments, please do join in!
>
> ------------------------------------
>
> Report 7/06/2016
>
> Date:  7 June 2016
>
> Accomplishments in the past week: Improve Kafka and Spark integration as
> well as learned how to run it within IntelliJ, Found out problem with Kafka
> Connect in brew, Created custom JSON2CSV converter to pre-process file
> before converting it to Vectors for Spark MLib rather than dumping whole
> json string,
>
> Goals for the upcoming week: Use the JSON2CSV converter with Map Reduce in
> Spark to be able to handle streaming JSON or use Spark SQL to generate
> tables for this purpose, Run clustering algorithm with different settings
> on the extracted features, Use interesting features from the result of
> clustering and try to use at least one classification algorithm to do
> analysis on the dataset.
>
> Blockers (things preventing forward progress):
>
> Adjustments to project plan, if any:
>
>
> Other: (use this space to document anything you learned, what worked well,
> what didn't work well, etc.):
>
>    -
>
>    Close kafka servers before closing zookeeper. Otherwise, it will
>    remain open and occupy a port.
>    -
>
>    Brew install kafka for Mac - only installs and gradle builds core
>    kafka - missing components like connect
>    -
>
>    Source download for Kafka - Need to run gradle then ./gradlew to build
>    jars required to run kafka (brew does this step automatically but other
>    components are missing).
>    -
>
>    Ensure any running application using the same consumer group is
>    closed, otherwise it may seem like the consumer code in the app is not
>    working but its actually being processed already (Can look at Zookeeper
>    logs)
>    -
>
>    Can install a Spark plugin to run Spark apps in IntelliJ or Eclipse
>    (will document in github repo).
>    - Convert JSON coming from Kafka using custom JSON2CSV converter or
>    Spark SQL to be able to use Word2Vec feature extraction in Spark.
>
>
> ------------------------------------
>
> Kind Regards,
> Tim
>
> _______________________________________________
> Owasp-appsensor-project mailing list
> Owasp-appsensor-project at lists.owasp.org
> https://lists.owasp.org/mailman/listinfo/owasp-appsensor-project
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.owasp.org/pipermail/owasp-appsensor-project/attachments/20160609/8766b381/attachment.html>

From timothy22000 at gmail.com  Wed Jun 15 23:29:13 2016
From: timothy22000 at gmail.com (Timothy Sum Hon Mun)
Date: Thu, 16 Jun 2016 00:29:13 +0100
Subject: [Owasp-appsensor-project] AppSensor GSoC ML Weekly Summary
In-Reply-To: <CAAxdBBkvMMjmvisXhmcUZ2vAiNUZd-P2XM1J3gSPDXQC31hChw@mail.gmail.com>
References: <CAPQUyzw2Em0BPm3PFw0x6gP3GMJyzCEM-ww6BKAAgqbp_E0qzQ@mail.gmail.com>
	<CAAxdBBkvMMjmvisXhmcUZ2vAiNUZd-P2XM1J3gSPDXQC31hChw@mail.gmail.com>
Message-ID: <CAPQUyzx0nL-YW40_Bee4atFvnK31owebbxXB=t+B5m7Dmfsuuw@mail.gmail.com>

Hey Colin,

Thanks for the warm welcome!

Best Regards,
Tim

On Thu, Jun 9, 2016 at 1:01 PM, Colin Watson <colin.watson at owasp.org> wrote:

> Welcome Tim. It all sounds great.
>
> Regards
>
> Colin
>
>
>
> On 8 June 2016 at 17:39, Timothy Sum Hon Mun <timothy22000 at gmail.com>
> wrote:
>
>> Hi All,
>>
>> I am Tim, the student working with AppSensor for this year's GSoC and
>> have been working closely with mentors on the project. Here is my weekly
>> update on my progress so far.
>>
>> If you have any ideas, questions or comments, please do join in!
>>
>> ------------------------------------
>>
>> Report 7/06/2016
>>
>> Date:  7 June 2016
>>
>> Accomplishments in the past week: Improve Kafka and Spark integration as
>> well as learned how to run it within IntelliJ, Found out problem with Kafka
>> Connect in brew, Created custom JSON2CSV converter to pre-process file
>> before converting it to Vectors for Spark MLib rather than dumping whole
>> json string,
>>
>> Goals for the upcoming week: Use the JSON2CSV converter with Map Reduce
>> in Spark to be able to handle streaming JSON or use Spark SQL to generate
>> tables for this purpose, Run clustering algorithm with different settings
>> on the extracted features, Use interesting features from the result of
>> clustering and try to use at least one classification algorithm to do
>> analysis on the dataset.
>>
>> Blockers (things preventing forward progress):
>>
>> Adjustments to project plan, if any:
>>
>>
>> Other: (use this space to document anything you learned, what worked
>> well, what didn't work well, etc.):
>>
>>    -
>>
>>    Close kafka servers before closing zookeeper. Otherwise, it will
>>    remain open and occupy a port.
>>    -
>>
>>    Brew install kafka for Mac - only installs and gradle builds core
>>    kafka - missing components like connect
>>    -
>>
>>    Source download for Kafka - Need to run gradle then ./gradlew to
>>    build jars required to run kafka (brew does this step automatically but
>>    other components are missing).
>>    -
>>
>>    Ensure any running application using the same consumer group is
>>    closed, otherwise it may seem like the consumer code in the app is not
>>    working but its actually being processed already (Can look at Zookeeper
>>    logs)
>>    -
>>
>>    Can install a Spark plugin to run Spark apps in IntelliJ or Eclipse
>>    (will document in github repo).
>>    - Convert JSON coming from Kafka using custom JSON2CSV converter or
>>    Spark SQL to be able to use Word2Vec feature extraction in Spark.
>>
>>
>> ------------------------------------
>>
>> Kind Regards,
>> Tim
>>
>> _______________________________________________
>> Owasp-appsensor-project mailing list
>> Owasp-appsensor-project at lists.owasp.org
>> https://lists.owasp.org/mailman/listinfo/owasp-appsensor-project
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.owasp.org/pipermail/owasp-appsensor-project/attachments/20160616/9194b373/attachment.html>

From timothy22000 at gmail.com  Wed Jun 15 23:38:11 2016
From: timothy22000 at gmail.com (Timothy Sum Hon Mun)
Date: Thu, 16 Jun 2016 00:38:11 +0100
Subject: [Owasp-appsensor-project] AppSensor GSoC ML Weekly Summary
In-Reply-To: <CAPQUyzw2Em0BPm3PFw0x6gP3GMJyzCEM-ww6BKAAgqbp_E0qzQ@mail.gmail.com>
References: <CAPQUyzw2Em0BPm3PFw0x6gP3GMJyzCEM-ww6BKAAgqbp_E0qzQ@mail.gmail.com>
Message-ID: <CAPQUyzwoAdCYd43mp1oPQu2nQ6mwKBaJRvWfXbs7yRMUY3du5w@mail.gmail.com>

Hi All,

Here is my weekly update on my progress for the past week. As usual, I am
open to comments, suggestions and any ideas. :)

------------------------------------

Report 14/06/2016

Date:  14 June 2016

Accomplishments in the past week: Updated repo with guide on how to set up (
https://github.com/timothy22000/LogstashKafkaSparkStreamingMlibTest/tree/dev),
Using SparkSQL and DataFrames to setup SQL table to store log data,
Converting categorical data to numeric for clustering, Ran normal KMeans
(non streaming KMeans) and obtaining results albeit not meaningful for now
due to it being in numeric form.

Goals for the upcoming week: Converting results from clustering for
categorical data back to numeric to better understand result and decide
which feature to use for classification, Get clustering to work with
streaming kmeans, run classification (streaming linear regression first)
with 1 feature (simple analysis).

Blockers (things preventing forward progress):

Adjustments to project plan, if any: WIll be in Ireland for holidays until
the 22nd June. (Already taken into account in project plan)

Other: (use this space to document anything you learned, what worked well,
what didn't work well, etc.):

   -

   SparkSQL and Dataframes have built in pre-processing (feature extraction
   etc) to handle different data types for different ML algo. Decided to
   abandon using custom JSON2CSV converter since I would have to implement a
   custom encoder to convert categorical data to numerical data whereas these
   capabilities are built into SparkSQL.
   -

   Spend some time learning how to convert categorical features to numeric
   (kmeans in mlib only handles numeric data) on using extractors in SparkMlib
   such as StringIndexer, OneHotEncoder
   -

   Using StringIndexer and there is a IndexToString to convert back.
   Converting back is not as straightforward as I thought or I am overlooking
   something. The SQL table is created per RDD as it arrive (this is when we
   convert incoming data to SQL dataframe and then use StringIndexer) whereas
   after the output is done, the results are from total data coming in so
   converting it back is not as straightforward since the index in the output
   is based on total output which may or may not represent the original
   categorical data. Tried moving SQL context outside of a DStreams (the
   suggested approach that I found during the meeting last night) to get a
   complete SQL table with all data but ran into a multiple Spark Context error


Kind Regards,
Tim

On Wed, Jun 8, 2016 at 5:39 PM, Timothy Sum Hon Mun <timothy22000 at gmail.com>
wrote:

> Hi All,
>
> I am Tim, the student working with AppSensor for this year's GSoC and have
> been working closely with mentors on the project. Here is my weekly update
> on my progress so far.
>
> If you have any ideas, questions or comments, please do join in!
>
> ------------------------------------
>
> Report 7/06/2016
>
> Date:  7 June 2016
>
> Accomplishments in the past week: Improve Kafka and Spark integration as
> well as learned how to run it within IntelliJ, Found out problem with Kafka
> Connect in brew, Created custom JSON2CSV converter to pre-process file
> before converting it to Vectors for Spark MLib rather than dumping whole
> json string,
>
> Goals for the upcoming week: Use the JSON2CSV converter with Map Reduce in
> Spark to be able to handle streaming JSON or use Spark SQL to generate
> tables for this purpose, Run clustering algorithm with different settings
> on the extracted features, Use interesting features from the result of
> clustering and try to use at least one classification algorithm to do
> analysis on the dataset.
>
> Blockers (things preventing forward progress):
>
> Adjustments to project plan, if any:
>
>
> Other: (use this space to document anything you learned, what worked well,
> what didn't work well, etc.):
>
>    -
>
>    Close kafka servers before closing zookeeper. Otherwise, it will
>    remain open and occupy a port.
>    -
>
>    Brew install kafka for Mac - only installs and gradle builds core
>    kafka - missing components like connect
>    -
>
>    Source download for Kafka - Need to run gradle then ./gradlew to build
>    jars required to run kafka (brew does this step automatically but other
>    components are missing).
>    -
>
>    Ensure any running application using the same consumer group is
>    closed, otherwise it may seem like the consumer code in the app is not
>    working but its actually being processed already (Can look at Zookeeper
>    logs)
>    -
>
>    Can install a Spark plugin to run Spark apps in IntelliJ or Eclipse
>    (will document in github repo).
>    - Convert JSON coming from Kafka using custom JSON2CSV converter or
>    Spark SQL to be able to use Word2Vec feature extraction in Spark.
>
>
> ------------------------------------
>
> Kind Regards,
> Tim
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.owasp.org/pipermail/owasp-appsensor-project/attachments/20160616/f322ad9c/attachment-0001.html>

From timothy22000 at gmail.com  Mon Jun 27 00:49:29 2016
From: timothy22000 at gmail.com (Timothy Sum Hon Mun)
Date: Mon, 27 Jun 2016 01:49:29 +0100
Subject: [Owasp-appsensor-project] AppSensor GSoC ML Weekly Summary
In-Reply-To: <CAPQUyzwoAdCYd43mp1oPQu2nQ6mwKBaJRvWfXbs7yRMUY3du5w@mail.gmail.com>
References: <CAPQUyzw2Em0BPm3PFw0x6gP3GMJyzCEM-ww6BKAAgqbp_E0qzQ@mail.gmail.com>
	<CAPQUyzwoAdCYd43mp1oPQu2nQ6mwKBaJRvWfXbs7yRMUY3du5w@mail.gmail.com>
Message-ID: <CAPQUyzwa9reGNJg8P8RKrQ-bEPzt+OVncS+sadTgBrpaJd9xrA@mail.gmail.com>

Hi All,


My weekly update for the previous week. A bit later than the usual day I
post my weekly report due to being away on vacation. Without further ado,
here:


--------------------------------------


Report 25/06/2016

Date:  25 June 2016

Accomplishments in the past week: Implement KMeans clustering (Can filter
data points that belong to a cluster centre and run descriptive stats on it
such as mean etc and get results of cluster centres), Used results from
clustering to run in classification algo, Implemented a simple analysis
using one feature with linearRegressionWithSGD, Refactored code,
Implemented complex analysis (but results is weird and does not match
labels so working on this)

Goals for the upcoming week: Improve results for complex analysis, add test
cases to code, complete mid term evaluation form, Get the streaming kmeans
working (low priority for now), looking into how to deal with geolocation
data (Google Geocoding API)

Blockers (things preventing forward progress):

Adjustments to project plan, if any:

Other: (use this space to document anything you learned, what worked well,
what didn't work well, etc.):

   -

   Couldn?t get Streaming KMeans to work. Problem with converting DStreams
   of String into DStreams of Vectors.
   -

   Tried to use a table that will be updated with new data from streams and
   building kmeans model on that. Essentially saving state between streams.
   http://stackoverflow.com/questions/25484879/sql-over-spark-streaming

http://www.spark.tc/stateful-spark-streaming-using-transform/

   -

   Get cluster centre model value (values are not integers so can?t be
   converted back to categorical feature), filter the results and run
   descriptive statistics to understand it better. (Replace geoIpCityName with
   size of file). Can use either one of the features for simple analysis and
   the combined features for complex analysis. Cannot index country, city name
   to numeric values using normal encoders as well running clustering on them
   since it will just be based on the purely numeric value. Natural
   clusterting of cities within a country and country within a continent - so
   no need to run clustering algo - not sure how to take advantage of this yet.
   -

   Guessing what each cluster means (Currently assuming clusters with low
   mean of response and verb signifies a normal behaviour)
   -

   Need a low number for stepSize when using linear regression with
   Stochastic Gradient Descent otheriwise you will get NaN for predictions.
   Tweaking step size sees to be trial and error process as described here in
   the spark mlib optimization docs. (
   https://spark.apache.org/docs/latest/mllib-optimization.html#stochastic-gradient-descent-sgd).
   The closest thing that I can find to determining the ideal number of step
   size is here (
   http://stackoverflow.com/questions/30981471/sparks-linearregressionwithsgd-is-very-sensitive-to-feature-scaling/34168467)
   with a link to a issue raised in Spark. The formula used to determine the
   step size is 1/2L where L is (x^2)/y where x is largest feature value and y
   is number of data points.
   - Need to scale data (normalize) using StandardScaler to get better
   predictions when doing classification with 3 features.


Kind Regards,
Tim

On Thu, Jun 16, 2016 at 12:38 AM, Timothy Sum Hon Mun <
timothy22000 at gmail.com> wrote:

> Hi All,
>
> Here is my weekly update on my progress for the past week. As usual, I am
> open to comments, suggestions and any ideas. :)
>
> ------------------------------------
>
> Report 14/06/2016
>
> Date:  14 June 2016
>
> Accomplishments in the past week: Updated repo with guide on how to set up
> (
> https://github.com/timothy22000/LogstashKafkaSparkStreamingMlibTest/tree/dev),
> Using SparkSQL and DataFrames to setup SQL table to store log data,
> Converting categorical data to numeric for clustering, Ran normal KMeans
> (non streaming KMeans) and obtaining results albeit not meaningful for now
> due to it being in numeric form.
>
> Goals for the upcoming week: Converting results from clustering for
> categorical data back to numeric to better understand result and decide
> which feature to use for classification, Get clustering to work with
> streaming kmeans, run classification (streaming linear regression first)
> with 1 feature (simple analysis).
>
> Blockers (things preventing forward progress):
>
> Adjustments to project plan, if any: WIll be in Ireland for holidays until
> the 22nd June. (Already taken into account in project plan)
>
> Other: (use this space to document anything you learned, what worked well,
> what didn't work well, etc.):
>
>    -
>
>    SparkSQL and Dataframes have built in pre-processing (feature
>    extraction etc) to handle different data types for different ML algo.
>    Decided to abandon using custom JSON2CSV converter since I would have to
>    implement a custom encoder to convert categorical data to numerical data
>    whereas these capabilities are built into SparkSQL.
>    -
>
>    Spend some time learning how to convert categorical features to
>    numeric (kmeans in mlib only handles numeric data) on using extractors in
>    SparkMlib such as StringIndexer, OneHotEncoder
>    -
>
>    Using StringIndexer and there is a IndexToString to convert back.
>    Converting back is not as straightforward as I thought or I am overlooking
>    something. The SQL table is created per RDD as it arrive (this is when we
>    convert incoming data to SQL dataframe and then use StringIndexer) whereas
>    after the output is done, the results are from total data coming in so
>    converting it back is not as straightforward since the index in the output
>    is based on total output which may or may not represent the original
>    categorical data. Tried moving SQL context outside of a DStreams (the
>    suggested approach that I found during the meeting last night) to get a
>    complete SQL table with all data but ran into a multiple Spark Context error
>
>
> Kind Regards,
> Tim
>
> On Wed, Jun 8, 2016 at 5:39 PM, Timothy Sum Hon Mun <
> timothy22000 at gmail.com> wrote:
>
>> Hi All,
>>
>> I am Tim, the student working with AppSensor for this year's GSoC and
>> have been working closely with mentors on the project. Here is my weekly
>> update on my progress so far.
>>
>> If you have any ideas, questions or comments, please do join in!
>>
>> ------------------------------------
>>
>> Report 7/06/2016
>>
>> Date:  7 June 2016
>>
>> Accomplishments in the past week: Improve Kafka and Spark integration as
>> well as learned how to run it within IntelliJ, Found out problem with Kafka
>> Connect in brew, Created custom JSON2CSV converter to pre-process file
>> before converting it to Vectors for Spark MLib rather than dumping whole
>> json string,
>>
>> Goals for the upcoming week: Use the JSON2CSV converter with Map Reduce
>> in Spark to be able to handle streaming JSON or use Spark SQL to generate
>> tables for this purpose, Run clustering algorithm with different settings
>> on the extracted features, Use interesting features from the result of
>> clustering and try to use at least one classification algorithm to do
>> analysis on the dataset.
>>
>> Blockers (things preventing forward progress):
>>
>> Adjustments to project plan, if any:
>>
>>
>> Other: (use this space to document anything you learned, what worked
>> well, what didn't work well, etc.):
>>
>>    -
>>
>>    Close kafka servers before closing zookeeper. Otherwise, it will
>>    remain open and occupy a port.
>>    -
>>
>>    Brew install kafka for Mac - only installs and gradle builds core
>>    kafka - missing components like connect
>>    -
>>
>>    Source download for Kafka - Need to run gradle then ./gradlew to
>>    build jars required to run kafka (brew does this step automatically but
>>    other components are missing).
>>    -
>>
>>    Ensure any running application using the same consumer group is
>>    closed, otherwise it may seem like the consumer code in the app is not
>>    working but its actually being processed already (Can look at Zookeeper
>>    logs)
>>    -
>>
>>    Can install a Spark plugin to run Spark apps in IntelliJ or Eclipse
>>    (will document in github repo).
>>    - Convert JSON coming from Kafka using custom JSON2CSV converter or
>>    Spark SQL to be able to use Word2Vec feature extraction in Spark.
>>
>>
>> ------------------------------------
>>
>> Kind Regards,
>> Tim
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.owasp.org/pipermail/owasp-appsensor-project/attachments/20160627/c51ce9a8/attachment-0001.html>

