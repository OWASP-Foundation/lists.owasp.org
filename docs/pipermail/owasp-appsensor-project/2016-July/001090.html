<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Owasp-appsensor-project] Fwd: AppSensor GSoC ML Weekly Summary
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:owasp-appsensor-project%40lists.owasp.org?Subject=Re%3A%20%5BOwasp-appsensor-project%5D%20Fwd%3A%20AppSensor%20GSoC%20ML%20Weekly%20Summary&In-Reply-To=%3CCAPQUyzwR-dTq9WFftf-_BYRS11%3DULJBWydeyfrCynzjncxNE0Q%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   
   <LINK REL="Next"  HREF="001091.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Owasp-appsensor-project] Fwd: AppSensor GSoC ML Weekly Summary</H1>
    <B>Timothy Sum Hon Mun</B> 
    <A HREF="mailto:owasp-appsensor-project%40lists.owasp.org?Subject=Re%3A%20%5BOwasp-appsensor-project%5D%20Fwd%3A%20AppSensor%20GSoC%20ML%20Weekly%20Summary&In-Reply-To=%3CCAPQUyzwR-dTq9WFftf-_BYRS11%3DULJBWydeyfrCynzjncxNE0Q%40mail.gmail.com%3E"
       TITLE="[Owasp-appsensor-project] Fwd: AppSensor GSoC ML Weekly Summary">timothy22000 at gmail.com
       </A><BR>
    <I>Fri Jul  8 19:56:47 UTC 2016</I>
    <P><UL>
        
        <LI>Next message: <A HREF="001091.html">[Owasp-appsensor-project] Fwd: AppSensor GSoC ML Weekly Summary
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1090">[ date ]</a>
              <a href="thread.html#1090">[ thread ]</a>
              <a href="subject.html#1090">[ subject ]</a>
              <a href="author.html#1090">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Hi All,


My weekly update for my work on the AppSensor project so far. This week's
update will consist of two weeks. As per usual, if anyone has ideas or
suggestions, I am open to them!


--------------------------------------


Report 28/6/2016

Date:  28 June 2016


Accomplishments in the past week: Implemented normalization when doing
complex analysis and tried to improve results with different parameters
(still weird), Add some test cases for code so far


Goals for the upcoming week: Implement alternative classification algo
(decision tree, random fore etc) to compare results and see if can get
better predictions for both simple and complex analysis, convert country
code to lat/long to be used as feature in classification step, Get the
streaming kmeans working (low priority)


Blockers (things preventing forward progress):


Adjustments to project plan, if any:


Other: (use this space to document anything you learned, what worked well,
what didn't work well, etc.):


   -

   Normalizating the features before running the linearRegressionWithSGD
   seems to have improve the results (smaller number for Predictions but still
   a large number like 1.3564E-10). Will implement one or more different
   classification algorithm (decision tree, logistic regression) to compare
   the results with this one and use it if the prediction is better
   -

   Google Geocoding API (Register API key for 25000 requests) - to geocode
   country code to lat/long - Aim to integrate this as feature with algorithms


--------------------------------------------

Report 7/7/2016

Date:  7 July 2016

Accomplishments in the past week: Implemented logisticRegressionWithLBFGS
for simplex and complex analysis, Implemented Naive Bayes for simple and
complex analysis, Implemented Decision Trees for simplex and complex
analysis (There is still bug with complex analysis). All of them are
performing better than linearRegressionWithSGD, Read about the
implementation of the algorithms (Details below), Attended a Spark coding
dojo (Details below).

Goals for the upcoming week: Geocode location data and try to integrate as
a feature for classification, Experiment on doing rule suggestion with a
single feature first, Watch video on Process Mining, Get the streaming
kmeans working (low priority)

Blockers (things preventing forward progress):

Adjustments to project plan, if any:

Other: (use this space to document anything you learned, what worked well,
what didn't work well, etc.):

   -

   Went to a Spark coding dojo and met Jacek who is a contributor of Spark.
   Learned some things from the dojo and the cool thing is that he has a book
   on Spark to help others. (warning: a lot of pages) The ML section is the
   relevant bits to my project.

Link:
<A HREF="https://www.gitbook.com/book/jaceklaskowski/mastering-apache-spark/details">https://www.gitbook.com/book/jaceklaskowski/mastering-apache-spark/details</A>


   -

   Understand more about the differences between normal gradient descent,
   stochastic gradient descent and batch gradient descent - essentially
   selecting only one training example or a subset of training example instead
   of the whole training set for the calculation of loss. (
   <A HREF="https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent">https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent</A>
   )



   -

   Differences between stochastic gradient descent and BFGS (as well
   L-BFGS)

(
<A HREF="https://www.quora.com/When-should-I-use-BFGS-instead-of-the-more-popular-stochastic-gradient-descent">https://www.quora.com/When-should-I-use-BFGS-instead-of-the-more-popular-stochastic-gradient-descent</A>
)


   -

   Noticed that there was a LinearRegressionWithElasticNet in Spark MLib
   and wondered if it could improved the weird results when using
   linearRegressionWithSGD. From there, I learned about the role of
   regularization in the formula of SGD implemented in Spark and the effects
   it had on feature selection. Regularization is the R half of the formula
   which is independent of the datapoints. f(w):=&#955;R(w)+1n&#8721;i=1nL(w;xi,yi) .(
   <A HREF="https://spark.apache.org/docs/1.6.2/mllib-linear-methods.html#classification">https://spark.apache.org/docs/1.6.2/mllib-linear-methods.html#classification</A>).
   So, there are 3 types of regularization which are L1 (Lasso), L2 (Ridge)
   and Elastic Net (using a mixture of both them where you can specify the
   weights for L1 and L2). To learn more about them like I did, here are the
   links.

(
<A HREF="https://spark-summit.org/2015/events/large-scale-lasso-and-elastic-net-regularized-generalized-linear-models/">https://spark-summit.org/2015/events/large-scale-lasso-and-elastic-net-regularized-generalized-linear-models/</A>
,
<A HREF="http://www.slideshare.net/dbtsai/2015-06-largescale-lasso-and-elasticnet-regularized-generalized-linear-models-at-spark-summit">http://www.slideshare.net/dbtsai/2015-06-largescale-lasso-and-elasticnet-regularized-generalized-linear-models-at-spark-summit</A>
,

<A HREF="https://www.quora.com/Are-there-any-real-applications-of-using-Elastic-Net">https://www.quora.com/Are-there-any-real-applications-of-using-Elastic-Net</A>
, <A HREF="https://www.quora.com/What-is-an-Elastic-Net-for-and-how-is-it-used">https://www.quora.com/What-is-an-Elastic-Net-for-and-how-is-it-used</A> ,
<A HREF="https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization">https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization</A>
)


   -

   Interesting to note that in linearRegressionWithSGD in Spark uses no
   regularization by default while logisticRegressionWithLBFGS uses L2
   regularization by default. (
   <A HREF="https://spark.apache.org/docs/1.6.2/api/java/org/apache/spark/mllib/regression/LinearRegressionWithSGD.html">https://spark.apache.org/docs/1.6.2/api/java/org/apache/spark/mllib/regression/LinearRegressionWithSGD.html</A>
   ,
   <A HREF="https://spark.apache.org/docs/1.6.2/api/java/org/apache/spark/mllib/classification/LogisticRegressionWithLBFGS.html">https://spark.apache.org/docs/1.6.2/api/java/org/apache/spark/mllib/classification/LogisticRegressionWithLBFGS.html</A>
   )



   -

   The reason with regards to why LogisticRegression, Naive Bayes and
   Decision Tree (non-linear) performed better than LinearRegression in my
   case using Spark ML is because the former 3 are classification algos
   instead of regression algos (under classification package instead of
   regression package) which are suited for discrete labels/classes as in the
   case for the cluster centers generated by KMeans clustering.

(
<A HREF="http://www.simafore.com/blog/bid/62482/2-main-differences-between-classification-and-regression-trees">http://www.simafore.com/blog/bid/62482/2-main-differences-between-classification-and-regression-trees</A>
)


   -

   Some additional bits regarding usage of stochastic gradient descent
   (SGD) and markov chain monte carlo simulation (
   <A HREF="https://www.quora.com/When-should-I-use-MCMC-rather-than-Stochastic-Gradient-Descent">https://www.quora.com/When-should-I-use-MCMC-rather-than-Stochastic-Gradient-Descent</A>
   )



   -

   If you receive an exception (DecisionTreeClassifier was given input with
   invalid label column clusters, without the number of classes specified. See
   StringIndexer.) when using DecisionTrees in ML (ML is the newer and higher
   level API for Spark compared to MLib), this is because it expects to know
   the number of labels/classes beforehand. You could specify it explicitly in
   the MLib API but not in the ML API so in the case of my label/classes which
   were generated by KMeans, I had to convert them back to String and then
   re-index it using StringIndexer which implicityly generates some metadata
   with regards to number of label/classes to get it to work with
   DecisionTreeClassifier.

(
<A HREF="http://stackoverflow.com/questions/36517302/randomforestclassifier-was-given-input-with-invalid-label-column-error-in-apache">http://stackoverflow.com/questions/36517302/randomforestclassifier-was-given-input-with-invalid-label-column-error-in-apache</A>
- Related problem but with RandomForest instead)

--------------------------------------

Kind Regards,
Tim

On Thu, Jun 16, 2016 at 12:38 AM, Timothy Sum Hon Mun &lt;
<A HREF="https://lists.owasp.org/mailman/listinfo/owasp-appsensor-project">timothy22000 at gmail.com</A>&gt; wrote:

&gt;<i> Hi All,
</I>&gt;<i>
</I>&gt;<i> Here is my weekly update on my progress for the past week. As usual, I am
</I>&gt;<i> open to comments, suggestions and any ideas. :)
</I>&gt;<i>
</I>&gt;<i> ------------------------------------
</I>&gt;<i>
</I>&gt;<i> Report 14/06/2016
</I>&gt;<i>
</I>&gt;<i> Date:  14 June 2016
</I>&gt;<i>
</I>&gt;<i> Accomplishments in the past week: Updated repo with guide on how to set up
</I>&gt;<i> (
</I>&gt;<i> <A HREF="https://github.com/timothy22000/LogstashKafkaSparkStreamingMlibTest/tree/dev">https://github.com/timothy22000/LogstashKafkaSparkStreamingMlibTest/tree/dev</A>),
</I>&gt;<i> Using SparkSQL and DataFrames to setup SQL table to store log data,
</I>&gt;<i> Converting categorical data to numeric for clustering, Ran normal KMeans
</I>&gt;<i> (non streaming KMeans) and obtaining results albeit not meaningful for now
</I>&gt;<i> due to it being in numeric form.
</I>&gt;<i>
</I>&gt;<i> Goals for the upcoming week: Converting results from clustering for
</I>&gt;<i> categorical data back to numeric to better understand result and decide
</I>&gt;<i> which feature to use for classification, Get clustering to work with
</I>&gt;<i> streaming kmeans, run classification (streaming linear regression first)
</I>&gt;<i> with 1 feature (simple analysis).
</I>&gt;<i>
</I>&gt;<i> Blockers (things preventing forward progress):
</I>&gt;<i>
</I>&gt;<i> Adjustments to project plan, if any: WIll be in Ireland for holidays until
</I>&gt;<i> the 22nd June. (Already taken into account in project plan)
</I>&gt;<i>
</I>&gt;<i> Other: (use this space to document anything you learned, what worked well,
</I>&gt;<i> what didn't work well, etc.):
</I>&gt;<i>
</I>&gt;<i>    -
</I>&gt;<i>
</I>&gt;<i>    SparkSQL and Dataframes have built in pre-processing (feature
</I>&gt;<i>    extraction etc) to handle different data types for different ML algo.
</I>&gt;<i>    Decided to abandon using custom JSON2CSV converter since I would have to
</I>&gt;<i>    implement a custom encoder to convert categorical data to numerical data
</I>&gt;<i>    whereas these capabilities are built into SparkSQL.
</I>&gt;<i>    -
</I>&gt;<i>
</I>&gt;<i>    Spend some time learning how to convert categorical features to
</I>&gt;<i>    numeric (kmeans in mlib only handles numeric data) on using extractors in
</I>&gt;<i>    SparkMlib such as StringIndexer, OneHotEncoder
</I>&gt;<i>    -
</I>&gt;<i>
</I>&gt;<i>    Using StringIndexer and there is a IndexToString to convert back.
</I>&gt;<i>    Converting back is not as straightforward as I thought or I am overlooking
</I>&gt;<i>    something. The SQL table is created per RDD as it arrive (this is when we
</I>&gt;<i>    convert incoming data to SQL dataframe and then use StringIndexer) whereas
</I>&gt;<i>    after the output is done, the results are from total data coming in so
</I>&gt;<i>    converting it back is not as straightforward since the index in the output
</I>&gt;<i>    is based on total output which may or may not represent the original
</I>&gt;<i>    categorical data. Tried moving SQL context outside of a DStreams (the
</I>&gt;<i>    suggested approach that I found during the meeting last night) to get a
</I>&gt;<i>    complete SQL table with all data but ran into a multiple Spark Context error
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> Kind Regards,
</I>&gt;<i> Tim
</I>&gt;<i>
</I>&gt;<i> On Wed, Jun 8, 2016 at 5:39 PM, Timothy Sum Hon Mun &lt;
</I>&gt;<i> <A HREF="https://lists.owasp.org/mailman/listinfo/owasp-appsensor-project">timothy22000 at gmail.com</A>&gt; wrote:
</I>&gt;<i>
</I>&gt;&gt;<i> Hi All,
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> I am Tim, the student working with AppSensor for this year's GSoC and
</I>&gt;&gt;<i> have been working closely with mentors on the project. Here is my weekly
</I>&gt;&gt;<i> update on my progress so far.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> If you have any ideas, questions or comments, please do join in!
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> ------------------------------------
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Report 7/06/2016
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Date:  7 June 2016
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Accomplishments in the past week: Improve Kafka and Spark integration as
</I>&gt;&gt;<i> well as learned how to run it within IntelliJ, Found out problem with Kafka
</I>&gt;&gt;<i> Connect in brew, Created custom JSON2CSV converter to pre-process file
</I>&gt;&gt;<i> before converting it to Vectors for Spark MLib rather than dumping whole
</I>&gt;&gt;<i> json string,
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Goals for the upcoming week: Use the JSON2CSV converter with Map Reduce
</I>&gt;&gt;<i> in Spark to be able to handle streaming JSON or use Spark SQL to generate
</I>&gt;&gt;<i> tables for this purpose, Run clustering algorithm with different settings
</I>&gt;&gt;<i> on the extracted features, Use interesting features from the result of
</I>&gt;&gt;<i> clustering and try to use at least one classification algorithm to do
</I>&gt;&gt;<i> analysis on the dataset.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Blockers (things preventing forward progress):
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Adjustments to project plan, if any:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Other: (use this space to document anything you learned, what worked
</I>&gt;&gt;<i> well, what didn't work well, etc.):
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    -
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    Close kafka servers before closing zookeeper. Otherwise, it will
</I>&gt;&gt;<i>    remain open and occupy a port.
</I>&gt;&gt;<i>    -
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    Brew install kafka for Mac - only installs and gradle builds core
</I>&gt;&gt;<i>    kafka - missing components like connect
</I>&gt;&gt;<i>    -
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    Source download for Kafka - Need to run gradle then ./gradlew to
</I>&gt;&gt;<i>    build jars required to run kafka (brew does this step automatically but
</I>&gt;&gt;<i>    other components are missing).
</I>&gt;&gt;<i>    -
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    Ensure any running application using the same consumer group is
</I>&gt;&gt;<i>    closed, otherwise it may seem like the consumer code in the app is not
</I>&gt;&gt;<i>    working but its actually being processed already (Can look at Zookeeper
</I>&gt;&gt;<i>    logs)
</I>&gt;&gt;<i>    -
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    Can install a Spark plugin to run Spark apps in IntelliJ or Eclipse
</I>&gt;&gt;<i>    (will document in github repo).
</I>&gt;&gt;<i>    - Convert JSON coming from Kafka using custom JSON2CSV converter or
</I>&gt;&gt;<i>    Spark SQL to be able to use Word2Vec feature extraction in Spark.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> ------------------------------------
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Kind Regards,
</I>&gt;&gt;<i> Tim
</I>&gt;&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://lists.owasp.org/pipermail/owasp-appsensor-project/attachments/20160708/1158a899/attachment-0001.html">http://lists.owasp.org/pipermail/owasp-appsensor-project/attachments/20160708/1158a899/attachment-0001.html</A>&gt;
</PRE>


<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	
	<LI>Next message: <A HREF="001091.html">[Owasp-appsensor-project] Fwd: AppSensor GSoC ML Weekly Summary
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1090">[ date ]</a>
              <a href="thread.html#1090">[ thread ]</a>
              <a href="subject.html#1090">[ subject ]</a>
              <a href="author.html#1090">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.owasp.org/mailman/listinfo/owasp-appsensor-project">More information about the Owasp-appsensor-project
mailing list</a><br>
</body></html>
