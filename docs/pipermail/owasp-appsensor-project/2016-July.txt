From timothy22000 at gmail.com  Fri Jul  8 19:56:47 2016
From: timothy22000 at gmail.com (Timothy Sum Hon Mun)
Date: Fri, 8 Jul 2016 20:56:47 +0100
Subject: [Owasp-appsensor-project] Fwd: AppSensor GSoC ML Weekly Summary
In-Reply-To: <CAPQUyzwa9reGNJg8P8RKrQ-bEPzt+OVncS+sadTgBrpaJd9xrA@mail.gmail.com>
References: <CAPQUyzw2Em0BPm3PFw0x6gP3GMJyzCEM-ww6BKAAgqbp_E0qzQ@mail.gmail.com>
	<CAPQUyzwoAdCYd43mp1oPQu2nQ6mwKBaJRvWfXbs7yRMUY3du5w@mail.gmail.com>
	<CAPQUyzwa9reGNJg8P8RKrQ-bEPzt+OVncS+sadTgBrpaJd9xrA@mail.gmail.com>
Message-ID: <CAPQUyzwR-dTq9WFftf-_BYRS11=ULJBWydeyfrCynzjncxNE0Q@mail.gmail.com>

Hi All,


My weekly update for my work on the AppSensor project so far. This week's
update will consist of two weeks. As per usual, if anyone has ideas or
suggestions, I am open to them!


--------------------------------------


Report 28/6/2016

Date:  28 June 2016


Accomplishments in the past week: Implemented normalization when doing
complex analysis and tried to improve results with different parameters
(still weird), Add some test cases for code so far


Goals for the upcoming week: Implement alternative classification algo
(decision tree, random fore etc) to compare results and see if can get
better predictions for both simple and complex analysis, convert country
code to lat/long to be used as feature in classification step, Get the
streaming kmeans working (low priority)


Blockers (things preventing forward progress):


Adjustments to project plan, if any:


Other: (use this space to document anything you learned, what worked well,
what didn't work well, etc.):


   -

   Normalizating the features before running the linearRegressionWithSGD
   seems to have improve the results (smaller number for Predictions but still
   a large number like 1.3564E-10). Will implement one or more different
   classification algorithm (decision tree, logistic regression) to compare
   the results with this one and use it if the prediction is better
   -

   Google Geocoding API (Register API key for 25000 requests) - to geocode
   country code to lat/long - Aim to integrate this as feature with algorithms


--------------------------------------------

Report 7/7/2016

Date:  7 July 2016

Accomplishments in the past week: Implemented logisticRegressionWithLBFGS
for simplex and complex analysis, Implemented Naive Bayes for simple and
complex analysis, Implemented Decision Trees for simplex and complex
analysis (There is still bug with complex analysis). All of them are
performing better than linearRegressionWithSGD, Read about the
implementation of the algorithms (Details below), Attended a Spark coding
dojo (Details below).

Goals for the upcoming week: Geocode location data and try to integrate as
a feature for classification, Experiment on doing rule suggestion with a
single feature first, Watch video on Process Mining, Get the streaming
kmeans working (low priority)

Blockers (things preventing forward progress):

Adjustments to project plan, if any:

Other: (use this space to document anything you learned, what worked well,
what didn't work well, etc.):

   -

   Went to a Spark coding dojo and met Jacek who is a contributor of Spark.
   Learned some things from the dojo and the cool thing is that he has a book
   on Spark to help others. (warning: a lot of pages) The ML section is the
   relevant bits to my project.

Link:
https://www.gitbook.com/book/jaceklaskowski/mastering-apache-spark/details


   -

   Understand more about the differences between normal gradient descent,
   stochastic gradient descent and batch gradient descent - essentially
   selecting only one training example or a subset of training example instead
   of the whole training set for the calculation of loss. (
   https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent
   )



   -

   Differences between stochastic gradient descent and BFGS (as well
   L-BFGS)

(
https://www.quora.com/When-should-I-use-BFGS-instead-of-the-more-popular-stochastic-gradient-descent
)


   -

   Noticed that there was a LinearRegressionWithElasticNet in Spark MLib
   and wondered if it could improved the weird results when using
   linearRegressionWithSGD. From there, I learned about the role of
   regularization in the formula of SGD implemented in Spark and the effects
   it had on feature selection. Regularization is the R half of the formula
   which is independent of the datapoints. f(w):=?R(w)+1n?i=1nL(w;xi,yi) .(
   https://spark.apache.org/docs/1.6.2/mllib-linear-methods.html#classification).
   So, there are 3 types of regularization which are L1 (Lasso), L2 (Ridge)
   and Elastic Net (using a mixture of both them where you can specify the
   weights for L1 and L2). To learn more about them like I did, here are the
   links.

(
https://spark-summit.org/2015/events/large-scale-lasso-and-elastic-net-regularized-generalized-linear-models/
,
http://www.slideshare.net/dbtsai/2015-06-largescale-lasso-and-elasticnet-regularized-generalized-linear-models-at-spark-summit
,

https://www.quora.com/Are-there-any-real-applications-of-using-Elastic-Net
, https://www.quora.com/What-is-an-Elastic-Net-for-and-how-is-it-used ,
https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization
)


   -

   Interesting to note that in linearRegressionWithSGD in Spark uses no
   regularization by default while logisticRegressionWithLBFGS uses L2
   regularization by default. (
   https://spark.apache.org/docs/1.6.2/api/java/org/apache/spark/mllib/regression/LinearRegressionWithSGD.html
   ,
   https://spark.apache.org/docs/1.6.2/api/java/org/apache/spark/mllib/classification/LogisticRegressionWithLBFGS.html
   )



   -

   The reason with regards to why LogisticRegression, Naive Bayes and
   Decision Tree (non-linear) performed better than LinearRegression in my
   case using Spark ML is because the former 3 are classification algos
   instead of regression algos (under classification package instead of
   regression package) which are suited for discrete labels/classes as in the
   case for the cluster centers generated by KMeans clustering.

(
http://www.simafore.com/blog/bid/62482/2-main-differences-between-classification-and-regression-trees
)


   -

   Some additional bits regarding usage of stochastic gradient descent
   (SGD) and markov chain monte carlo simulation (
   https://www.quora.com/When-should-I-use-MCMC-rather-than-Stochastic-Gradient-Descent
   )



   -

   If you receive an exception (DecisionTreeClassifier was given input with
   invalid label column clusters, without the number of classes specified. See
   StringIndexer.) when using DecisionTrees in ML (ML is the newer and higher
   level API for Spark compared to MLib), this is because it expects to know
   the number of labels/classes beforehand. You could specify it explicitly in
   the MLib API but not in the ML API so in the case of my label/classes which
   were generated by KMeans, I had to convert them back to String and then
   re-index it using StringIndexer which implicityly generates some metadata
   with regards to number of label/classes to get it to work with
   DecisionTreeClassifier.

(
http://stackoverflow.com/questions/36517302/randomforestclassifier-was-given-input-with-invalid-label-column-error-in-apache
- Related problem but with RandomForest instead)

--------------------------------------

Kind Regards,
Tim

On Thu, Jun 16, 2016 at 12:38 AM, Timothy Sum Hon Mun <
timothy22000 at gmail.com> wrote:

> Hi All,
>
> Here is my weekly update on my progress for the past week. As usual, I am
> open to comments, suggestions and any ideas. :)
>
> ------------------------------------
>
> Report 14/06/2016
>
> Date:  14 June 2016
>
> Accomplishments in the past week: Updated repo with guide on how to set up
> (
> https://github.com/timothy22000/LogstashKafkaSparkStreamingMlibTest/tree/dev),
> Using SparkSQL and DataFrames to setup SQL table to store log data,
> Converting categorical data to numeric for clustering, Ran normal KMeans
> (non streaming KMeans) and obtaining results albeit not meaningful for now
> due to it being in numeric form.
>
> Goals for the upcoming week: Converting results from clustering for
> categorical data back to numeric to better understand result and decide
> which feature to use for classification, Get clustering to work with
> streaming kmeans, run classification (streaming linear regression first)
> with 1 feature (simple analysis).
>
> Blockers (things preventing forward progress):
>
> Adjustments to project plan, if any: WIll be in Ireland for holidays until
> the 22nd June. (Already taken into account in project plan)
>
> Other: (use this space to document anything you learned, what worked well,
> what didn't work well, etc.):
>
>    -
>
>    SparkSQL and Dataframes have built in pre-processing (feature
>    extraction etc) to handle different data types for different ML algo.
>    Decided to abandon using custom JSON2CSV converter since I would have to
>    implement a custom encoder to convert categorical data to numerical data
>    whereas these capabilities are built into SparkSQL.
>    -
>
>    Spend some time learning how to convert categorical features to
>    numeric (kmeans in mlib only handles numeric data) on using extractors in
>    SparkMlib such as StringIndexer, OneHotEncoder
>    -
>
>    Using StringIndexer and there is a IndexToString to convert back.
>    Converting back is not as straightforward as I thought or I am overlooking
>    something. The SQL table is created per RDD as it arrive (this is when we
>    convert incoming data to SQL dataframe and then use StringIndexer) whereas
>    after the output is done, the results are from total data coming in so
>    converting it back is not as straightforward since the index in the output
>    is based on total output which may or may not represent the original
>    categorical data. Tried moving SQL context outside of a DStreams (the
>    suggested approach that I found during the meeting last night) to get a
>    complete SQL table with all data but ran into a multiple Spark Context error
>
>
> Kind Regards,
> Tim
>
> On Wed, Jun 8, 2016 at 5:39 PM, Timothy Sum Hon Mun <
> timothy22000 at gmail.com> wrote:
>
>> Hi All,
>>
>> I am Tim, the student working with AppSensor for this year's GSoC and
>> have been working closely with mentors on the project. Here is my weekly
>> update on my progress so far.
>>
>> If you have any ideas, questions or comments, please do join in!
>>
>> ------------------------------------
>>
>> Report 7/06/2016
>>
>> Date:  7 June 2016
>>
>> Accomplishments in the past week: Improve Kafka and Spark integration as
>> well as learned how to run it within IntelliJ, Found out problem with Kafka
>> Connect in brew, Created custom JSON2CSV converter to pre-process file
>> before converting it to Vectors for Spark MLib rather than dumping whole
>> json string,
>>
>> Goals for the upcoming week: Use the JSON2CSV converter with Map Reduce
>> in Spark to be able to handle streaming JSON or use Spark SQL to generate
>> tables for this purpose, Run clustering algorithm with different settings
>> on the extracted features, Use interesting features from the result of
>> clustering and try to use at least one classification algorithm to do
>> analysis on the dataset.
>>
>> Blockers (things preventing forward progress):
>>
>> Adjustments to project plan, if any:
>>
>>
>> Other: (use this space to document anything you learned, what worked
>> well, what didn't work well, etc.):
>>
>>    -
>>
>>    Close kafka servers before closing zookeeper. Otherwise, it will
>>    remain open and occupy a port.
>>    -
>>
>>    Brew install kafka for Mac - only installs and gradle builds core
>>    kafka - missing components like connect
>>    -
>>
>>    Source download for Kafka - Need to run gradle then ./gradlew to
>>    build jars required to run kafka (brew does this step automatically but
>>    other components are missing).
>>    -
>>
>>    Ensure any running application using the same consumer group is
>>    closed, otherwise it may seem like the consumer code in the app is not
>>    working but its actually being processed already (Can look at Zookeeper
>>    logs)
>>    -
>>
>>    Can install a Spark plugin to run Spark apps in IntelliJ or Eclipse
>>    (will document in github repo).
>>    - Convert JSON coming from Kafka using custom JSON2CSV converter or
>>    Spark SQL to be able to use Word2Vec feature extraction in Spark.
>>
>>
>> ------------------------------------
>>
>> Kind Regards,
>> Tim
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.owasp.org/pipermail/owasp-appsensor-project/attachments/20160708/1158a899/attachment-0001.html>

From timothy22000 at gmail.com  Tue Jul 19 14:46:27 2016
From: timothy22000 at gmail.com (Timothy Sum Hon Mun)
Date: Tue, 19 Jul 2016 15:46:27 +0100
Subject: [Owasp-appsensor-project] Fwd: AppSensor GSoC ML Weekly Summary
In-Reply-To: <CAPQUyzwR-dTq9WFftf-_BYRS11=ULJBWydeyfrCynzjncxNE0Q@mail.gmail.com>
References: <CAPQUyzw2Em0BPm3PFw0x6gP3GMJyzCEM-ww6BKAAgqbp_E0qzQ@mail.gmail.com>
	<CAPQUyzwoAdCYd43mp1oPQu2nQ6mwKBaJRvWfXbs7yRMUY3du5w@mail.gmail.com>
	<CAPQUyzwa9reGNJg8P8RKrQ-bEPzt+OVncS+sadTgBrpaJd9xrA@mail.gmail.com>
	<CAPQUyzwR-dTq9WFftf-_BYRS11=ULJBWydeyfrCynzjncxNE0Q@mail.gmail.com>
Message-ID: <CAPQUyzzvZkVnt+Gh1XCRYxojZB8SCsBfr9d3bRBbF4cTsCHzVw@mail.gmail.com>

Hi All,

My update for my progress last week failed to send out and I didn't realize
it soon enough so here it is instead.

If anyone has ideas or suggestions, please feel free to chip in.
--------------------------------------

Report 12/7/2016

Date:  12 July 2016

Accomplishments in the past week: Got Streaming KMeans working, Geocoded
location data with google geocoding API and integrated it with logistic
regression, Fix the noOfBins problem with decision trees for complex
analysis

Goals for the upcoming week: Integrate lat/long with other classification
algorithm, cache or save geocoded lat/long to a persistent storage or just
on local file system due to request limits, cluster on one feature for rule
suggestion, create artificial anomalies into dataset (simple for now)

Blockers (things preventing forward progress):

Adjustments to project plan, if any:

Other: (use this space to document anything you learned, what worked well,
what didn't work well, etc.):

   -

   maxBins in MLib Decision Tree will default to lower value if it is
   greater than the largest number of possible values for categorical values
   -

   Google Geocoding API - limits requests to 2500 a day. Problem for large
   datasets - Should be alright for our prototype case - otherwise consider
   using cache or database


--------------------------------------

Kind Regards,
Tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.owasp.org/pipermail/owasp-appsensor-project/attachments/20160719/1921aafd/attachment.html>

From timothy22000 at gmail.com  Tue Jul 26 06:42:36 2016
From: timothy22000 at gmail.com (Timothy Sum Hon Mun)
Date: Tue, 26 Jul 2016 07:42:36 +0100
Subject: [Owasp-appsensor-project] AppSensor GSoC ML Weekly Summary
In-Reply-To: <CAPQUyzzvZkVnt+Gh1XCRYxojZB8SCsBfr9d3bRBbF4cTsCHzVw@mail.gmail.com>
References: <CAPQUyzw2Em0BPm3PFw0x6gP3GMJyzCEM-ww6BKAAgqbp_E0qzQ@mail.gmail.com>
	<CAPQUyzwoAdCYd43mp1oPQu2nQ6mwKBaJRvWfXbs7yRMUY3du5w@mail.gmail.com>
	<CAPQUyzwa9reGNJg8P8RKrQ-bEPzt+OVncS+sadTgBrpaJd9xrA@mail.gmail.com>
	<CAPQUyzwR-dTq9WFftf-_BYRS11=ULJBWydeyfrCynzjncxNE0Q@mail.gmail.com>
	<CAPQUyzzvZkVnt+Gh1XCRYxojZB8SCsBfr9d3bRBbF4cTsCHzVw@mail.gmail.com>
Message-ID: <CAPQUyzz+N_JvQ0Yne1CrEpJ2VaMoqtSTeuriwx8716EqCXW1RQ@mail.gmail.com>

Hi All,

My weekly project update as per usual. :)

If anyone has ideas or suggestions, please feel free to chip in.

--------------------------------------


Report 19/7/2016

Date:  19 July 2016

Accomplishments in the past week: Geocoded API - caching - convert to
column, Found out that Streaming KMeans is not suitable for my use-case
(reasoning below), Did clustering with one feature for rule suggestion
(HTTP Verb - added it in weird verb and clustered - there is a separate
cluster that signify anomalous)

Goals for the upcoming week: Clustering for more features for rule
suggestion, Experiment with rule suggestion with only clustering and
clustering then classification for more than one feature, Create script for
generating artificial dataset

Blockers (things preventing forward progress):

Adjustments to project plan, if any: - Working on a university project
deadline this week - slower progress

Other: (use this space to document anything you learned, what worked well,
what didn't work well, etc.):

   -

   Index stability with VectorIndexer and StringIndexer - Index for
   categorical to numerical changes every time for both indexers
   (VectorIndexer:
   https://spark.apache.org/docs/1.6.0/api/java/org/apache/spark/ml/feature/VectorIndexer.html
   and StringIndexer:
   https://spark.apache.org/docs/1.6.0/api/java/org/apache/spark/ml/feature/StringIndexer.html
   - most frequent label get index 0). Therefore, if I index streaming data as
   they come in, I cannot ensure that the index for a feature being taken in
   by an algorithm can be consistent as new data comes in. The alternative
   would be to keep a table and update/re-index it as new data points comes in
   and only feed in the new data points to the streaming kmeans - the index
   may change as well if new data points come in. So current approach where I
   re-train the kmeans model on the whole dataset or table is fine for now to
   ensure index stability.


--------------------------------------

Kind Regards,
Tim


On Tue, Jul 19, 2016 at 3:46 PM, Timothy Sum Hon Mun <timothy22000 at gmail.com
> wrote:

> Hi All,
>
> My update for my progress last week failed to send out and I didn't
> realize it soon enough so here it is instead.
>
> If anyone has ideas or suggestions, please feel free to chip in.
> --------------------------------------
>
> Report 12/7/2016
>
> Date:  12 July 2016
>
> Accomplishments in the past week: Got Streaming KMeans working, Geocoded
> location data with google geocoding API and integrated it with logistic
> regression, Fix the noOfBins problem with decision trees for complex
> analysis
>
> Goals for the upcoming week: Integrate lat/long with other classification
> algorithm, cache or save geocoded lat/long to a persistent storage or just
> on local file system due to request limits, cluster on one feature for rule
> suggestion, create artificial anomalies into dataset (simple for now)
>
> Blockers (things preventing forward progress):
>
> Adjustments to project plan, if any:
>
> Other: (use this space to document anything you learned, what worked well,
> what didn't work well, etc.):
>
>    -
>
>    maxBins in MLib Decision Tree will default to lower value if it is
>    greater than the largest number of possible values for categorical values
>    -
>
>    Google Geocoding API - limits requests to 2500 a day. Problem for
>    large datasets - Should be alright for our prototype case - otherwise
>    consider using cache or database
>
>
> --------------------------------------
>
> Kind Regards,
> Tim
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.owasp.org/pipermail/owasp-appsensor-project/attachments/20160726/b57d8377/attachment.html>

