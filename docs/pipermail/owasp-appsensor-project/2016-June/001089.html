<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Owasp-appsensor-project] AppSensor GSoC ML Weekly Summary
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:owasp-appsensor-project%40lists.owasp.org?Subject=Re%3A%20%5BOwasp-appsensor-project%5D%20AppSensor%20GSoC%20ML%20Weekly%20Summary&In-Reply-To=%3CCAPQUyzwa9reGNJg8P8RKrQ-bEPzt%2BOVncS%2BsadTgBrpaJd9xrA%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001088.html">
   
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Owasp-appsensor-project] AppSensor GSoC ML Weekly Summary</H1>
    <B>Timothy Sum Hon Mun</B> 
    <A HREF="mailto:owasp-appsensor-project%40lists.owasp.org?Subject=Re%3A%20%5BOwasp-appsensor-project%5D%20AppSensor%20GSoC%20ML%20Weekly%20Summary&In-Reply-To=%3CCAPQUyzwa9reGNJg8P8RKrQ-bEPzt%2BOVncS%2BsadTgBrpaJd9xrA%40mail.gmail.com%3E"
       TITLE="[Owasp-appsensor-project] AppSensor GSoC ML Weekly Summary">timothy22000 at gmail.com
       </A><BR>
    <I>Mon Jun 27 00:49:29 UTC 2016</I>
    <P><UL>
        <LI>Previous message: <A HREF="001088.html">[Owasp-appsensor-project] AppSensor GSoC ML Weekly Summary
</A></li>
        
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1089">[ date ]</a>
              <a href="thread.html#1089">[ thread ]</a>
              <a href="subject.html#1089">[ subject ]</a>
              <a href="author.html#1089">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Hi All,


My weekly update for the previous week. A bit later than the usual day I
post my weekly report due to being away on vacation. Without further ado,
here:


--------------------------------------


Report 25/06/2016

Date:  25 June 2016

Accomplishments in the past week: Implement KMeans clustering (Can filter
data points that belong to a cluster centre and run descriptive stats on it
such as mean etc and get results of cluster centres), Used results from
clustering to run in classification algo, Implemented a simple analysis
using one feature with linearRegressionWithSGD, Refactored code,
Implemented complex analysis (but results is weird and does not match
labels so working on this)

Goals for the upcoming week: Improve results for complex analysis, add test
cases to code, complete mid term evaluation form, Get the streaming kmeans
working (low priority for now), looking into how to deal with geolocation
data (Google Geocoding API)

Blockers (things preventing forward progress):

Adjustments to project plan, if any:

Other: (use this space to document anything you learned, what worked well,
what didn't work well, etc.):

   -

   Couldn&#8217;t get Streaming KMeans to work. Problem with converting DStreams
   of String into DStreams of Vectors.
   -

   Tried to use a table that will be updated with new data from streams and
   building kmeans model on that. Essentially saving state between streams.
   <A HREF="http://stackoverflow.com/questions/25484879/sql-over-spark-streaming">http://stackoverflow.com/questions/25484879/sql-over-spark-streaming</A>

<A HREF="http://www.spark.tc/stateful-spark-streaming-using-transform/">http://www.spark.tc/stateful-spark-streaming-using-transform/</A>

   -

   Get cluster centre model value (values are not integers so can&#8217;t be
   converted back to categorical feature), filter the results and run
   descriptive statistics to understand it better. (Replace geoIpCityName with
   size of file). Can use either one of the features for simple analysis and
   the combined features for complex analysis. Cannot index country, city name
   to numeric values using normal encoders as well running clustering on them
   since it will just be based on the purely numeric value. Natural
   clusterting of cities within a country and country within a continent - so
   no need to run clustering algo - not sure how to take advantage of this yet.
   -

   Guessing what each cluster means (Currently assuming clusters with low
   mean of response and verb signifies a normal behaviour)
   -

   Need a low number for stepSize when using linear regression with
   Stochastic Gradient Descent otheriwise you will get NaN for predictions.
   Tweaking step size sees to be trial and error process as described here in
   the spark mlib optimization docs. (
   <A HREF="https://spark.apache.org/docs/latest/mllib-optimization.html#stochastic-gradient-descent-sgd">https://spark.apache.org/docs/latest/mllib-optimization.html#stochastic-gradient-descent-sgd</A>).
   The closest thing that I can find to determining the ideal number of step
   size is here (
   <A HREF="http://stackoverflow.com/questions/30981471/sparks-linearregressionwithsgd-is-very-sensitive-to-feature-scaling/34168467">http://stackoverflow.com/questions/30981471/sparks-linearregressionwithsgd-is-very-sensitive-to-feature-scaling/34168467</A>)
   with a link to a issue raised in Spark. The formula used to determine the
   step size is 1/2L where L is (x^2)/y where x is largest feature value and y
   is number of data points.
   - Need to scale data (normalize) using StandardScaler to get better
   predictions when doing classification with 3 features.


Kind Regards,
Tim

On Thu, Jun 16, 2016 at 12:38 AM, Timothy Sum Hon Mun &lt;
<A HREF="https://lists.owasp.org/mailman/listinfo/owasp-appsensor-project">timothy22000 at gmail.com</A>&gt; wrote:

&gt;<i> Hi All,
</I>&gt;<i>
</I>&gt;<i> Here is my weekly update on my progress for the past week. As usual, I am
</I>&gt;<i> open to comments, suggestions and any ideas. :)
</I>&gt;<i>
</I>&gt;<i> ------------------------------------
</I>&gt;<i>
</I>&gt;<i> Report 14/06/2016
</I>&gt;<i>
</I>&gt;<i> Date:  14 June 2016
</I>&gt;<i>
</I>&gt;<i> Accomplishments in the past week: Updated repo with guide on how to set up
</I>&gt;<i> (
</I>&gt;<i> <A HREF="https://github.com/timothy22000/LogstashKafkaSparkStreamingMlibTest/tree/dev">https://github.com/timothy22000/LogstashKafkaSparkStreamingMlibTest/tree/dev</A>),
</I>&gt;<i> Using SparkSQL and DataFrames to setup SQL table to store log data,
</I>&gt;<i> Converting categorical data to numeric for clustering, Ran normal KMeans
</I>&gt;<i> (non streaming KMeans) and obtaining results albeit not meaningful for now
</I>&gt;<i> due to it being in numeric form.
</I>&gt;<i>
</I>&gt;<i> Goals for the upcoming week: Converting results from clustering for
</I>&gt;<i> categorical data back to numeric to better understand result and decide
</I>&gt;<i> which feature to use for classification, Get clustering to work with
</I>&gt;<i> streaming kmeans, run classification (streaming linear regression first)
</I>&gt;<i> with 1 feature (simple analysis).
</I>&gt;<i>
</I>&gt;<i> Blockers (things preventing forward progress):
</I>&gt;<i>
</I>&gt;<i> Adjustments to project plan, if any: WIll be in Ireland for holidays until
</I>&gt;<i> the 22nd June. (Already taken into account in project plan)
</I>&gt;<i>
</I>&gt;<i> Other: (use this space to document anything you learned, what worked well,
</I>&gt;<i> what didn't work well, etc.):
</I>&gt;<i>
</I>&gt;<i>    -
</I>&gt;<i>
</I>&gt;<i>    SparkSQL and Dataframes have built in pre-processing (feature
</I>&gt;<i>    extraction etc) to handle different data types for different ML algo.
</I>&gt;<i>    Decided to abandon using custom JSON2CSV converter since I would have to
</I>&gt;<i>    implement a custom encoder to convert categorical data to numerical data
</I>&gt;<i>    whereas these capabilities are built into SparkSQL.
</I>&gt;<i>    -
</I>&gt;<i>
</I>&gt;<i>    Spend some time learning how to convert categorical features to
</I>&gt;<i>    numeric (kmeans in mlib only handles numeric data) on using extractors in
</I>&gt;<i>    SparkMlib such as StringIndexer, OneHotEncoder
</I>&gt;<i>    -
</I>&gt;<i>
</I>&gt;<i>    Using StringIndexer and there is a IndexToString to convert back.
</I>&gt;<i>    Converting back is not as straightforward as I thought or I am overlooking
</I>&gt;<i>    something. The SQL table is created per RDD as it arrive (this is when we
</I>&gt;<i>    convert incoming data to SQL dataframe and then use StringIndexer) whereas
</I>&gt;<i>    after the output is done, the results are from total data coming in so
</I>&gt;<i>    converting it back is not as straightforward since the index in the output
</I>&gt;<i>    is based on total output which may or may not represent the original
</I>&gt;<i>    categorical data. Tried moving SQL context outside of a DStreams (the
</I>&gt;<i>    suggested approach that I found during the meeting last night) to get a
</I>&gt;<i>    complete SQL table with all data but ran into a multiple Spark Context error
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> Kind Regards,
</I>&gt;<i> Tim
</I>&gt;<i>
</I>&gt;<i> On Wed, Jun 8, 2016 at 5:39 PM, Timothy Sum Hon Mun &lt;
</I>&gt;<i> <A HREF="https://lists.owasp.org/mailman/listinfo/owasp-appsensor-project">timothy22000 at gmail.com</A>&gt; wrote:
</I>&gt;<i>
</I>&gt;&gt;<i> Hi All,
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> I am Tim, the student working with AppSensor for this year's GSoC and
</I>&gt;&gt;<i> have been working closely with mentors on the project. Here is my weekly
</I>&gt;&gt;<i> update on my progress so far.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> If you have any ideas, questions or comments, please do join in!
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> ------------------------------------
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Report 7/06/2016
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Date:  7 June 2016
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Accomplishments in the past week: Improve Kafka and Spark integration as
</I>&gt;&gt;<i> well as learned how to run it within IntelliJ, Found out problem with Kafka
</I>&gt;&gt;<i> Connect in brew, Created custom JSON2CSV converter to pre-process file
</I>&gt;&gt;<i> before converting it to Vectors for Spark MLib rather than dumping whole
</I>&gt;&gt;<i> json string,
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Goals for the upcoming week: Use the JSON2CSV converter with Map Reduce
</I>&gt;&gt;<i> in Spark to be able to handle streaming JSON or use Spark SQL to generate
</I>&gt;&gt;<i> tables for this purpose, Run clustering algorithm with different settings
</I>&gt;&gt;<i> on the extracted features, Use interesting features from the result of
</I>&gt;&gt;<i> clustering and try to use at least one classification algorithm to do
</I>&gt;&gt;<i> analysis on the dataset.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Blockers (things preventing forward progress):
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Adjustments to project plan, if any:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Other: (use this space to document anything you learned, what worked
</I>&gt;&gt;<i> well, what didn't work well, etc.):
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    -
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    Close kafka servers before closing zookeeper. Otherwise, it will
</I>&gt;&gt;<i>    remain open and occupy a port.
</I>&gt;&gt;<i>    -
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    Brew install kafka for Mac - only installs and gradle builds core
</I>&gt;&gt;<i>    kafka - missing components like connect
</I>&gt;&gt;<i>    -
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    Source download for Kafka - Need to run gradle then ./gradlew to
</I>&gt;&gt;<i>    build jars required to run kafka (brew does this step automatically but
</I>&gt;&gt;<i>    other components are missing).
</I>&gt;&gt;<i>    -
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    Ensure any running application using the same consumer group is
</I>&gt;&gt;<i>    closed, otherwise it may seem like the consumer code in the app is not
</I>&gt;&gt;<i>    working but its actually being processed already (Can look at Zookeeper
</I>&gt;&gt;<i>    logs)
</I>&gt;&gt;<i>    -
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>    Can install a Spark plugin to run Spark apps in IntelliJ or Eclipse
</I>&gt;&gt;<i>    (will document in github repo).
</I>&gt;&gt;<i>    - Convert JSON coming from Kafka using custom JSON2CSV converter or
</I>&gt;&gt;<i>    Spark SQL to be able to use Word2Vec feature extraction in Spark.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> ------------------------------------
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Kind Regards,
</I>&gt;&gt;<i> Tim
</I>&gt;&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://lists.owasp.org/pipermail/owasp-appsensor-project/attachments/20160627/c51ce9a8/attachment-0001.html">http://lists.owasp.org/pipermail/owasp-appsensor-project/attachments/20160627/c51ce9a8/attachment-0001.html</A>&gt;
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001088.html">[Owasp-appsensor-project] AppSensor GSoC ML Weekly Summary
</A></li>
	
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1089">[ date ]</a>
              <a href="thread.html#1089">[ thread ]</a>
              <a href="subject.html#1089">[ subject ]</a>
              <a href="author.html#1089">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.owasp.org/mailman/listinfo/owasp-appsensor-project">More information about the Owasp-appsensor-project
mailing list</a><br>
</body></html>
