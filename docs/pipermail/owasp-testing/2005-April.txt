From julian.harty at btinternet.com  Sat Apr  9 17:45:56 2005
From: julian.harty at btinternet.com (Julian Harty @ BT)
Date: Sat, 9 Apr 2005 22:45:56 +0100
Subject: [OWASP-TESTING] Hello from a new member of the list
Message-ID: <002201c53d4d$84132330$6800a8c0@JULIANVAIO01>

I'm Julian Harty and have just joined the owasp-testing list. I'm hoping to be able to help work on the documentation, etc. for security testing. This work overlaps with other work I'm involved in related to the broad topic of non-functional software testing so I'm hoping there'll be some overlap where the material is applicable for both audiences.

Feel free to get in contact, I guess initially via this e-mail list - Daniel Cuthbert will also have my contact details...

All the best



Julian Harty
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.owasp.org/pipermail/owasp-testing/attachments/20050409/f590b8e3/attachment.html 

From daniel.cuthbert at owasp.org  Mon Apr 11 16:21:44 2005
From: daniel.cuthbert at owasp.org (Daniel Cuthbert)
Date: Mon, 11 Apr 2005 21:21:44 +0100
Subject: [OWASP-TESTING] finally!
Message-ID: <ba009ec5d78bc37d9c1f7bb6f2b2014e@owasp.org>

After months of nothing really happening,which was my fault so 
apologies there, we are now ready to kickstart the 2nd phase.
Recently i've subscribed a large amount of people to the list and i'm 
hoping we have more interaction this time around.

I have drawn up a schedule for this second phase and it looks like:

1st May: Final version of the 2nd phase outline
15th May: 2nd phase broken up into sections where everyone can choose 
which section they want to contribute
31st May: Work starts on sections
15th July: All sections completed and sent in to me, so i can add them 
all to the main document
1st Aug: Proof reading begins
1st Sep: Release OWASP Testing guide
1st Oct: Multilingual versions released

As you can see, i've not gone totally made with the timescale, and 
think its totally doable in that timeframe, given the amount of 
contributors we have onboard.

If you can all review the current outline and give me feedback, we can 
get this project going again

Daniel
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Testing_Guide_II_structure_v5.doc
Type: application/msword
Size: 48128 bytes
Desc: not available
Url : http://lists.owasp.org/pipermail/owasp-testing/attachments/20050411/aedc5f51/attachment.doc 

From daniel.cuthbert at owasp.org  Tue Apr 12 06:29:59 2005
From: daniel.cuthbert at owasp.org (Daniel Cuthbert)
Date: Tue, 12 Apr 2005 06:29:59 -0400 (EDT)
Subject: [OWASP-TESTING] previous document
Message-ID: <21228.212.32.48.101.1113301799.squirrel@webmail.owasp.org>

has anyone else had issues saving/viewing the document i attached in the
previous e-mail?




Daniel



From sroses at itdeusto.com  Tue Apr 12 07:42:43 2005
From: sroses at itdeusto.com (Simon Roses Femerling)
Date: Tue, 12 Apr 2005 13:42:43 +0200
Subject: [OWASP-TESTING] previous document
References: <21228.212.32.48.101.1113301799.squirrel@webmail.owasp.org>
Message-ID: <001601c53f54$bed351d0$4a0818ac@dis>

No problems at all viewing the doc.

Simon Roses Femerling
Consultor en Seguridad / IT Security Consultant
IT Deusto
http://www.itdeusto.com
Madrid, Spain
----- Original Message ----- 
From: "Daniel Cuthbert" <daniel.cuthbert at owasp.org>
To: <owasp-testing at lists.sourceforge.net>
Sent: Tuesday, April 12, 2005 12:29 PM
Subject: [OWASP-TESTING] previous document


> 
> has anyone else had issues saving/viewing the document i attached in the
> previous e-mail?
> 
> 
> 
> 
> Daniel
> 
> 
> -------------------------------------------------------
> SF email is sponsored by - The IT Product Guide
> Read honest & candid reviews on hundreds of IT Products from real users.
> Discover which products truly live up to the hype. Start reading now.
> http://ads.osdn.com/?ad_id=6595&alloc_id=14396&op=click
> _______________________________________________
> owasp-testing mailing list
> owasp-testing at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/owasp-testing
> 




From jfernandez at germinus.com  Wed Apr 13 05:07:51 2005
From: jfernandez at germinus.com (Javier Fernandez-Sanguino)
Date: Wed, 13 Apr 2005 11:07:51 +0200
Subject: [OWASP-TESTING] finally!
In-Reply-To: <ba009ec5d78bc37d9c1f7bb6f2b2014e@owasp.org>
References: <ba009ec5d78bc37d9c1f7bb6f2b2014e@owasp.org>
Message-ID: <425CE167.40109@germinus.com>

Daniel Cuthbert wrote:


> If you can all review the current outline and give me feedback, we can 
> get this project going again

Some comments:

- "Authentication":
	* Session token generation (are they generated by the web server 
software or by the application itself?)
	* Token storage? (if not marked as 'secure' a cookie will be stored 
on hard disk...)
	* Control for automated attacks? (i.e. does the authentication 
mechanism block remote systems that try a brute force attack against a 
simple user/password form?)
	* Authentication logging (i.e. is information stored of the number of 
failed attempts of a given user?_)
	* Authentication restrictions (i.e. same user cannot be logged on 
twice, no user can log on from a given location - using a restricted 
IP address space)

- Application Dos:
	* Misuse of CPU-intensive operations? (imagine a SQL query that draws 
from million of records and any user can execute it as whim, it might 
be legitimate, there's no flooding, but the web server cannot cope 
with say, 5 parallel requests)

- "Configuration Management Infrastructure":
	* Authentication back-ends (LDAP, DBMS, text files...)

- "Data protection":
	* Privileges granted from the web server to data backends (CMS or 
DBMS, many people use the admin users to connect both, i.e. 'sa' in MsSQL)

- Language-specific testing:
	* Related to SOAP services I wrote a while back some tests associated 
with XML-powered web services (for the OSSTM), it might be a little 
bit dated, but might be useful. It is attached.

	* Flash testing and applet analysis might involve de-compilation of 
the objects in order to determine what requests do they make or how do 
they store information.

- Analyzing Results
	* Maybe use CVSS to rate vulnerabilities? In order to classify 
threats it might be best to take into account the vulnerability itself 
(SQL injection) and the expertise need to make a successful attack (do 
you need internal information of the application or is it easy to 
determine what the application is doing through its errors?)

Regards

Javier
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: xml-tests.txt
Url: http://lists.owasp.org/pipermail/owasp-testing/attachments/20050413/195974c4/attachment.txt 

From mauro.bregolin at gmail.com  Fri Apr 15 08:58:01 2005
From: mauro.bregolin at gmail.com (Mauro Bregolin)
Date: Fri, 15 Apr 2005 14:58:01 +0200
Subject: [OWASP-TESTING] finally!
In-Reply-To: <425CE167.40109@germinus.com>
References: <ba009ec5d78bc37d9c1f7bb6f2b2014e@owasp.org>
	 <425CE167.40109@germinus.com>
Message-ID: <9f95393505041505587aafa2e6@mail.gmail.com>

A few comments of mine next - Mauro

First, what is the assumed point of view? Many areas may be performed
both black-box and white-box. Techniques will vary - just a trivial
example, black box testing to identify "backup files" left in the web
filesystem space amounts to do some sort of scanning, while if you go
white box and have access to the web server box, you can explore the
filesystem to spot those files. What is going to be the philosophy of
the guide? For example, if it will cover both scenarios (and in fact
it should, since we're talking about code review as well), for each
section it could separately detail black box techniques and white box
techniques.

- Configuration Management Infrastructure. The first items (Listening
HTTP ports, HTTP banner etc.) are part of a preliminary discovery
phase. I think  it would deserve its own section, sub topics are (I
assume black box in the following):
  - network services related to the applications (obviously this
includes HTTP(s) ports but other ports might be present as well in
some cases). This is akin to do some port scanning
  - http fingerprinting
  - other fingerprinting techniques; for example, trying to identify
web modules, technologies etc. by looking at URL filename extensions
(such as the obvious: .pl, .php, .exe... - and the less obvious, there
are dozen of weird extensions nowadays)
  - information leakage: looking for sensitive information on the
Internet which appear related to the target (which consists of: IP
address(es), dns name(s), application and corporate name and
information, etc.), via search engines (google...), newsgroups, news
portals, whois-like services, etc.
  - application architecture: trying to determine how the application
is structured; identify tiers (for example, balancers, web servers,
application servers, database etc.) and gather information about them
(IP, type/version etc.).

- what about google hacking? this is related to both what I called
"information leakage" above and scanning for known vulnerabilities
(though it is indirect...). In this area I guess it'd be appropriate
to look at the work being done by the folks at Sensepost (see wikto in
http://www.sensepost.com/garage_portal.html).



From eoin.keary at ie.fid-intl.com  Fri Apr 15 09:58:11 2005
From: eoin.keary at ie.fid-intl.com (Keary, Eoin)
Date: Fri, 15 Apr 2005 14:58:11 +0100
Subject: [OWASP-TESTING] finally!
Message-ID: <8C1B61DBB6DDA241B26E151434EA760F080E7962@irldub703nts.ie.fid-intl.com>

Personally we view whitebox as audit and blackbox as testing.
Audit we see, say, the source code and review if it conforms to internal
policy and best practice.
Testing is from a user perspective, what the user sees. No code exposed just
inputs and corresponding outputs.

Regarding port scanning and footprinting these are initial phases of a pen
test, the assessment phase. And it seems correct to cover assessment tasks
in their own section.
Information leakage is also a part of the assessment phase but is closely
related to the attack phase as a slight adjustment to the attack vector can
lead to an exploit.

Regarding patching and versions of appserver this is related to the "secure
code environment": this includes configuration and deployment, versioning,
administration policy and redundancy/failover.

-----Original Message-----
From: Mauro Bregolin [mailto:mauro.bregolin at gmail.com] 
Sent: Friday, April 15, 2005 1:58 PM
To: Daniel Cuthbert; owasp-testing at lists.sourceforge.net
Subject: Re: [OWASP-TESTING] finally!


A few comments of mine next - Mauro

First, what is the assumed point of view? Many areas may be performed
both black-box and white-box. Techniques will vary - just a trivial
example, black box testing to identify "backup files" left in the web
filesystem space amounts to do some sort of scanning, while if you go
white box and have access to the web server box, you can explore the
filesystem to spot those files. What is going to be the philosophy of
the guide? For example, if it will cover both scenarios (and in fact
it should, since we're talking about code review as well), for each
section it could separately detail black box techniques and white box
techniques.

- Configuration Management Infrastructure. The first items (Listening
HTTP ports, HTTP banner etc.) are part of a preliminary discovery
phase. I think  it would deserve its own section, sub topics are (I
assume black box in the following):
  - network services related to the applications (obviously this
includes HTTP(s) ports but other ports might be present as well in
some cases). This is akin to do some port scanning
  - http fingerprinting
  - other fingerprinting techniques; for example, trying to identify
web modules, technologies etc. by looking at URL filename extensions
(such as the obvious: .pl, .php, .exe... - and the less obvious, there
are dozen of weird extensions nowadays)
  - information leakage: looking for sensitive information on the
Internet which appear related to the target (which consists of: IP
address(es), dns name(s), application and corporate name and
information, etc.), via search engines (google...), newsgroups, news
portals, whois-like services, etc.
  - application architecture: trying to determine how the application
is structured; identify tiers (for example, balancers, web servers,
application servers, database etc.) and gather information about them
(IP, type/version etc.).

- what about google hacking? this is related to both what I called
"information leakage" above and scanning for known vulnerabilities
(though it is indirect...). In this area I guess it'd be appropriate
to look at the work being done by the folks at Sensepost (see wikto in
http://www.sensepost.com/garage_portal.html).


-------------------------------------------------------
SF email is sponsored by - The IT Product Guide
Read honest & candid reviews on hundreds of IT Products from real users.
Discover which products truly live up to the hype. Start reading now.
http://ads.osdn.com/?ad_ide95&alloc_id396&op=click
_______________________________________________
owasp-testing mailing list
owasp-testing at lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/owasp-testing



From mark.curphey at foundstone.com  Fri Apr 15 15:21:45 2005
From: mark.curphey at foundstone.com (Curphey, Mark)
Date: Fri, 15 Apr 2005 12:21:45 -0700
Subject: [OWASP-TESTING] finally!
Message-ID: <9DC8A3D37E31E043BD516142594BDDFA0F12F29A@MISSION.foundstone.com>

You might want to consider this carefully as this is in contradiction to
the way the software development community does (and has always seen)
testing. If you take this approach you may marginalize the product to
being only useful to security folks (and a subset at that). Just my 2
cents....

-----Original Message-----
From: owasp-testing-admin at lists.sourceforge.net
[mailto:owasp-testing-admin at lists.sourceforge.net] On Behalf Of Keary,
Eoin
Sent: Friday, April 15, 2005 6:58 AM
To: 'Mauro Bregolin'; Daniel Cuthbert;
owasp-testing at lists.sourceforge.net
Subject: RE: [OWASP-TESTING] finally!

Personally we view whitebox as audit and blackbox as testing.
Audit we see, say, the source code and review if it conforms to internal
policy and best practice.
Testing is from a user perspective, what the user sees. No code exposed
just
inputs and corresponding outputs.

Regarding port scanning and footprinting these are initial phases of a
pen
test, the assessment phase. And it seems correct to cover assessment
tasks
in their own section.
Information leakage is also a part of the assessment phase but is
closely
related to the attack phase as a slight adjustment to the attack vector
can
lead to an exploit.

Regarding patching and versions of appserver this is related to the
"secure
code environment": this includes configuration and deployment,
versioning,
administration policy and redundancy/failover.

-----Original Message-----
From: Mauro Bregolin [mailto:mauro.bregolin at gmail.com] 
Sent: Friday, April 15, 2005 1:58 PM
To: Daniel Cuthbert; owasp-testing at lists.sourceforge.net
Subject: Re: [OWASP-TESTING] finally!


A few comments of mine next - Mauro

First, what is the assumed point of view? Many areas may be performed
both black-box and white-box. Techniques will vary - just a trivial
example, black box testing to identify "backup files" left in the web
filesystem space amounts to do some sort of scanning, while if you go
white box and have access to the web server box, you can explore the
filesystem to spot those files. What is going to be the philosophy of
the guide? For example, if it will cover both scenarios (and in fact
it should, since we're talking about code review as well), for each
section it could separately detail black box techniques and white box
techniques.

- Configuration Management Infrastructure. The first items (Listening
HTTP ports, HTTP banner etc.) are part of a preliminary discovery
phase. I think  it would deserve its own section, sub topics are (I
assume black box in the following):
  - network services related to the applications (obviously this
includes HTTP(s) ports but other ports might be present as well in
some cases). This is akin to do some port scanning
  - http fingerprinting
  - other fingerprinting techniques; for example, trying to identify
web modules, technologies etc. by looking at URL filename extensions
(such as the obvious: .pl, .php, .exe... - and the less obvious, there
are dozen of weird extensions nowadays)
  - information leakage: looking for sensitive information on the
Internet which appear related to the target (which consists of: IP
address(es), dns name(s), application and corporate name and
information, etc.), via search engines (google...), newsgroups, news
portals, whois-like services, etc.
  - application architecture: trying to determine how the application
is structured; identify tiers (for example, balancers, web servers,
application servers, database etc.) and gather information about them
(IP, type/version etc.).

- what about google hacking? this is related to both what I called
"information leakage" above and scanning for known vulnerabilities
(though it is indirect...). In this area I guess it'd be appropriate
to look at the work being done by the folks at Sensepost (see wikto in
http://www.sensepost.com/garage_portal.html).


-------------------------------------------------------
SF email is sponsored by - The IT Product Guide
Read honest & candid reviews on hundreds of IT Products from real users.
Discover which products truly live up to the hype. Start reading now.
http://ads.osdn.com/?ad_ide95&alloc_id396&op=click
_______________________________________________
owasp-testing mailing list
owasp-testing at lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/owasp-testing


-------------------------------------------------------
SF email is sponsored by - The IT Product Guide
Read honest & candid reviews on hundreds of IT Products from real users.
Discover which products truly live up to the hype. Start reading now.
http://ads.osdn.com/?ad_id=6595&alloc_id=14396&op=click
_______________________________________________
owasp-testing mailing list
owasp-testing at lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/owasp-testing




From harinath.pudipeddi at softrel.org  Sat Apr 16 00:33:57 2005
From: harinath.pudipeddi at softrel.org (Harinath Pudipeddi)
Date: Sat, 16 Apr 2005 10:03:57 +0530
Subject: [OWASP-TESTING] finally!
In-Reply-To: <8C1B61DBB6DDA241B26E151434EA760F080E7962@irldub703nts.ie.fid-intl.com>
Message-ID: <000a01c5423d$872b3040$6401a8c0@Hari>

Hello Eoin,

I differ to agree with your first paragraph on Testing and Audit. Code
Audit and White Box are two different approaches to ensure Quality and
Stability of code. If you are seeing White box testing as audit for
code, then you are missing key ingredients in making your code "Error
Free". We have many white box testing tools in the market today. Also,
the approach for White box testing is quite different than auditing. 

Hari

-----Original Message-----
From: owasp-testing-admin at lists.sourceforge.net
[mailto:owasp-testing-admin at lists.sourceforge.net] On Behalf Of Keary,
Eoin
Sent: Friday, April 15, 2005 7:28 PM
To: 'Mauro Bregolin'; Daniel Cuthbert;
owasp-testing at lists.sourceforge.net
Subject: SPAM-LOW: RE: [OWASP-TESTING] finally!

Personally we view whitebox as audit and blackbox as testing.
Audit we see, say, the source code and review if it conforms to internal
policy and best practice.
Testing is from a user perspective, what the user sees. No code exposed
just
inputs and corresponding outputs.

Regarding port scanning and footprinting these are initial phases of a
pen
test, the assessment phase. And it seems correct to cover assessment
tasks
in their own section.
Information leakage is also a part of the assessment phase but is
closely
related to the attack phase as a slight adjustment to the attack vector
can
lead to an exploit.

Regarding patching and versions of appserver this is related to the
"secure
code environment": this includes configuration and deployment,
versioning,
administration policy and redundancy/failover.







From jeff.williams at aspectsecurity.com  Sat Apr 16 09:45:20 2005
From: jeff.williams at aspectsecurity.com (Jeff Williams)
Date: Sat, 16 Apr 2005 09:45:20 -0400
Subject: [OWASP-TESTING] finally!
References: <000a01c5423d$872b3040$6401a8c0@Hari>
Message-ID: <02c901c5428a$886a8710$0664a8c0@intranet.aspectsecurity.com>

I think we should keep the techniques (scanning, manual pentest, static 
analysis, manual code review) separate from the purpose (audit or test). 
There's some more on this in this thread on webappsec from a while back.

http://seclists.org/lists/webappsec/2005/Jan-Mar/0360.html

--Jeff

----- Original Message ----- 
From: "Harinath Pudipeddi" <harinath.pudipeddi at softrel.org>
To: "'Keary, Eoin'" <eoin.keary at ie.fid-intl.com>; "'Mauro Bregolin'" 
<mauro.bregolin at gmail.com>; "'Daniel Cuthbert'" <daniel.cuthbert at owasp.org>; 
<owasp-testing at lists.sourceforge.net>
Sent: Saturday, April 16, 2005 12:33 AM
Subject: RE: [OWASP-TESTING] finally!


> Hello Eoin,
>
> I differ to agree with your first paragraph on Testing and Audit. Code
> Audit and White Box are two different approaches to ensure Quality and
> Stability of code. If you are seeing White box testing as audit for
> code, then you are missing key ingredients in making your code "Error
> Free". We have many white box testing tools in the market today. Also,
> the approach for White box testing is quite different than auditing.
>
> Hari
>
> -----Original Message-----
> From: owasp-testing-admin at lists.sourceforge.net
> [mailto:owasp-testing-admin at lists.sourceforge.net] On Behalf Of Keary,
> Eoin
> Sent: Friday, April 15, 2005 7:28 PM
> To: 'Mauro Bregolin'; Daniel Cuthbert;
> owasp-testing at lists.sourceforge.net
> Subject: SPAM-LOW: RE: [OWASP-TESTING] finally!
>
> Personally we view whitebox as audit and blackbox as testing.
> Audit we see, say, the source code and review if it conforms to internal
> policy and best practice.
> Testing is from a user perspective, what the user sees. No code exposed
> just
> inputs and corresponding outputs.
>
> Regarding port scanning and footprinting these are initial phases of a
> pen
> test, the assessment phase. And it seems correct to cover assessment
> tasks
> in their own section.
> Information leakage is also a part of the assessment phase but is
> closely
> related to the attack phase as a slight adjustment to the attack vector
> can
> lead to an exploit.
>
> Regarding patching and versions of appserver this is related to the
> "secure
> code environment": this includes configuration and deployment,
> versioning,
> administration policy and redundancy/failover.
>
>
>
>
>
>
> -------------------------------------------------------
> SF email is sponsored by - The IT Product Guide
> Read honest & candid reviews on hundreds of IT Products from real users.
> Discover which products truly live up to the hype. Start reading now.
> http://ads.osdn.com/?ad_id=6595&alloc_id=14396&op=click
> _______________________________________________
> owasp-testing mailing list
> owasp-testing at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/owasp-testing 




From mauro.bregolin at gmail.com  Mon Apr 18 04:49:07 2005
From: mauro.bregolin at gmail.com (Mauro Bregolin)
Date: Mon, 18 Apr 2005 10:49:07 +0200
Subject: [OWASP-TESTING] finally!
In-Reply-To: <02c901c5428a$886a8710$0664a8c0@intranet.aspectsecurity.com>
Message-ID: <4263748b.77d0a1e5.6f56.fffff952@mx.gmail.com>

Jeff,

I agree with the posting you refer to. What do you mean exactly with "we
should keep the techniques (scanning, manual pentest, static analysis,
manual code review) separate from the purpose (audit or test)"?

Judging by how people replied to my original post, it appears there's not a
unified consensus right now.
Perhaps it is worth trying to synchronize everybody on this matter before
things get started?

Concerning port scanning, footprinting, etc., my point essentially is the
following. Of course they're part of a traditional pen test; and here we're
not talking about pen testing, but assessing a web app - which may or may
not be part of a pen test-like type of assignment. This meaning you might be
requested to "assess application X which you can access at name.domain",
without any explicit notion or expectation of port scanning and
fingerprinting any specific IT infrastructure. Nor is the testing guide
supposed to waste time on port scanning specifics, etc.
What I wanted to convey is that, in my opinion, there are a number of
activities whose purpose is that of "discovery", which should be emphasized.
In contrast with a pen test, in which the discovery process is well
characterized and can precede further analyses, discovery-related activities
in a web app assessment cannot be fully factorized at the beginning.
For example, determining architectural aspects of the application (such as
identifying components like web servers, application servers, DBMSs etc.)
might in part require information that you gather during the actual analysis
of the application (think of gleaning info from error messages or other
diagnostics).
My comment was more on the need of giving dignity to a set of separate
activities (or information collected), realizing that semantically they
fulfill the purpose of "discovering" key aspects of the application, than on
restructuring the testing guide TOC. Again, the difficulty lies in the fact
that you can't group them nicely, but logically there exist a number of
steps in the assessment process whose purpose is (directly or as a
by-product) related to "discovery", which I believe is worth emphasize.

Mauro


-----Original Message-----
From: Jeff Williams [mailto:jeff.williams at aspectsecurity.com] 
Sent: sabato 16 aprile 2005 15.45
To: Harinath Pudipeddi; 'Keary, Eoin'; 'Mauro Bregolin'; 'Daniel Cuthbert';
owasp-testing at lists.sourceforge.net
Subject: Re: [OWASP-TESTING] finally!

I think we should keep the techniques (scanning, manual pentest, static 
analysis, manual code review) separate from the purpose (audit or test). 
There's some more on this in this thread on webappsec from a while back.

http://seclists.org/lists/webappsec/2005/Jan-Mar/0360.html

--Jeff

----- Original Message ----- 
From: "Harinath Pudipeddi" <harinath.pudipeddi at softrel.org>
To: "'Keary, Eoin'" <eoin.keary at ie.fid-intl.com>; "'Mauro Bregolin'" 
<mauro.bregolin at gmail.com>; "'Daniel Cuthbert'" <daniel.cuthbert at owasp.org>;

<owasp-testing at lists.sourceforge.net>
Sent: Saturday, April 16, 2005 12:33 AM
Subject: RE: [OWASP-TESTING] finally!


> Hello Eoin,
>
> I differ to agree with your first paragraph on Testing and Audit. Code
> Audit and White Box are two different approaches to ensure Quality and
> Stability of code. If you are seeing White box testing as audit for
> code, then you are missing key ingredients in making your code "Error
> Free". We have many white box testing tools in the market today. Also,
> the approach for White box testing is quite different than auditing.
>
> Hari
>
> -----Original Message-----
> From: owasp-testing-admin at lists.sourceforge.net
> [mailto:owasp-testing-admin at lists.sourceforge.net] On Behalf Of Keary,
> Eoin
> Sent: Friday, April 15, 2005 7:28 PM
> To: 'Mauro Bregolin'; Daniel Cuthbert;
> owasp-testing at lists.sourceforge.net
> Subject: SPAM-LOW: RE: [OWASP-TESTING] finally!
>
> Personally we view whitebox as audit and blackbox as testing.
> Audit we see, say, the source code and review if it conforms to internal
> policy and best practice.
> Testing is from a user perspective, what the user sees. No code exposed
> just
> inputs and corresponding outputs.
>
> Regarding port scanning and footprinting these are initial phases of a
> pen
> test, the assessment phase. And it seems correct to cover assessment
> tasks
> in their own section.
> Information leakage is also a part of the assessment phase but is
> closely
> related to the attack phase as a slight adjustment to the attack vector
> can
> lead to an exploit.
>
> Regarding patching and versions of appserver this is related to the
> "secure
> code environment": this includes configuration and deployment,
> versioning,
> administration policy and redundancy/failover.
>
>
>
>
>
>
> -------------------------------------------------------
> SF email is sponsored by - The IT Product Guide
> Read honest & candid reviews on hundreds of IT Products from real users.
> Discover which products truly live up to the hype. Start reading now.
> http://ads.osdn.com/?ad_id=6595&alloc_id=14396&op=click
> _______________________________________________
> owasp-testing mailing list
> owasp-testing at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/owasp-testing 




From jfernandez at germinus.com  Mon Apr 18 05:13:18 2005
From: jfernandez at germinus.com (Javier Fernandez-Sanguino)
Date: Mon, 18 Apr 2005 11:13:18 +0200
Subject: [OWASP-TESTING] finally!
In-Reply-To: <4263748b.77d0a1e5.6f56.fffff952@mx.gmail.com>
References: <4263748b.77d0a1e5.6f56.fffff952@mx.gmail.com>
Message-ID: <42637A2E.5050805@germinus.com>

Mauro Bregolin wrote:

> Jeff,
> 
> I agree with the posting you refer to. What do you mean exactly with "we
> should keep the techniques (scanning, manual pentest, static analysis,
> manual code review) separate from the purpose (audit or test)"?
> 
> Judging by how people replied to my original post, it appears there's not a
> unified consensus right now.
> Perhaps it is worth trying to synchronize everybody on this matter before
> things get started?

Quite sincerely, I don't believe that talking semantics is useful at 
this stage. The previous version of the document, as well as the 
pentest checklist, already advanced what this document should be 
about. I think it would be best if we wrote content filling up the 
holes in the different chapters than discuss what each one's views on 
audit vs. testing are.

As for your mention on discovering information of the web app 
structure as you go along, please review what the OWASP checklist we 
wrote a while back says about this. That same content can be folded 
back in the OWASP Testing phase II.

Regards

Javier



From daniel.cuthbert at owasp.org  Mon Apr 18 04:07:02 2005
From: daniel.cuthbert at owasp.org (Daniel)
Date: Mon, 18 Apr 2005 04:07:02 -0400 (EDT)
Subject: [OWASP-TESTING] finally!
In-Reply-To: <4263748b.77d0a1e5.6f56.fffff952@mx.gmail.com>
References: <02c901c5428a$886a8710$0664a8c0@intranet.aspectsecurity.com>
    <4263748b.77d0a1e5.6f56.fffff952@mx.gmail.com>
Message-ID: <36661.212.32.48.101.1113811622.squirrel@webmail.owasp.org>

Initially i knew there would be a reaction to the basic pentest section i
added, but the reason i put it in there was to show the very basic
techniques performed at the start of any apptest (judging by the majority
of e-mails on the pentest/webappsec mailing list, i feel that a large
amount of people still dont understand how to do most of these tasks).

Yes this shouldnt be another hacking exposed guide, but i also feel there
is a need for us to explain the two main tasks performed during a web
application test..

There are two major parts to this guide (from where im standing)

Techniques:
Manual testing, scanning, basic pentest, static analysis etc
Audit: Code review, interviews with developers, business logic

WE need both of the above sections as they all make up a complete
application test in most, if not all, cases.

In my view, the audit/discovery phase is always first as without the
information gained from that phase, the manual section can be fruitless


Mauro Bregolin said:
> Jeff,
>
> I agree with the posting you refer to. What do you mean exactly with "we
> should keep the techniques (scanning, manual pentest, static analysis,
> manual code review) separate from the purpose (audit or test)"?
>
> Judging by how people replied to my original post, it appears there's not
> a
> unified consensus right now.
> Perhaps it is worth trying to synchronize everybody on this matter before
> things get started?
>
> Concerning port scanning, footprinting, etc., my point essentially is the
> following. Of course they're part of a traditional pen test; and here
> we're
> not talking about pen testing, but assessing a web app - which may or may
> not be part of a pen test-like type of assignment. This meaning you might
> be
> requested to "assess application X which you can access at name.domain",
> without any explicit notion or expectation of port scanning and
> fingerprinting any specific IT infrastructure. Nor is the testing guide
> supposed to waste time on port scanning specifics, etc.
> What I wanted to convey is that, in my opinion, there are a number of
> activities whose purpose is that of "discovery", which should be
> emphasized.
> In contrast with a pen test, in which the discovery process is well
> characterized and can precede further analyses, discovery-related
> activities
> in a web app assessment cannot be fully factorized at the beginning.
> For example, determining architectural aspects of the application (such as
> identifying components like web servers, application servers, DBMSs etc.)
> might in part require information that you gather during the actual
> analysis
> of the application (think of gleaning info from error messages or other
> diagnostics).
> My comment was more on the need of giving dignity to a set of separate
> activities (or information collected), realizing that semantically they
> fulfill the purpose of "discovering" key aspects of the application, than
> on
> restructuring the testing guide TOC. Again, the difficulty lies in the
> fact
> that you can't group them nicely, but logically there exist a number of
> steps in the assessment process whose purpose is (directly or as a
> by-product) related to "discovery", which I believe is worth emphasize.
>
> Mauro
>
>
> -----Original Message-----
> From: Jeff Williams [mailto:jeff.williams at aspectsecurity.com]
> Sent: sabato 16 aprile 2005 15.45
> To: Harinath Pudipeddi; 'Keary, Eoin'; 'Mauro Bregolin'; 'Daniel
> Cuthbert';
> owasp-testing at lists.sourceforge.net
> Subject: Re: [OWASP-TESTING] finally!
>
> I think we should keep the techniques (scanning, manual pentest, static
> analysis, manual code review) separate from the purpose (audit or test).
> There's some more on this in this thread on webappsec from a while back.
>
> http://seclists.org/lists/webappsec/2005/Jan-Mar/0360.html
>
> --Jeff
>
> ----- Original Message -----
> From: "Harinath Pudipeddi" <harinath.pudipeddi at softrel.org>
> To: "'Keary, Eoin'" <eoin.keary at ie.fid-intl.com>; "'Mauro Bregolin'"
> <mauro.bregolin at gmail.com>; "'Daniel Cuthbert'"
> <daniel.cuthbert at owasp.org>;
>
> <owasp-testing at lists.sourceforge.net>
> Sent: Saturday, April 16, 2005 12:33 AM
> Subject: RE: [OWASP-TESTING] finally!
>
>
>> Hello Eoin,
>>
>> I differ to agree with your first paragraph on Testing and Audit. Code
>> Audit and White Box are two different approaches to ensure Quality and
>> Stability of code. If you are seeing White box testing as audit for
>> code, then you are missing key ingredients in making your code "Error
>> Free". We have many white box testing tools in the market today. Also,
>> the approach for White box testing is quite different than auditing.
>>
>> Hari
>>
>> -----Original Message-----
>> From: owasp-testing-admin at lists.sourceforge.net
>> [mailto:owasp-testing-admin at lists.sourceforge.net] On Behalf Of Keary,
>> Eoin
>> Sent: Friday, April 15, 2005 7:28 PM
>> To: 'Mauro Bregolin'; Daniel Cuthbert;
>> owasp-testing at lists.sourceforge.net
>> Subject: SPAM-LOW: RE: [OWASP-TESTING] finally!
>>
>> Personally we view whitebox as audit and blackbox as testing.
>> Audit we see, say, the source code and review if it conforms to internal
>> policy and best practice.
>> Testing is from a user perspective, what the user sees. No code exposed
>> just
>> inputs and corresponding outputs.
>>
>> Regarding port scanning and footprinting these are initial phases of a
>> pen
>> test, the assessment phase. And it seems correct to cover assessment
>> tasks
>> in their own section.
>> Information leakage is also a part of the assessment phase but is
>> closely
>> related to the attack phase as a slight adjustment to the attack vector
>> can
>> lead to an exploit.
>>
>> Regarding patching and versions of appserver this is related to the
>> "secure
>> code environment": this includes configuration and deployment,
>> versioning,
>> administration policy and redundancy/failover.
>>
>>
>>
>>
>>
>>
>> -------------------------------------------------------
>> SF email is sponsored by - The IT Product Guide
>> Read honest & candid reviews on hundreds of IT Products from real users.
>> Discover which products truly live up to the hype. Start reading now.
>> http://ads.osdn.com/?ad_id=6595&alloc_id=14396&op=click
>> _______________________________________________
>> owasp-testing mailing list
>> owasp-testing at lists.sourceforge.net
>> https://lists.sourceforge.net/lists/listinfo/owasp-testing
>
>
>
> -------------------------------------------------------
> SF email is sponsored by - The IT Product Guide
> Read honest & candid reviews on hundreds of IT Products from real users.
> Discover which products truly live up to the hype. Start reading now.
> http://ads.osdn.com/?ad_id=6595&alloc_id=14396&op=click
> _______________________________________________
> owasp-testing mailing list
> owasp-testing at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/owasp-testing
>


Daniel



From daniel.cuthbert at owasp.org  Mon Apr 18 04:11:10 2005
From: daniel.cuthbert at owasp.org (Daniel)
Date: Mon, 18 Apr 2005 04:11:10 -0400 (EDT)
Subject: [OWASP-TESTING] finally!
In-Reply-To: <000a01c5423d$872b3040$6401a8c0@Hari>
References: 
    <8C1B61DBB6DDA241B26E151434EA760F080E7962@irldub703nts.ie.fid-intl.com>
    <000a01c5423d$872b3040$6401a8c0@Hari>
Message-ID: <36672.212.32.48.101.1113811870.squirrel@webmail.owasp.org>

I dont want the guide to have a detailed section on code auditing as there
are, as Harinath mentioned, loads of commercial and free tools which
perform the task.

What would be great is a section that explains how these tools work, maybe
gives some examples of dodgy code and also why this technique can yeild
some important information.

The testing guide, i think, should be aimed more towards the tester who
doesnt always have full access to the source code, hence more of a
blackbox testing guide
Harinath Pudipeddi said:
> Hello Eoin,
>
> I differ to agree with your first paragraph on Testing and Audit. Code
> Audit and White Box are two different approaches to ensure Quality and
> Stability of code. If you are seeing White box testing as audit for
> code, then you are missing key ingredients in making your code "Error
> Free". We have many white box testing tools in the market today. Also,
> the approach for White box testing is quite different than auditing.
>
> Hari
>
> -----Original Message-----
> From: owasp-testing-admin at lists.sourceforge.net
> [mailto:owasp-testing-admin at lists.sourceforge.net] On Behalf Of Keary,
> Eoin
> Sent: Friday, April 15, 2005 7:28 PM
> To: 'Mauro Bregolin'; Daniel Cuthbert;
> owasp-testing at lists.sourceforge.net
> Subject: SPAM-LOW: RE: [OWASP-TESTING] finally!
>
> Personally we view whitebox as audit and blackbox as testing.
> Audit we see, say, the source code and review if it conforms to internal
> policy and best practice.
> Testing is from a user perspective, what the user sees. No code exposed
> just
> inputs and corresponding outputs.
>
> Regarding port scanning and footprinting these are initial phases of a
> pen
> test, the assessment phase. And it seems correct to cover assessment
> tasks
> in their own section.
> Information leakage is also a part of the assessment phase but is
> closely
> related to the attack phase as a slight adjustment to the attack vector
> can
> lead to an exploit.
>
> Regarding patching and versions of appserver this is related to the
> "secure
> code environment": this includes configuration and deployment,
> versioning,
> administration policy and redundancy/failover.
>
>
>
>
>


Daniel



From daniel.cuthbert at owasp.org  Mon Apr 18 04:12:44 2005
From: daniel.cuthbert at owasp.org (Daniel)
Date: Mon, 18 Apr 2005 04:12:44 -0400 (EDT)
Subject: [OWASP-TESTING] finally!
In-Reply-To: 
     <9DC8A3D37E31E043BD516142594BDDFA0F12F29A@MISSION.foundstone.com>
References: 
    <9DC8A3D37E31E043BD516142594BDDFA0F12F29A@MISSION.foundstone.com>
Message-ID: <36677.212.32.48.101.1113811964.squirrel@webmail.owasp.org>

Very true and i'd rather the guide be aimed at anyone involved within the
application development process, than the security conslutants and pentest
scene.

Hence why i initially put in the section on basic pentesting techniques


Curphey, Mark said:
> You might want to consider this carefully as this is in contradiction to
> the way the software development community does (and has always seen)
> testing. If you take this approach you may marginalize the product to
> being only useful to security folks (and a subset at that). Just my 2
> cents....
>
> -----Original Message-----
> From: owasp-testing-admin at lists.sourceforge.net
> [mailto:owasp-testing-admin at lists.sourceforge.net] On Behalf Of Keary,
> Eoin
> Sent: Friday, April 15, 2005 6:58 AM
> To: 'Mauro Bregolin'; Daniel Cuthbert;
> owasp-testing at lists.sourceforge.net
> Subject: RE: [OWASP-TESTING] finally!
>
> Personally we view whitebox as audit and blackbox as testing.
> Audit we see, say, the source code and review if it conforms to internal
> policy and best practice.
> Testing is from a user perspective, what the user sees. No code exposed
> just
> inputs and corresponding outputs.
>
> Regarding port scanning and footprinting these are initial phases of a
> pen
> test, the assessment phase. And it seems correct to cover assessment
> tasks
> in their own section.
> Information leakage is also a part of the assessment phase but is
> closely
> related to the attack phase as a slight adjustment to the attack vector
> can
> lead to an exploit.
>
> Regarding patching and versions of appserver this is related to the
> "secure
> code environment": this includes configuration and deployment,
> versioning,
> administration policy and redundancy/failover.
>
> -----Original Message-----
> From: Mauro Bregolin [mailto:mauro.bregolin at gmail.com]
> Sent: Friday, April 15, 2005 1:58 PM
> To: Daniel Cuthbert; owasp-testing at lists.sourceforge.net
> Subject: Re: [OWASP-TESTING] finally!
>
>
> A few comments of mine next - Mauro
>
> First, what is the assumed point of view? Many areas may be performed
> both black-box and white-box. Techniques will vary - just a trivial
> example, black box testing to identify "backup files" left in the web
> filesystem space amounts to do some sort of scanning, while if you go
> white box and have access to the web server box, you can explore the
> filesystem to spot those files. What is going to be the philosophy of
> the guide? For example, if it will cover both scenarios (and in fact
> it should, since we're talking about code review as well), for each
> section it could separately detail black box techniques and white box
> techniques.
>
> - Configuration Management Infrastructure. The first items (Listening
> HTTP ports, HTTP banner etc.) are part of a preliminary discovery
> phase. I think  it would deserve its own section, sub topics are (I
> assume black box in the following):
>   - network services related to the applications (obviously this
> includes HTTP(s) ports but other ports might be present as well in
> some cases). This is akin to do some port scanning
>   - http fingerprinting
>   - other fingerprinting techniques; for example, trying to identify
> web modules, technologies etc. by looking at URL filename extensions
> (such as the obvious: .pl, .php, .exe... - and the less obvious, there
> are dozen of weird extensions nowadays)
>   - information leakage: looking for sensitive information on the
> Internet which appear related to the target (which consists of: IP
> address(es), dns name(s), application and corporate name and
> information, etc.), via search engines (google...), newsgroups, news
> portals, whois-like services, etc.
>   - application architecture: trying to determine how the application
> is structured; identify tiers (for example, balancers, web servers,
> application servers, database etc.) and gather information about them
> (IP, type/version etc.).
>
> - what about google hacking? this is related to both what I called
> "information leakage" above and scanning for known vulnerabilities
> (though it is indirect...). In this area I guess it'd be appropriate
> to look at the work being done by the folks at Sensepost (see wikto in
> http://www.sensepost.com/garage_portal.html).
>
>
> -------------------------------------------------------
> SF email is sponsored by - The IT Product Guide
> Read honest & candid reviews on hundreds of IT Products from real users.
> Discover which products truly live up to the hype. Start reading now.
> http://ads.osdn.com/?ad_ide95&alloc_id396&op=click
> _______________________________________________
> owasp-testing mailing list
> owasp-testing at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/owasp-testing
>
>
> -------------------------------------------------------
> SF email is sponsored by - The IT Product Guide
> Read honest & candid reviews on hundreds of IT Products from real users.
> Discover which products truly live up to the hype. Start reading now.
> http://ads.osdn.com/?ad_id=6595&alloc_id=14396&op=click
> _______________________________________________
> owasp-testing mailing list
> owasp-testing at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/owasp-testing
>
>


Daniel



From daniel.cuthbert at owasp.org  Mon Apr 18 04:20:30 2005
From: daniel.cuthbert at owasp.org (Daniel)
Date: Mon, 18 Apr 2005 04:20:30 -0400 (EDT)
Subject: [OWASP-TESTING] finally!
In-Reply-To: <42637A2E.5050805@germinus.com>
References: <4263748b.77d0a1e5.6f56.fffff952@mx.gmail.com>
    <42637A2E.5050805@germinus.com>
Message-ID: <36691.212.32.48.101.1113812430.squirrel@webmail.owasp.org>

I dont want us to get stuck in the audit vs. pentest argument as we need
to decide on the outline of the 2nd phase so we can get started on the
actual content.

The checklist is morphing into a more structured methodology (as talked
about at the recent owasp conference)
I'd love for the testing methodology to be scene as _the_ standard
methodology used when testing web applications (or traditional
applications to a degree)

Ill be working on the hows and when of this methodology alongside the
testing phase and hopefully we can keep the two projects in sync





Javier Fernandez-Sanguino said:
> Mauro Bregolin wrote:
>
>> Jeff,
>>
>> I agree with the posting you refer to. What do you mean exactly with "we
>> should keep the techniques (scanning, manual pentest, static analysis,
>> manual code review) separate from the purpose (audit or test)"?
>>
>> Judging by how people replied to my original post, it appears there's
>> not a
>> unified consensus right now.
>> Perhaps it is worth trying to synchronize everybody on this matter
>> before
>> things get started?
>
> Quite sincerely, I don't believe that talking semantics is useful at
> this stage. The previous version of the document, as well as the
> pentest checklist, already advanced what this document should be
> about. I think it would be best if we wrote content filling up the
> holes in the different chapters than discuss what each one's views on
> audit vs. testing are.
>
> As for your mention on discovering information of the web app
> structure as you go along, please review what the OWASP checklist we
> wrote a while back says about this. That same content can be folded
> back in the OWASP Testing phase II.
>
> Regards
>
> Javier
>
>
> -------------------------------------------------------
> SF email is sponsored by - The IT Product Guide
> Read honest & candid reviews on hundreds of IT Products from real users.
> Discover which products truly live up to the hype. Start reading now.
> http://ads.osdn.com/?ad_id=6595&alloc_id=14396&op=click
> _______________________________________________
> owasp-testing mailing list
> owasp-testing at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/owasp-testing
>


Daniel



From jeff.williams at aspectsecurity.com  Mon Apr 18 08:50:00 2005
From: jeff.williams at aspectsecurity.com (Jeff Williams)
Date: Mon, 18 Apr 2005 08:50:00 -0400
Subject: [OWASP-TESTING] finally!
References: <4263748b.77d0a1e5.6f56.fffff952@mx.gmail.com>
Message-ID: <016101c54415$2267ff10$0264a8c0@intranet.aspectsecurity.com>

> I agree with the posting you refer to. What do you mean exactly with "we
> should keep the techniques (scanning, manual pentest, static analysis,
> manual code review) separate from the purpose (audit or test)"?

When you use the word "audit" many people will think of a process by 
corporate entities to ensure compliance with some standard.  But several 
people in this discussion are using the word "audit" to mean code analysis.

I think our guide should focus on the pros and cons of the *techniques* and 
not worry about how people might use them (the purpose).  If people want to 
use our guide as part of a formal ISO17799 audit, great.

Can we agree (for our purposes here) that even if they're doing pure code 
review, they're using "test" techniques that are part of our "testing" 
guide.

--Jeff

>
> -----Original Message-----
> From: Jeff Williams [mailto:jeff.williams at aspectsecurity.com]
> Sent: sabato 16 aprile 2005 15.45
> To: Harinath Pudipeddi; 'Keary, Eoin'; 'Mauro Bregolin'; 'Daniel 
> Cuthbert';
> owasp-testing at lists.sourceforge.net
> Subject: Re: [OWASP-TESTING] finally!
>
> I think we should keep the techniques (scanning, manual pentest, static
> analysis, manual code review) separate from the purpose (audit or test).
> There's some more on this in this thread on webappsec from a while back.
>
> http://seclists.org/lists/webappsec/2005/Jan-Mar/0360.html
>
> --Jeff
>
> ----- Original Message ----- 
> From: "Harinath Pudipeddi" <harinath.pudipeddi at softrel.org>
> To: "'Keary, Eoin'" <eoin.keary at ie.fid-intl.com>; "'Mauro Bregolin'"
> <mauro.bregolin at gmail.com>; "'Daniel Cuthbert'" 
> <daniel.cuthbert at owasp.org>;
>
> <owasp-testing at lists.sourceforge.net>
> Sent: Saturday, April 16, 2005 12:33 AM
> Subject: RE: [OWASP-TESTING] finally!
>
>
>> Hello Eoin,
>>
>> I differ to agree with your first paragraph on Testing and Audit. Code
>> Audit and White Box are two different approaches to ensure Quality and
>> Stability of code. If you are seeing White box testing as audit for
>> code, then you are missing key ingredients in making your code "Error
>> Free". We have many white box testing tools in the market today. Also,
>> the approach for White box testing is quite different than auditing.
>>
>> Hari
>>
>> -----Original Message-----
>> From: owasp-testing-admin at lists.sourceforge.net
>> [mailto:owasp-testing-admin at lists.sourceforge.net] On Behalf Of Keary,
>> Eoin
>> Sent: Friday, April 15, 2005 7:28 PM
>> To: 'Mauro Bregolin'; Daniel Cuthbert;
>> owasp-testing at lists.sourceforge.net
>> Subject: SPAM-LOW: RE: [OWASP-TESTING] finally!
>>
>> Personally we view whitebox as audit and blackbox as testing.
>> Audit we see, say, the source code and review if it conforms to internal
>> policy and best practice.
>> Testing is from a user perspective, what the user sees. No code exposed
>> just
>> inputs and corresponding outputs.
>>
>> Regarding port scanning and footprinting these are initial phases of a
>> pen
>> test, the assessment phase. And it seems correct to cover assessment
>> tasks
>> in their own section.
>> Information leakage is also a part of the assessment phase but is
>> closely
>> related to the attack phase as a slight adjustment to the attack vector
>> can
>> lead to an exploit.
>>
>> Regarding patching and versions of appserver this is related to the
>> "secure
>> code environment": this includes configuration and deployment,
>> versioning,
>> administration policy and redundancy/failover.
>>
>>
>>
>>
>>
>>
>> -------------------------------------------------------
>> SF email is sponsored by - The IT Product Guide
>> Read honest & candid reviews on hundreds of IT Products from real users.
>> Discover which products truly live up to the hype. Start reading now.
>> http://ads.osdn.com/?ad_id=6595&alloc_id=14396&op=click
>> _______________________________________________
>> owasp-testing mailing list
>> owasp-testing at lists.sourceforge.net
>> https://lists.sourceforge.net/lists/listinfo/owasp-testing
> 




From eoin.keary at ie.fid-intl.com  Mon Apr 18 09:34:54 2005
From: eoin.keary at ie.fid-intl.com (Keary, Eoin)
Date: Mon, 18 Apr 2005 14:34:54 +0100
Subject: [OWASP-TESTING] finally!
Message-ID: <8C1B61DBB6DDA241B26E151434EA760F080E796C@irldub703nts.ie.fid-intl.com>

Grand,
Bair in mind that this is the view taken by my organization. We are large
enough to break the two disciplines in two.


-----Original Message-----
From: Curphey, Mark [mailto:mark.curphey at foundstone.com] 
Sent: Friday, April 15, 2005 8:22 PM
To: Keary, Eoin; Mauro Bregolin; Daniel Cuthbert;
owasp-testing at lists.sourceforge.net
Subject: RE: [OWASP-TESTING] finally!


You might want to consider this carefully as this is in contradiction to
the way the software development community does (and has always seen)
testing. If you take this approach you may marginalize the product to
being only useful to security folks (and a subset at that). Just my 2
cents....

-----Original Message-----
From: owasp-testing-admin at lists.sourceforge.net
[mailto:owasp-testing-admin at lists.sourceforge.net] On Behalf Of Keary,
Eoin
Sent: Friday, April 15, 2005 6:58 AM
To: 'Mauro Bregolin'; Daniel Cuthbert;
owasp-testing at lists.sourceforge.net
Subject: RE: [OWASP-TESTING] finally!

Personally we view whitebox as audit and blackbox as testing.
Audit we see, say, the source code and review if it conforms to internal
policy and best practice.
Testing is from a user perspective, what the user sees. No code exposed
just
inputs and corresponding outputs.

Regarding port scanning and footprinting these are initial phases of a
pen
test, the assessment phase. And it seems correct to cover assessment
tasks
in their own section.
Information leakage is also a part of the assessment phase but is
closely
related to the attack phase as a slight adjustment to the attack vector
can
lead to an exploit.

Regarding patching and versions of appserver this is related to the
"secure
code environment": this includes configuration and deployment,
versioning,
administration policy and redundancy/failover.

-----Original Message-----
From: Mauro Bregolin [mailto:mauro.bregolin at gmail.com] 
Sent: Friday, April 15, 2005 1:58 PM
To: Daniel Cuthbert; owasp-testing at lists.sourceforge.net
Subject: Re: [OWASP-TESTING] finally!


A few comments of mine next - Mauro

First, what is the assumed point of view? Many areas may be performed
both black-box and white-box. Techniques will vary - just a trivial
example, black box testing to identify "backup files" left in the web
filesystem space amounts to do some sort of scanning, while if you go
white box and have access to the web server box, you can explore the
filesystem to spot those files. What is going to be the philosophy of
the guide? For example, if it will cover both scenarios (and in fact
it should, since we're talking about code review as well), for each
section it could separately detail black box techniques and white box
techniques.

- Configuration Management Infrastructure. The first items (Listening
HTTP ports, HTTP banner etc.) are part of a preliminary discovery
phase. I think  it would deserve its own section, sub topics are (I
assume black box in the following):
  - network services related to the applications (obviously this
includes HTTP(s) ports but other ports might be present as well in
some cases). This is akin to do some port scanning
  - http fingerprinting
  - other fingerprinting techniques; for example, trying to identify
web modules, technologies etc. by looking at URL filename extensions
(such as the obvious: .pl, .php, .exe... - and the less obvious, there
are dozen of weird extensions nowadays)
  - information leakage: looking for sensitive information on the
Internet which appear related to the target (which consists of: IP
address(es), dns name(s), application and corporate name and
information, etc.), via search engines (google...), newsgroups, news
portals, whois-like services, etc.
  - application architecture: trying to determine how the application
is structured; identify tiers (for example, balancers, web servers,
application servers, database etc.) and gather information about them
(IP, type/version etc.).

- what about google hacking? this is related to both what I called
"information leakage" above and scanning for known vulnerabilities
(though it is indirect...). In this area I guess it'd be appropriate
to look at the work being done by the folks at Sensepost (see wikto in
http://www.sensepost.com/garage_portal.html).


-------------------------------------------------------
SF email is sponsored by - The IT Product Guide
Read honest & candid reviews on hundreds of IT Products from real users.
Discover which products truly live up to the hype. Start reading now.
http://ads.osdn.com/?ad_ide95&alloc_id396&op=click
_______________________________________________
owasp-testing mailing list
owasp-testing at lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/owasp-testing


-------------------------------------------------------
SF email is sponsored by - The IT Product Guide
Read honest & candid reviews on hundreds of IT Products from real users.
Discover which products truly live up to the hype. Start reading now.
http://ads.osdn.com/?ad_id=6595&alloc_id=14396&op=click
_______________________________________________
owasp-testing mailing list
owasp-testing at lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/owasp-testing



From daniel.cuthbert at owasp.org  Mon Apr 18 10:56:31 2005
From: daniel.cuthbert at owasp.org (Daniel)
Date: Mon, 18 Apr 2005 10:56:31 -0400 (EDT)
Subject: [OWASP-TESTING] call to others on this list
Message-ID: <40699.212.32.48.101.1113836191.squirrel@webmail.owasp.org>

I understand that everyone is busy with normal life/work etc, but so far
i've only seen the regulars offering their feedback on the 2nd phase.

Please, if you are subscribed to this list, offer your experience and
thoughts on the second phase and get involved.

sorry to be a nag

Daniel



From stephen.venter at gmail.com  Mon Apr 18 13:11:37 2005
From: stephen.venter at gmail.com (Stephen Venter)
Date: Mon, 18 Apr 2005 18:11:37 +0100
Subject: [OWASP-TESTING] Ideas for WAVA Testing Overview & Procedures Checklist documents
Message-ID: <39a4a99c050418101140efa501@mail.gmail.com>

Hi all

I'd like to I put forward another concept for consideration here. As
most people have been mentioning, there are two different perspectives
that you (as the application tester) normally adopt when you perform
your testing activities against the target system: black-box and
white-box.

However, I feel there is a need to consider that perhaps these
perspectives (coined when security testing was all about
infrastructure reviewing / pentesting) don't adequately reflect /
encompass the approaches adopted during Web Application security
reviewing (and here's another phrase that a previous colleague of mine
came up with which I feel might be more appropriate, to help get away
from the word "pentest": Web Application Vulnerability Assessment, or
WAVA).

My suggestion is that we consider defining an additional perspective:
"Translucent box" [or whatever name might be more appropriate here ? I
was just trying to find one that sort of conveyed the concept of
fitting in-between black & white]), and that some or all of the actual
testing techniques, might be appropriate (and some not) depending on
which point of view you are approaching the application from ? and off
course, some testing procedures may need to be performed differently
depending on this perspective.

These testing perspective definitions would be best suited to being
inclusions into the OWASPTesting_PhaseOne document (although I would
like to offer an alternative name for that too: something like the
"Testing Overview" or "approach" or "guideline document", because, as
far as I see it, it actually covers a number of phases not just one),
and then the Web App Testing Checklist table could be expanded to
include a column for specifics about the testing phases from the
Overview document where the tests are appropriate, or should be
customised, etc And then the appendices to the checklist could be the
place to include specific examples of how to carry out those tests and
what the expected results are, etc.

I feel it would be appropriate to define the testing perspectives
something like:
1.	Black box - from the perspective of an anonymous, unauthenticated
user, with minimal knowledge about the target system
2.	Translucent box [or whatever name might be more appropriate here ?
I was just trying to find one that sort of conveyed the concept of
fitting in-between black & white] ? from the perspective of a "normal"
application user, where they have a login account (through whatever
means a "normal" user of the system gets assigned an "authorised"
login to it)
3.	White box ? from the perspective of a person with full system
access, and thus full access to the application code and servers, etc
[typically the level of access the system admin, application author,
system auditor might have]

And then you can discuss how each of the testing procedures /
techniques / steps fit in with each of the above perspectives,
together with how they might or might not be appropriate in the
different testing perspectives, or might need to be modified to suit
the particular perspective.

In each perspective there are a certain amount of activities to be
performed that fall into the common class of categories like:
?	Planning
?	Information gathering
?	Target identification
?	Service enumeration & service information mining
?	Automated testing procedures
?	Manual testing procedures
?	Feedback / report writing

So, for example, in the traditional black box pentest, the pentester
might just be given a server name or IP address as the target (i.e.
the anonymous user approach, with minimal information up front), and
he will be required to go through the usual testing steps like:
?	Planning: understanding the security significance of the target and
why someone might want to compromise it;
?	Information gathering: querying public resources, whois records,
mailing lists, for information that may help, etc;
?	Target identification: ping sweeps, icmp vs tcp, etc
?	Service enumeration (portscans) & service information mining
(fingerprinting, harvesting service version info from what is returned
when you connect to them, etc)
?	Automated testing using vulnerability scanners
?	Manual testing procedures (weed out false-positives, perform
additional  checks to compliment the automated scanners, etc)
?	Feedback / report writing

In an application security vulnerability assessment (WAVA) scenario,
the black box approach could be where the tester has been given the
specific URL or web server address only. Again the testing techniques
/ steps / procedures might include:
?	Planning: understanding the security significance of the target and
why someone might want to compromise it;
?	Information gathering: querying public resources, connecting to the
server with a browser and reading what information is presented, etc;
?	Target identification: HTTP vs HTTPS ports, SSL certificate
information, a lot of the work here could be considered more
traditional infrastructure pentest work, and sometimes we need to be
wary of being drawn too much into this when the primary target is the
web app, not necessarily the infrastructure [and yes, of course the
infrastructure needs to be secure, which is what pure pentests are
there for?]
?	Service enumeration & service information mining ? observing what
web application specific information is obtainable, noting the input
fields and variables, including hidden fields, client side scripting,
etc.
?	Manual testing procedures ? I'd say that an application tester is
likely to start with manual testing, and only get on to automated
testing when he has a better understanding of what functions are
performed by the application - testing areas like user authentication
processes (session manipulation / user escalation), input validation,
code injection, etc.
?	Automated testing ? like sampling session management values (e.g.
cookie) to assess for predictability, brute force password guessing,
code injection testing of input fields (i.e. SQL injection, etc)
?	Feedback / report writing

In a pentesting situation, translucent box testing might be testing
from the position of the DMZ or corporate network, where you have
access behind firewalls, etc. Not quite administrative access, but
closer to the level of access that a normal (a.k.a. "corporate") user
might have.

With the application (WAVA) tester, the translucent box testing can be
seen to be where the tester is given example user accounts for users
of different levels of privilege (from the application's perspective,
not the operating system perspective!). And then your manual testing
activities take on a whole new angle where you attempt to see if you
can call admin level menu functions while only logged in as a
low-privileged user, see how cross-site scripting might allow a
low-privileged user to escalate their privileges to those of the admin
level user, or even to gain unauthorised access to records of other
low-privileged users, etc.  For the translucent box perspective you
can take the manual testing procedures phase (as just one of the items
on its own) and expand it into sub categories, like:
?	User Authentication and Authorisation mechanisms - test a user's
ability to perform unauthorised tasks such as: Access data or
resources that they are not entitled to; Escalate their current
privileges; Transact as another user; Access resources after logoff /
session timeouts; etc
?	Access Protection controls - test access protection mechanisms over
system resources, e.g. testing if a user can: Gain unauthorised access
to system files, data or other resources; Circumvent the access
protection controls using atypical methods such as alternate character
sets, or URL munging / manipulation / fuzzing, etc.
?	Data Validation - test the application for its vulnerability to data
injection and insertion techniques (buffer overflows, string
formatting problems, etc), i.e. test if the application: Correctly
parses any data inputted by the user; Performs adequate bounds
checking; Performs adequate validation of any data inputs; etc
?	Session Management - assess whether a user can: Steal or highjack
the credentials or session of another user; Change their current
identity to that of another user (account or role "hopping");
Manipulate or falsify the session management controls for any other
means; etc

Also, while doing translucent box testing, the WAVA tester will gather
new information that would be useful to try from the black-box WAVA
perspective ? like trying to call internal web app functions before
you are authenticated with the server (e.g. I have encountered
situations where simply connecting to the login page / function and
being issued with a session ID / cookie allowed me to call a function
that lists the transaction record ? in other words the authorisation
procedures were not being invoked when that function was called, all
that it looked at was that a session ID in the correct format was
included within the user's POST request?)

And then you get on to the white-box testing. 

In the pentest situation, I'd be more inclined to call this the
infrastructure hardening review / audit ? with admin level access you
review operating system settings and installed applications; IT
department organisation reviews; change control procedures; disaster
recovery planning; and all the other normal audit / compliance
objectives?

The white-box testing in the WAVA situation could be where you include
categories of activities like code reviews / audits, architectural
design reviews, interviewing developers, threat modelling, DRP, change
control procedures, etc

What do you think?

Regards,
Steve

From jeff.williams at owasp.org  Mon Apr 18 13:35:54 2005
From: jeff.williams at owasp.org (Jeff Williams)
Date: Mon, 18 Apr 2005 13:35:54 -0400
Subject: [OWASP-TESTING] Ideas for WAVA Testing Overview & Procedures Checklist documents
References: <39a4a99c050418101140efa501@mail.gmail.com>
Message-ID: <020501c5443d$12f90290$0264a8c0@intranet.aspectsecurity.com>

I like the concept of a WAVA, as it does get away from the techniques that 
get used.  But I don't want to encourage anyone to think that a black box 
penetration test is a good way to get secure.  You don't learn anything 
useful from this kind of security testing.  And I don't think it's a good 
idea to promote the idea that you go from black-box to white-box.

I'd like to see us go in a direction where the testing guide encourages the 
use of the most cost-effective technique for detecting each type of problem 
(taking into account the particulars of the application being analyzed).

So, for example, if I think the most cost-effective way to find SQL 
injection in a particular application to use static analysis, I break out my 
favorite tool and find all the database accesses.  If there's one that looks 
like a hole, I might use WebScarab to demonstrate it to the customer (lots 
of value in this).  On that same WAVA, I might also use scanners and some 
manual code review to find problems.  I use the most cost-effective 
technique where it makes sense for the current application.

That's what makes sense to me.

--Jeff

----- Original Message ----- 
From: "Stephen Venter" <stephen.venter at gmail.com>
To: <owasp-testing at lists.sourceforge.net>
Sent: Monday, April 18, 2005 1:11 PM
Subject: [OWASP-TESTING] Ideas for WAVA Testing Overview & Procedures 
Checklist documents


> Hi all
>
> I'd like to I put forward another concept for consideration here. As
> most people have been mentioning, there are two different perspectives
> that you (as the application tester) normally adopt when you perform
> your testing activities against the target system: black-box and
> white-box.
>
> However, I feel there is a need to consider that perhaps these
> perspectives (coined when security testing was all about
> infrastructure reviewing / pentesting) don't adequately reflect /
> encompass the approaches adopted during Web Application security
> reviewing (and here's another phrase that a previous colleague of mine
> came up with which I feel might be more appropriate, to help get away
> from the word "pentest": Web Application Vulnerability Assessment, or
> WAVA).
>
> My suggestion is that we consider defining an additional perspective:
> "Translucent box" [or whatever name might be more appropriate here ? I
> was just trying to find one that sort of conveyed the concept of
> fitting in-between black & white]), and that some or all of the actual
> testing techniques, might be appropriate (and some not) depending on
> which point of view you are approaching the application from ? and off
> course, some testing procedures may need to be performed differently
> depending on this perspective.
>
> These testing perspective definitions would be best suited to being
> inclusions into the OWASPTesting_PhaseOne document (although I would
> like to offer an alternative name for that too: something like the
> "Testing Overview" or "approach" or "guideline document", because, as
> far as I see it, it actually covers a number of phases not just one),
> and then the Web App Testing Checklist table could be expanded to
> include a column for specifics about the testing phases from the
> Overview document where the tests are appropriate, or should be
> customised, etc And then the appendices to the checklist could be the
> place to include specific examples of how to carry out those tests and
> what the expected results are, etc.
>
> I feel it would be appropriate to define the testing perspectives
> something like:
> 1. Black box - from the perspective of an anonymous, unauthenticated
> user, with minimal knowledge about the target system
> 2. Translucent box [or whatever name might be more appropriate here ?
> I was just trying to find one that sort of conveyed the concept of
> fitting in-between black & white] ? from the perspective of a "normal"
> application user, where they have a login account (through whatever
> means a "normal" user of the system gets assigned an "authorised"
> login to it)
> 3. White box ? from the perspective of a person with full system
> access, and thus full access to the application code and servers, etc
> [typically the level of access the system admin, application author,
> system auditor might have]
>
> And then you can discuss how each of the testing procedures /
> techniques / steps fit in with each of the above perspectives,
> together with how they might or might not be appropriate in the
> different testing perspectives, or might need to be modified to suit
> the particular perspective.
>
> In each perspective there are a certain amount of activities to be
> performed that fall into the common class of categories like:
> ? Planning
> ? Information gathering
> ? Target identification
> ? Service enumeration & service information mining
> ? Automated testing procedures
> ? Manual testing procedures
> ? Feedback / report writing
>
> So, for example, in the traditional black box pentest, the pentester
> might just be given a server name or IP address as the target (i.e.
> the anonymous user approach, with minimal information up front), and
> he will be required to go through the usual testing steps like:
> ? Planning: understanding the security significance of the target and
> why someone might want to compromise it;
> ? Information gathering: querying public resources, whois records,
> mailing lists, for information that may help, etc;
> ? Target identification: ping sweeps, icmp vs tcp, etc
> ? Service enumeration (portscans) & service information mining
> (fingerprinting, harvesting service version info from what is returned
> when you connect to them, etc)
> ? Automated testing using vulnerability scanners
> ? Manual testing procedures (weed out false-positives, perform
> additional  checks to compliment the automated scanners, etc)
> ? Feedback / report writing
>
> In an application security vulnerability assessment (WAVA) scenario,
> the black box approach could be where the tester has been given the
> specific URL or web server address only. Again the testing techniques
> / steps / procedures might include:
> ? Planning: understanding the security significance of the target and
> why someone might want to compromise it;
> ? Information gathering: querying public resources, connecting to the
> server with a browser and reading what information is presented, etc;
> ? Target identification: HTTP vs HTTPS ports, SSL certificate
> information, a lot of the work here could be considered more
> traditional infrastructure pentest work, and sometimes we need to be
> wary of being drawn too much into this when the primary target is the
> web app, not necessarily the infrastructure [and yes, of course the
> infrastructure needs to be secure, which is what pure pentests are
> there for?]
> ? Service enumeration & service information mining ? observing what
> web application specific information is obtainable, noting the input
> fields and variables, including hidden fields, client side scripting,
> etc.
> ? Manual testing procedures ? I'd say that an application tester is
> likely to start with manual testing, and only get on to automated
> testing when he has a better understanding of what functions are
> performed by the application - testing areas like user authentication
> processes (session manipulation / user escalation), input validation,
> code injection, etc.
> ? Automated testing ? like sampling session management values (e.g.
> cookie) to assess for predictability, brute force password guessing,
> code injection testing of input fields (i.e. SQL injection, etc)
> ? Feedback / report writing
>
> In a pentesting situation, translucent box testing might be testing
> from the position of the DMZ or corporate network, where you have
> access behind firewalls, etc. Not quite administrative access, but
> closer to the level of access that a normal (a.k.a. "corporate") user
> might have.
>
> With the application (WAVA) tester, the translucent box testing can be
> seen to be where the tester is given example user accounts for users
> of different levels of privilege (from the application's perspective,
> not the operating system perspective!). And then your manual testing
> activities take on a whole new angle where you attempt to see if you
> can call admin level menu functions while only logged in as a
> low-privileged user, see how cross-site scripting might allow a
> low-privileged user to escalate their privileges to those of the admin
> level user, or even to gain unauthorised access to records of other
> low-privileged users, etc.  For the translucent box perspective you
> can take the manual testing procedures phase (as just one of the items
> on its own) and expand it into sub categories, like:
> ? User Authentication and Authorisation mechanisms - test a user's
> ability to perform unauthorised tasks such as: Access data or
> resources that they are not entitled to; Escalate their current
> privileges; Transact as another user; Access resources after logoff /
> session timeouts; etc
> ? Access Protection controls - test access protection mechanisms over
> system resources, e.g. testing if a user can: Gain unauthorised access
> to system files, data or other resources; Circumvent the access
> protection controls using atypical methods such as alternate character
> sets, or URL munging / manipulation / fuzzing, etc.
> ? Data Validation - test the application for its vulnerability to data
> injection and insertion techniques (buffer overflows, string
> formatting problems, etc), i.e. test if the application: Correctly
> parses any data inputted by the user; Performs adequate bounds
> checking; Performs adequate validation of any data inputs; etc
> ? Session Management - assess whether a user can: Steal or highjack
> the credentials or session of another user; Change their current
>identity to that of another user (account or role "hopping");
> Manipulate or falsify the session management controls for any other
> means; etc
>
> Also, while doing translucent box testing, the WAVA tester will gather
> new information that would be useful to try from the black-box WAVA
> perspective ? like trying to call internal web app functions before
> you are authenticated with the server (e.g. I have encountered
> situations where simply connecting to the login page / function and
> being issued with a session ID / cookie allowed me to call a function
> that lists the transaction record ? in other words the authorisation
> procedures were not being invoked when that function was called, all
> that it looked at was that a session ID in the correct format was
> included within the user's POST request?)
>
> And then you get on to the white-box testing.
>
> In the pentest situation, I'd be more inclined to call this the
> infrastructure hardening review / audit ? with admin level access you
> review operating system settings and installed applications; IT
> department organisation reviews; change control procedures; disaster
> recovery planning; and all the other normal audit / compliance
> objectives?
>
> The white-box testing in the WAVA situation could be where you include
> categories of activities like code reviews / audits, architectural
> design reviews, interviewing developers, threat modelling, DRP, change
> control procedures, etc
>
> What do you think?
>
> Regards,
> Steve
> Hj??yNLvyvzjv???v!???jz???????z??J?v????ri???rjz?????????~z????q?z????jz 




From stephen.venter at gmail.com  Mon Apr 18 16:32:39 2005
From: stephen.venter at gmail.com (Stephen Venter)
Date: Mon, 18 Apr 2005 21:32:39 +0100
Subject: [OWASP-TESTING] Ideas for WAVA Testing Overview & Procedures Checklist documents
In-Reply-To: <020501c5443d$12f90290$0264a8c0@intranet.aspectsecurity.com>
References: <39a4a99c050418101140efa501@mail.gmail.com>
	 <020501c5443d$12f90290$0264a8c0@intranet.aspectsecurity.com>
Message-ID: <39a4a99c05041813322e598b8d@mail.gmail.com>

Hi Jeff

I don't mean my ideas to be taken as prescriptive in terms of which
testing approaches or tools you choose, or how you choose to perform
or use them.

I fully agree with you. When it comes down to it, it is our duty as
professionals in the field to choose the most efficient and effective
techniques and technologies to achieve those goals.

I certainly would never encourage anyone to believe that there's only
one way to tackle the problem of trying to identify and eradicate
vulnerabilities.

My main aim is to get people to think of web application security more
as a "three dimensional" problem, instead of just a "two dimensional"
one.

Perhaps the use of any term containing the word "box" in it is NOT
even appropriate here for the WAVA. Perhaps my ideas would be better
summarised as:

?	The perspectives you SHOULD consider adopting when you plan to do a
vulnerability assessment:
o	Unauthorised, anonymous user
o	Authorised normal user
o	Auditor / Super user  - Full access to all source code, systems,
documentation, policies, procedures, etc

?	And the security assessment phases to be applied to whichever (or
ALL) of those approaches you adopt in testing your system:

o	Planning
o	Information gathering
o	Target identification
o	Service enumeration & service information mining
o	Automated testing procedures
o	Manual testing procedures
o	Feedback / report writing
o	(Others? Different wording for these?)

None of this should be performed in isolation.  Information gained
from one testing approach / perspective SHOULD feed into the other
perspectives to ensure that all the angles are covered.

Regards
Steve

On 4/18/05, Jeff Williams <jeff.williams at owasp.org> wrote:
> I like the concept of a WAVA, as it does get away from the techniques that
> get used.  But I don't want to encourage anyone to think that a black box
> penetration test is a good way to get secure.  You don't learn anything
> useful from this kind of security testing.  And I don't think it's a good
> idea to promote the idea that you go from black-box to white-box.
> 
> I'd like to see us go in a direction where the testing guide encourages the
> use of the most cost-effective technique for detecting each type of problem
> (taking into account the particulars of the application being analyzed).
> 
> So, for example, if I think the most cost-effective way to find SQL
> injection in a particular application to use static analysis, I break out my
> favorite tool and find all the database accesses.  If there's one that looks
> like a hole, I might use WebScarab to demonstrate it to the customer (lots
> of value in this).  On that same WAVA, I might also use scanners and some
> manual code review to find problems.  I use the most cost-effective
> technique where it makes sense for the current application.
> 
> That's what makes sense to me.
> 
> --Jeff
> 
> ----- Original Message -----
> From: "Stephen Venter" <stephen.venter at gmail.com>
> To: <owasp-testing at lists.sourceforge.net>
> Sent: Monday, April 18, 2005 1:11 PM
> Subject: [OWASP-TESTING] Ideas for WAVA Testing Overview & Procedures
> Checklist documents
> 
> > Hi all
> >
> > I'd like to I put forward another concept for consideration here. As
> > most people have been mentioning, there are two different perspectives
> > that you (as the application tester) normally adopt when you perform
> > your testing activities against the target system: black-box and
> > white-box.
> >
> > However, I feel there is a need to consider that perhaps these
> > perspectives (coined when security testing was all about
> > infrastructure reviewing / pentesting) don't adequately reflect /
> > encompass the approaches adopted during Web Application security
> > reviewing (and here's another phrase that a previous colleague of mine
> > came up with which I feel might be more appropriate, to help get away
> > from the word "pentest": Web Application Vulnerability Assessment, or
> > WAVA).
> >
> > My suggestion is that we consider defining an additional perspective:
> > "Translucent box" [or whatever name might be more appropriate here ? I
> > was just trying to find one that sort of conveyed the concept of
> > fitting in-between black & white]), and that some or all of the actual
> > testing techniques, might be appropriate (and some not) depending on
> > which point of view you are approaching the application from ? and off
> > course, some testing procedures may need to be performed differently
> > depending on this perspective.
> >
> > These testing perspective definitions would be best suited to being
> > inclusions into the OWASPTesting_PhaseOne document (although I would
> > like to offer an alternative name for that too: something like the
> > "Testing Overview" or "approach" or "guideline document", because, as
> > far as I see it, it actually covers a number of phases not just one),
> > and then the Web App Testing Checklist table could be expanded to
> > include a column for specifics about the testing phases from the
> > Overview document where the tests are appropriate, or should be
> > customised, etc And then the appendices to the checklist could be the
> > place to include specific examples of how to carry out those tests and
> > what the expected results are, etc.
> >
> > I feel it would be appropriate to define the testing perspectives
> > something like:
> > 1. Black box - from the perspective of an anonymous, unauthenticated
> > user, with minimal knowledge about the target system
> > 2. Translucent box [or whatever name might be more appropriate here ?
> > I was just trying to find one that sort of conveyed the concept of
> > fitting in-between black & white] ? from the perspective of a "normal"
> > application user, where they have a login account (through whatever
> > means a "normal" user of the system gets assigned an "authorised"
> > login to it)
> > 3. White box ? from the perspective of a person with full system
> > access, and thus full access to the application code and servers, etc
> > [typically the level of access the system admin, application author,
> > system auditor might have]
> >
> > And then you can discuss how each of the testing procedures /
> > techniques / steps fit in with each of the above perspectives,
> > together with how they might or might not be appropriate in the
> > different testing perspectives, or might need to be modified to suit
> > the particular perspective.
> >
> > In each perspective there are a certain amount of activities to be
> > performed that fall into the common class of categories like:
> > ? Planning
> > ? Information gathering
> > ? Target identification
> > ? Service enumeration & service information mining
> > ? Automated testing procedures
> > ? Manual testing procedures
> > ? Feedback / report writing
> >
> > So, for example, in the traditional black box pentest, the pentester
> > might just be given a server name or IP address as the target (i.e.
> > the anonymous user approach, with minimal information up front), and
> > he will be required to go through the usual testing steps like:
> > ? Planning: understanding the security significance of the target and
> > why someone might want to compromise it;
> > ? Information gathering: querying public resources, whois records,
> > mailing lists, for information that may help, etc;
> > ? Target identification: ping sweeps, icmp vs tcp, etc
> > ? Service enumeration (portscans) & service information mining
> > (fingerprinting, harvesting service version info from what is returned
> > when you connect to them, etc)
> > ? Automated testing using vulnerability scanners
> > ? Manual testing procedures (weed out false-positives, perform
> > additional  checks to compliment the automated scanners, etc)
> > ? Feedback / report writing
> >
> > In an application security vulnerability assessment (WAVA) scenario,
> > the black box approach could be where the tester has been given the
> > specific URL or web server address only. Again the testing techniques
> > / steps / procedures might include:
> > ? Planning: understanding the security significance of the target and
> > why someone might want to compromise it;
> > ? Information gathering: querying public resources, connecting to the
> > server with a browser and reading what information is presented, etc;
> > ? Target identification: HTTP vs HTTPS ports, SSL certificate
> > information, a lot of the work here could be considered more
> > traditional infrastructure pentest work, and sometimes we need to be
> > wary of being drawn too much into this when the primary target is the
> > web app, not necessarily the infrastructure [and yes, of course the
> > infrastructure needs to be secure, which is what pure pentests are
> > there for?]
> > ? Service enumeration & service information mining ? observing what
> > web application specific information is obtainable, noting the input
> > fields and variables, including hidden fields, client side scripting,
> > etc.
> > ? Manual testing procedures ? I'd say that an application tester is
> > likely to start with manual testing, and only get on to automated
> > testing when he has a better understanding of what functions are
> > performed by the application - testing areas like user authentication
> > processes (session manipulation / user escalation), input validation,
> > code injection, etc.
> > ? Automated testing ? like sampling session management values (e.g.
> > cookie) to assess for predictability, brute force password guessing,
> > code injection testing of input fields (i.e. SQL injection, etc)
> > ? Feedback / report writing
> >
> > In a pentesting situation, translucent box testing might be testing
> > from the position of the DMZ or corporate network, where you have
> > access behind firewalls, etc. Not quite administrative access, but
> > closer to the level of access that a normal (a.k.a. "corporate") user
> > might have.
> >
> > With the application (WAVA) tester, the translucent box testing can be
> > seen to be where the tester is given example user accounts for users
> > of different levels of privilege (from the application's perspective,
> > not the operating system perspective!). And then your manual testing
> > activities take on a whole new angle where you attempt to see if you
> > can call admin level menu functions while only logged in as a
> > low-privileged user, see how cross-site scripting might allow a
> > low-privileged user to escalate their privileges to those of the admin
> > level user, or even to gain unauthorised access to records of other
> > low-privileged users, etc.  For the translucent box perspective you
> > can take the manual testing procedures phase (as just one of the items
> > on its own) and expand it into sub categories, like:
> > ? User Authentication and Authorisation mechanisms - test a user's
> > ability to perform unauthorised tasks such as: Access data or
> > resources that they are not entitled to; Escalate their current
> > privileges; Transact as another user; Access resources after logoff /
> > session timeouts; etc
> > ? Access Protection controls - test access protection mechanisms over
> > system resources, e.g. testing if a user can: Gain unauthorised access
> > to system files, data or other resources; Circumvent the access
> > protection controls using atypical methods such as alternate character
> > sets, or URL munging / manipulation / fuzzing, etc.
> > ? Data Validation - test the application for its vulnerability to data
> > injection and insertion techniques (buffer overflows, string
> > formatting problems, etc), i.e. test if the application: Correctly
> > parses any data inputted by the user; Performs adequate bounds
> > checking; Performs adequate validation of any data inputs; etc
> > ? Session Management - assess whether a user can: Steal or highjack
> > the credentials or session of another user; Change their current
> >identity to that of another user (account or role "hopping");
> > Manipulate or falsify the session management controls for any other
> > means; etc
> >
> > Also, while doing translucent box testing, the WAVA tester will gather
> > new information that would be useful to try from the black-box WAVA
> > perspective ? like trying to call internal web app functions before
> > you are authenticated with the server (e.g. I have encountered
> > situations where simply connecting to the login page / function and
> > being issued with a session ID / cookie allowed me to call a function
> > that lists the transaction record ? in other words the authorisation
> > procedures were not being invoked when that function was called, all
> > that it looked at was that a session ID in the correct format was
> > included within the user's POST request?)
> >
> > And then you get on to the white-box testing.
> >
> > In the pentest situation, I'd be more inclined to call this the
> > infrastructure hardening review / audit ? with admin level access you
> > review operating system settings and installed applications; IT
> > department organisation reviews; change control procedures; disaster
> > recovery planning; and all the other normal audit / compliance
> > objectives?
> >
> > The white-box testing in the WAVA situation could be where you include
> > categories of activities like code reviews / audits, architectural
> > design reviews, interviewing developers, threat modelling, DRP, change
> > control procedures, etc
> >
> > What do you think?
> >
> > Regards,
> > Steve

From vanderaj at greebo.net  Tue Apr 19 03:47:00 2005
From: vanderaj at greebo.net (Andrew van der Stock)
Date: Tue, 19 Apr 2005 17:47:00 +1000 (EST)
Subject: [OWASP-TESTING] call to others on this list
In-Reply-To: <40699.212.32.48.101.1113836191.squirrel@webmail.owasp.org>
References: <40699.212.32.48.101.1113836191.squirrel@webmail.owasp.org>
Message-ID: <38836.203.57.241.67.1113896820.squirrel@webmail2.pair.com>

Daniel,

I will check this out tomorrow as I have a huge preso to give to about 50
people tomorrow.

Andrew



From daniel.cuthbert at owasp.org  Tue Apr 19 03:58:04 2005
From: daniel.cuthbert at owasp.org (Daniel)
Date: Tue, 19 Apr 2005 03:58:04 -0400 (EDT)
Subject: [OWASP-TESTING] Semi-anonymous e-mail account for submissions to OWASP
Message-ID: <41649.212.32.48.101.1113897484.squirrel@webmail.owasp.org>

Morning all,

At the previous OWASP London meeting (and AppSec con held the other
weekend), there was talk of having some form of mechanism in which people
could submit documents/code/tools/advice etc to OWASP but not be in breach
of their NDA/Contract/Current terms of employment.

Whilst we work on a more suitable solution, I've setup a gmail account
called owaspsubmissions at gmail.com

All the relevant people will have access to this mail account and we will
take the information sent from you all and sanitize it so that no personal
details are left that could identify you.

Hope this helps?


Daniel



From vanderaj at greebo.net  Wed Apr 20 19:04:33 2005
From: vanderaj at greebo.net (Andrew van der Stock)
Date: Thu, 21 Apr 2005 09:04:33 +1000
Subject: [OWASP-TESTING] Re: [Owasp-dotnet] Semi-anonymous e-mail account for submissions
 to OWASP
In-Reply-To: <41649.212.32.48.101.1113897484.squirrel@webmail.owasp.org>
Message-ID: <BE8D1D21.F0%vanderaj@greebo.net>

I think the major problem with this that the people who wish to contribute
anonymously need to seriously think about getting their contracts modified.

In Australia, it's a breach of the Trade Practices Act to restrict people
working outside hours on ANYTHING - even industries closely allied with your
day job. 

I am not suggesting that people should violate their NDAs or IP with their
employer - if you know something about a particular product, it's probably
not useful to us anyway, and if you developed a new attack method on your
employer's time and used their resources, well they paid for it, and they
own it. Again, not something we can easily use.

However, the rest of your time is your own, despite employer claims to the
contrary. Particularly when you do not use any employer time or resources to
do activity X. Employers have often shoved "everything you know and
everything you will do whilst employed by us is ours". I refer them to the
TPA. You cannot contract an illegal act, and you cannot contract to reduce
rights given to you by law. Generally, I am contracted to do 40 hours of
work a week, plus whatever I am willing to give away if I've made an
estimation error in my timings. If they really want to own all of my stuff,
they can pay me for 168 hours a week, and give me a laptop computer and
outfit my home office, pay for the space on a commercial basis, and pay my
mobile and comms bills. They don't do these things, so they don't get
anything after 5.30 pm.

A few years ago, my friend Luke and I came up with the idea that it is a
good idea to get the IP thing out of the road early on. As I was the
President of SAGE-AU at the time, we had a lawyer draft it, and it's legal
for Australia. If you share our common law, it's likely that it'll work in
the UK and possibly even the US.

http://sourceforge.net/projects/osda/

The OSDA explicitly sets out what you can do out of hours if you're in any
doubt. I use it from time to time when I don't get through to a particular
employer's thick skull that they actually don't own my time.

Andrew


On 19/4/05 5:58 PM, "Daniel" <daniel.cuthbert at owasp.org> wrote:

> Morning all,
> 
> At the previous OWASP London meeting (and AppSec con held the other
> weekend), there was talk of having some form of mechanism in which people
> could submit documents/code/tools/advice etc to OWASP but not be in breach
> of their NDA/Contract/Current terms of employment.
> 
> Whilst we work on a more suitable solution, I've setup a gmail account
> called owaspsubmissions at gmail.com
> 
> All the relevant people will have access to this mail account and we will
> take the information sent from you all and sanitize it so that no personal
> details are left that could identify you.
> 
> Hope this helps?
> 
> 
> Daniel
> 
> 
> -------------------------------------------------------
> This SF.Net email is sponsored by: New Crystal Reports XI.
> Version 11 adds new functionality designed to reduce time involved in
> creating, integrating, and deploying reporting solutions. Free runtime info,
> new features, or free trial, at: http://www.businessobjects.com/devxi/728
> _______________________________________________
> Owasp-dotnet mailing list
> Owasp-dotnet at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/owasp-dotnet
> 





