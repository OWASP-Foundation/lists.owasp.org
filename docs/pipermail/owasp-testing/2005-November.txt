From jfernandez at germinus.com  Mon Nov 21 07:40:02 2005
From: jfernandez at germinus.com (Javier Fernandez-Sanguino)
Date: Mon, 21 Nov 2005 13:40:02 +0100
Subject: [OWASP-TESTING] WAPT submission, revised
In-Reply-To: <200509211527.j8LFRGDT018575@uxserv1.isyioff.it>
References: <200509211527.j8LFRGDT018575@uxserv1.isyioff.it>
Message-ID: <4381C022.8020104@germinus.com>

Mauro Bregolin wrote:
> please find attached a revised version of my submission, which includes 
> comments and feedbacks.
> 
> The "Old, backup and unreferenced files" has been expanded to include 
> material written by Dafydd Stuttard (made available by Javier 
> Fernandez-Sanguino).

This section looks much better, however, I believe the "White box" 
testing section of the "Old, backup and unreferenced files" should be 
expanded. It only suggests looking at unreferenced files based on 
extensions (i.e. 'naming conventions'). It seems you added Dafydd's 
content but did not review my mail and include the comments I made.

IMHO, there are some additional activities (besides doing a "hard 
review" of the directory structure as suggested there) that might help 
finding unreferenced files in the web server through white box 
testing. Some of these can actually be automated through a cron job. 
This is my proposal to expand that section with additional activities:

-----------------------------------------------------------------------

- Review the web server logs and generate a list of files demanded by 
clients (optionally ordered by number of hits). The files that have 
never been accessed by any client are a candidated for removal. Of 
course, the number of false positives of this method depends on the 
log history (if you review logs of a single month you might miss files 
which have not been accessed that month, like season-based 
promotions). In a web server farm, make sure you retrieve the logs of 
all servers, not just one member of the farm.

- Crawl the site fully (either automatic robot, an inline proxy 
logging a person doing a manual navigation through the full website or 
a combination of both) to generate a list of reference files in the 
system. Compare this agains the directory tree and files on the server 
to determine which files might not be reference and/or used any more. 
This can also be combined with the above since this "crawling" session 
should generate more hits than your average user connection and the 
robot logs should be similar to the web server's logs.

- Review the access time of the files in the web server tree.
If this is a web server, most of the files will have their "last 
access" timestamp modified everytime the web server reads them in to 
provide them to an end-user or run them through an application engine. 
  Review those files that have been accessed only a long time ago 
(several months or a year) since those might be candidates for removal 
as either backup files or unreference files. Typically, a the used 
files of a web server will have a similar access timestamp range and 
those files outside that range can be considered old files. Notice 
that this method will fail to spot unreferenced files recently created.

-----------------------------------------------------------------------

And, let me post some of the comments I originally made which might be 
worthwhile adding to that section too, there's an additional thread 
there (old vulnerable applictions) and an additional "black box" test 
(finding common files generated by third-party applications in web 
server directories). Feel free to adapt any of these and include them too:

"Unreferenced application files are not only dangerous because they 
provide you with internal source code but because they might be 
vulnerable! Imagine the case of a web administrator taking a file that 
has an SQL injection, making a backup copy of it, editing it to remove 
the SQL injection present and leaving. If he forgets to remove the 
backup file _And_ the backup file's extension will be executed by the 
webserver (think: original: login.jsp, backup: login.old.jsp) then you 
might think you are safe when you are actually just as vulnerable as 
before (just that people don't know that the file exists).

You also fail to mention files that do not belong in the web server 
but are typically generated by upload tools (think WS_FTP.log files). 
These files are "unreferenced" in the sense that they should not be 
there, and contain sensitive information (like the user that ftp's in 
the web server) that might be valuable to the user. "

Finally, as I commented in my last post, I think it's worthwhile 
adding a mention to "Archive.org" in the "Use of publicly available 
information" part. You are guaranteed to have more information there 
than the one archived in Yahoo! or Google's cache. You can use the 
"Take me back!" time machine there to browse server content which are 
years old (this can be done programatically to generate a list of the 
web server contents for a given period of time).

Hope that helps improve the document.

Regards

Javier



From mauro.bregolin at gmail.com  Tue Nov 22 04:07:09 2005
From: mauro.bregolin at gmail.com (Mauro Bregolin)
Date: Tue, 22 Nov 2005 10:07:09 +0100
Subject: [OWASP-TESTING] WAPT submission, revised
In-Reply-To: <4381C022.8020104@germinus.com>
Message-ID: <4382dfcd.1aaa0f3b.1340.4461@mx.gmail.com>

Javier,

I realized I (dumbly) overlooked your previous message and failed to read
your original txt attachment. That's why your input never made it to the
revised version.

Concerning your original comments regarding strategies to spot files, I do
not completely agree.
I wouldn't say that looking at access times is less FP-prone than the method
I was writing about (finding files according to extensions). How much "old"
is old enough to account for being unreferenced? One month? Three months?
One week? Your method (which has its own merits) wouldn't notice the huge
.tar file left yesterday by a careless sysadmin in the web server tree,
until it "ages" up to the point of being considered "old and unreferenced"
(provided it doesn't get accessed by an evil hacker in the interim... which
would keep it "legitimate" for another while).
Now, there are some file extensions which look definitely *wrong*, and would
be easily identified by a simple file search (I'm not saying they are
necessarily illegitimate; but many *usually* are: A "login.jsp.old" file
looks definitely out of place, and while a web app may rely on .tar files,
it rarely does...).
In my opinion, there's no silver bullet in the form of a fully automatic
solution, because all strategies are based on euristhics, and as such are
always approximations. Looking via file extensions takes into account
semantic information related to the purpose of the file (.tar, .tgz, .zip,
... : backup files; .old, ...: evidence of out-of-date copies); usually this
information is rather accurate. However, it fails to find unreferenced files
having legitimate-looking extensions (for example, .jsp files left over
after a broken software upgrade). On the other hand, looking at access times
takes into account time, which is undoubtedly important. But it will find
candidates only after a time delay, which leaves a window of exposure.
Crawling may help solve this, but depending on the web app structure may be
difficult to fully automatize (anyway, it's surely another source of input
worth considering).
A good approach would be to combine both search criteria (and possibly
others that might be devised in addition).
Best of all, however, is enforcing strong security policies. Spotting files
means you are *late*, and did something wrong. Found a .old? Someone did
in-place editing, what about your change management procedures? Found a .zip
or a .tar in the wrong place? This denotes inappropriate behavior as well.
After a software upgrade you find unused JSPs left over? You have some darn
problem with your software upgrade procedure... etc. etc. But, of course,
life is different and these things happen and will continue to happen, and
we must have a way to check.

I will reflect this in the chapter and go through your other comments as
well.

Thanks,

Mauro

P.S. btw, what happened to all other submissions? Did I miss something, or
were some of them submitted off-list?


-----Original Message-----
From: Javier Fernandez-Sanguino [mailto:jfernandez at germinus.com] 
Sent: luned? 21 novembre 2005 13.40
To: Mauro Bregolin
Cc: 'Daniel Cuthbert'; owasp-testing at lists.sourceforge.net
Subject: Re: [OWASP-TESTING] WAPT submission, revised

Mauro Bregolin wrote:
> please find attached a revised version of my submission, which includes 
> comments and feedbacks.
> 
> The "Old, backup and unreferenced files" has been expanded to include 
> material written by Dafydd Stuttard (made available by Javier 
> Fernandez-Sanguino).

This section looks much better, however, I believe the "White box" 
testing section of the "Old, backup and unreferenced files" should be 
expanded. It only suggests looking at unreferenced files based on 
extensions (i.e. 'naming conventions'). It seems you added Dafydd's 
content but did not review my mail and include the comments I made.

IMHO, there are some additional activities (besides doing a "hard 
review" of the directory structure as suggested there) that might help 
finding unreferenced files in the web server through white box 
testing. Some of these can actually be automated through a cron job. 
This is my proposal to expand that section with additional activities:

-----------------------------------------------------------------------

- Review the web server logs and generate a list of files demanded by 
clients (optionally ordered by number of hits). The files that have 
never been accessed by any client are a candidated for removal. Of 
course, the number of false positives of this method depends on the 
log history (if you review logs of a single month you might miss files 
which have not been accessed that month, like season-based 
promotions). In a web server farm, make sure you retrieve the logs of 
all servers, not just one member of the farm.

- Crawl the site fully (either automatic robot, an inline proxy 
logging a person doing a manual navigation through the full website or 
a combination of both) to generate a list of reference files in the 
system. Compare this agains the directory tree and files on the server 
to determine which files might not be reference and/or used any more. 
This can also be combined with the above since this "crawling" session 
should generate more hits than your average user connection and the 
robot logs should be similar to the web server's logs.

- Review the access time of the files in the web server tree.
If this is a web server, most of the files will have their "last 
access" timestamp modified everytime the web server reads them in to 
provide them to an end-user or run them through an application engine. 
  Review those files that have been accessed only a long time ago 
(several months or a year) since those might be candidates for removal 
as either backup files or unreference files. Typically, a the used 
files of a web server will have a similar access timestamp range and 
those files outside that range can be considered old files. Notice 
that this method will fail to spot unreferenced files recently created.

-----------------------------------------------------------------------

And, let me post some of the comments I originally made which might be 
worthwhile adding to that section too, there's an additional thread 
there (old vulnerable applictions) and an additional "black box" test 
(finding common files generated by third-party applications in web 
server directories). Feel free to adapt any of these and include them too:

"Unreferenced application files are not only dangerous because they 
provide you with internal source code but because they might be 
vulnerable! Imagine the case of a web administrator taking a file that 
has an SQL injection, making a backup copy of it, editing it to remove 
the SQL injection present and leaving. If he forgets to remove the 
backup file _And_ the backup file's extension will be executed by the 
webserver (think: original: login.jsp, backup: login.old.jsp) then you 
might think you are safe when you are actually just as vulnerable as 
before (just that people don't know that the file exists).

You also fail to mention files that do not belong in the web server 
but are typically generated by upload tools (think WS_FTP.log files). 
These files are "unreferenced" in the sense that they should not be 
there, and contain sensitive information (like the user that ftp's in 
the web server) that might be valuable to the user. "

Finally, as I commented in my last post, I think it's worthwhile 
adding a mention to "Archive.org" in the "Use of publicly available 
information" part. You are guaranteed to have more information there 
than the one archived in Yahoo! or Google's cache. You can use the 
"Take me back!" time machine there to browse server content which are 
years old (this can be done programatically to generate a list of the 
web server contents for a given period of time).

Hope that helps improve the document.

Regards

Javier




From jfernandez at germinus.com  Tue Nov 22 07:06:40 2005
From: jfernandez at germinus.com (Javier Fernandez-Sanguino)
Date: Tue, 22 Nov 2005 13:06:40 +0100
Subject: [OWASP-TESTING] WAPT submission, revised
In-Reply-To: <4382dfcd.1aaa0f3b.1340.4461@mx.gmail.com>
References: <4382dfcd.1aaa0f3b.1340.4461@mx.gmail.com>
Message-ID: <438309D0.6010408@germinus.com>

Mauro Bregolin wrote:

> Javier,
> 
> I realized I (dumbly) overlooked your previous message and failed to read
> your original txt attachment. That's why your input never made it to the
> revised version.
> 
> Concerning your original comments regarding strategies to spot files, I do
> not completely agree.
> I wouldn't say that looking at access times is less FP-prone than the method
> I was writing about (finding files according to extensions). How much "old"
> is old enough to account for being unreferenced? One month? Three months?
> One week? Your method (which has its own merits) wouldn't notice the huge
> .tar file left yesterday by a careless sysadmin in the web server tree,
> until it "ages" up to the point of being considered "old and unreferenced"
> (provided it doesn't get accessed by an evil hacker in the interim... which
> would keep it "legitimate" for another while).

I didn't want to imply that my approach was the only approach you 
should take. I just listed several approaches that would allow you to 
find backup/unreferenced files. Summarising, it's best if you do all 
three:

a) find files based on extension
b) find files with old access time
c) look at the logs (optionally traversing the server through a robot 
or manual session) and review which files are *not* being accessed

Those three combined would help spot most of those and weed out false 
positives / negatives of each approach if used exclusively.

For example, if you have a 'login.asp' in your system, but then moved 
over to a different authentication system 'login_pki.asp'. You might 
not be able to use a) in order to find that 'login.asp' is, indeed, 
not being used any more and can be a potential risk since, after all, 
it is a legitimate file extension for your server. You don't catch 
that with a) but you can catch it with b) or c)

> In my opinion, there's no silver bullet in the form of a fully automatic
> solution, because all strategies are based on euristhics, and as such are

(...)

That's why I propose listing all of them. It seems we agree here.

> Crawling may help solve this, but depending on the web app structure may be
> difficult to fully automatize (anyway, it's surely another source of input
> worth considering).

Notice that you can either have automatic crawling or "guided" 
crawling. Or, even, review logs from your user's accesses and infer 
from there.

> A good approach would be to combine both search criteria (and possibly
> others that might be devised in addition).

Agreed.

> Best of all, however, is enforcing strong security policies. Spotting files
> means you are *late*, and did something wrong. Found a .old? Someone did
> in-place editing, what about your change management procedures? Found a .zip
> or a .tar in the wrong place? This denotes inappropriate behavior as well.
> After a software upgrade you find unused JSPs left over? You have some darn
> problem with your software upgrade procedure... etc. etc. But, of course,
> life is different and these things happen and will continue to happen, and
> we must have a way to check.

That's why having proper policies for operation and maintenance of 
servers should be listed as 'countermeasures'. Indeed, that is already 
there in the section.


> I will reflect this in the chapter and go through your other comments as
> well.

Cool, thanks.

Regards


Javier



From jfernandez at germinus.com  Mon Nov 28 12:28:11 2005
From: jfernandez at germinus.com (Javier Fernandez-Sanguino)
Date: Mon, 28 Nov 2005 18:28:11 +0100
Subject: [OWASP-TESTING] WAPT submission, revised
In-Reply-To: <9f9539350511280911h653293f0w7d3a0ec986365b86@mail.gmail.com>
References: <9f9539350511280911h653293f0w7d3a0ec986365b86@mail.gmail.com>
Message-ID: <438B3E2B.1050305@germinus.com>

Mauro Bregolin wrote:

> Here is an updated version of the material (with respect to the "Old,
> backup and unreferenced files" section; others are unchanged), which I
> believe addresses all your points.

I've checked out the section and it looks great. I specially like your 
distinction on web server files and files accessed by web server 
applications (configuration files and such) which might not show up in 
the logs but will have their access timestamp modified. I hadn't 
thought about that one and is a good point to make.

Regards

Javier




From mauro.bregolin at gmail.com  Mon Nov 28 12:11:07 2005
From: mauro.bregolin at gmail.com (Mauro Bregolin)
Date: Mon, 28 Nov 2005 18:11:07 +0100
Subject: [OWASP-TESTING] WAPT submission, revised
Message-ID: <9f9539350511280911h653293f0w7d3a0ec986365b86@mail.gmail.com>

Here is an updated version of the material (with respect to the "Old,
backup and unreferenced files" section; others are unchanged), which I
believe addresses all your points.

Regards,

Mauro
-------------- next part --------------
A non-text attachment was scrubbed...
Name: WAPT 28-11-05.doc
Type: application/msword
Size: 302080 bytes
Desc: not available
Url : http://lists.owasp.org/pipermail/owasp-testing/attachments/20051128/7a57e8d9/attachment.doc 

