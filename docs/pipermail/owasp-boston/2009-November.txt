From Jim.Weiler at starwoodhotels.com  Sun Nov  1 21:19:26 2009
From: Jim.Weiler at starwoodhotels.com (Weiler, Jim)
Date: Sun, 1 Nov 2009 21:19:26 -0500
Subject: [Owasp-boston] No meeting this week
Message-ID: <0D8299A04960AE4C8906052835761AF103B29B94@STCEXCPMB02.corp.star>

We're not sure if we'll have a Nov meeting, we're working on speakers. 

 

Jim

 

Jim Weiler   CISSP  CSSLP

Starwood Hotels and Resorts

Sr. Mgr. Information Security Risk Assessment

Office - 781 356 0067

Cell - 781 654 6048



This electronic message transmission contains information from the Company that may be proprietary, confidential and/or privileged. 
The information is intended only for the use of the individual(s) or entity named above.  If you are not the intended recipient, be 
aware that any disclosure, copying or distribution or use of the contents of this information is prohibited.  If you have received 
this electronic transmission in error, please notify the sender immediately by replying to the address listed in the "From:" field. 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091101/7b2f3dca/attachment.html 

From Jim.Weiler at starwoodhotels.com  Mon Nov  2 07:23:39 2009
From: Jim.Weiler at starwoodhotels.com (Weiler, Jim)
Date: Mon, 2 Nov 2009 07:23:39 -0500
Subject: [Owasp-boston] anyone using or evaling web app vulnerability
	scanner?
Message-ID: <0D8299A04960AE4C8906052835761AF103B29B99@STCEXCPMB02.corp.star>

I'm doing an evaluation of web app vulnerability scanners and have used
one before - I'd like to have an off line conversation to share
information and experiences with anybody else using or evaluating this
type of tool.

 

Thanks, Jim

 

Jim Weiler   CISSP  CSSLP

Starwood Hotels and Resorts

Sr. Mgr. Information Security Risk Assessment

Office - 781 356 0067

Cell - 781 654 6048



This electronic message transmission contains information from the Company that may be proprietary, confidential and/or privileged. 
The information is intended only for the use of the individual(s) or entity named above.  If you are not the intended recipient, be 
aware that any disclosure, copying or distribution or use of the contents of this information is prohibited.  If you have received 
this electronic transmission in error, please notify the sender immediately by replying to the address listed in the "From:" field. 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091102/1e5f44a1/attachment.html 

From james at architectbook.com  Mon Nov  2 08:15:19 2009
From: james at architectbook.com (james at architectbook.com)
Date: Mon, 02 Nov 2009 06:15:19 -0700
Subject: [Owasp-boston]
 =?utf-8?q?anyone_using_or_evaling_web_app_vulnerab?=
 =?utf-8?q?ility_scanner=3F?=
Message-ID: <20091102061519.1b34e4c3b93181cbb56b6df77bbedd57.a1ab164486.wbe@email.secureserver.net>

An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091102/3c1718a8/attachment.html 

From smatsumoto at cigital.com  Mon Nov  2 08:23:51 2009
From: smatsumoto at cigital.com (Scott Matsumoto)
Date: Mon, 2 Nov 2009 08:23:51 -0500
Subject: [Owasp-boston] anyone using or evaling web app vulnerability
 scanner?
In-Reply-To: <20091102061519.1b34e4c3b93181cbb56b6df77bbedd57.a1ab164486.wbe@email.secureserver.net>
References: <20091102061519.1b34e4c3b93181cbb56b6df77bbedd57.a1ab164486.wbe@email.secureserver.net>
Message-ID: <41945506397C0C4886A8C5BFF089B5CA3AFA29C794@va-mailhub.cigital.com>

James,

Cigital also offer black-box and (coming soon) white-box outsourcing.  Send me a priviate email if you're interested.
________________________________
From: owasp-boston-bounces at lists.owasp.org [owasp-boston-bounces at lists.owasp.org] On Behalf Of james at architectbook.com [james at architectbook.com]
Sent: Monday, November 02, 2009 8:15 AM
To: Weiler,Jim
Cc: owasp-boston at lists.owasp.org; owasp-bostonfinancialdist at lists.owasp.org
Subject: Re: [Owasp-boston] anyone using or evaling web app vulnerability scanner?

We currently own both WebInspect and Appscan. Do to lack of resources, we are looking to outsource scanning and evaluating Whitehat and Redspin.

Will send you some free times to chat...
-------- Original Message --------
Subject: [Owasp-boston] anyone using or evaling web app vulnerability
scanner?
From: "Weiler, Jim" <Jim.Weiler at starwoodhotels.com>
Date: Mon, November 02, 2009 7:23 am
To: <owasp-boston at lists.owasp.org>,
<owasp-bostonfinancialdist at lists.owasp.org>

I'm doing an evaluation of web app vulnerability scanners and have used one before - I'd like to have an off line conversation to share information and experiences with anybody else using or evaluating this type of tool.

Thanks, Jim

Jim Weiler   CISSP  CSSLP
Starwood Hotels and Resorts
Sr. Mgr. Information Security Risk Assessment
Office - 781 356 0067
Cell - 781 654 6048

This electronic message transmission contains information from the Company that may be proprietary, confidential and/or privileged. The information is intended only for the use of the individual(s) or entity named above. If you are not the intended recipient, be aware that any disclosure, copying or distribution or use of the contents of this information is prohibited. If you have received this electronic transmission in error, please notify the sender immediately by replying to the address listed in the "From:" field.
________________________________
_______________________________________________
Owasp-boston mailing list
Owasp-boston at lists.owasp.org
https://lists.owasp.org/mailman/listinfo/owasp-boston

From jcran at 0x0e.org  Mon Nov  2 14:38:24 2009
From: jcran at 0x0e.org (Jonathan Cran)
Date: Mon, 2 Nov 2009 14:38:24 -0500
Subject: [Owasp-boston] anyone using or evaling web app vulnerability
	scanner?
In-Reply-To: <41945506397C0C4886A8C5BFF089B5CA3AFA29C794@va-mailhub.cigital.com>
References: <20091102061519.1b34e4c3b93181cbb56b6df77bbedd57.a1ab164486.wbe@email.secureserver.net>
	<41945506397C0C4886A8C5BFF089B5CA3AFA29C794@va-mailhub.cigital.com>
Message-ID: <40a166ad0911021138w1d81cb35x8f4daca9d9a54bd5@mail.gmail.com>

Jim,

You may want to take a look at the recently released Web Application
Security Scanner Evaluation Criteria (WASSEC) - more than a mouthful,
it's quite handy if you're evaluating scanners.

jcran

Jonathan Cran
jcran at 0x0e.org / jcran at rapid7.com


On Mon, Nov 2, 2009 at 8:23 AM, Scott Matsumoto <smatsumoto at cigital.com> wrote:
> James,
>
> Cigital also offer black-box and (coming soon) white-box outsourcing. ?Send me a priviate email if you're interested.
> ________________________________
> From: owasp-boston-bounces at lists.owasp.org [owasp-boston-bounces at lists.owasp.org] On Behalf Of james at architectbook.com [james at architectbook.com]
> Sent: Monday, November 02, 2009 8:15 AM
> To: Weiler,Jim
> Cc: owasp-boston at lists.owasp.org; owasp-bostonfinancialdist at lists.owasp.org
> Subject: Re: [Owasp-boston] anyone using or evaling web app vulnerability scanner?
>
> We currently own both WebInspect and Appscan. Do to lack of resources, we are looking to outsource scanning and evaluating Whitehat and Redspin.
>
> Will send you some free times to chat...
> -------- Original Message --------
> Subject: [Owasp-boston] anyone using or evaling web app vulnerability
> scanner?
> From: "Weiler, Jim" <Jim.Weiler at starwoodhotels.com>
> Date: Mon, November 02, 2009 7:23 am
> To: <owasp-boston at lists.owasp.org>,
> <owasp-bostonfinancialdist at lists.owasp.org>
>
> I'm doing an evaluation of web app vulnerability scanners and have used one before - I'd like to have an off line conversation to share information and experiences with anybody else using or evaluating this type of tool.
>
> Thanks, Jim
>
> Jim Weiler ? CISSP ?CSSLP
> Starwood Hotels and Resorts
> Sr. Mgr. Information Security Risk Assessment
> Office - 781 356 0067
> Cell - 781 654 6048
>
> This electronic message transmission contains information from the Company that may be proprietary, confidential and/or privileged. The information is intended only for the use of the individual(s) or entity named above. If you are not the intended recipient, be aware that any disclosure, copying or distribution or use of the contents of this information is prohibited. If you have received this electronic transmission in error, please notify the sender immediately by replying to the address listed in the "From:" field.
> ________________________________
> _______________________________________________
> Owasp-boston mailing list
> Owasp-boston at lists.owasp.org
> https://lists.owasp.org/mailman/listinfo/owasp-boston
> _______________________________________________
> Owasp-boston mailing list
> Owasp-boston at lists.owasp.org
> https://lists.owasp.org/mailman/listinfo/owasp-boston
>

From Patrick_Laverty at brown.edu  Tue Nov  3 09:16:42 2009
From: Patrick_Laverty at brown.edu (Laverty, Patrick)
Date: Tue, 3 Nov 2009 09:16:42 -0500
Subject: [Owasp-boston] anyone using or evaling web app
	vulnerabilityscanner?
In-Reply-To: <40a166ad0911021138w1d81cb35x8f4daca9d9a54bd5@mail.gmail.com>
References: <20091102061519.1b34e4c3b93181cbb56b6df77bbedd57.a1ab164486.wbe@email.secureserver.net><41945506397C0C4886A8C5BFF089B5CA3AFA29C794@va-mailhub.cigital.com>
	<40a166ad0911021138w1d81cb35x8f4daca9d9a54bd5@mail.gmail.com>
Message-ID: <09DED73804A95B42ACFEFF57AEA3B24E06FC3715@MAIL3.AD.Brown.Edu>

An app scanner is exactly what we need here at my school.  If people
don't mind, I'd love to hear the pros and cons of each of the top
products on the market.  I think I've been told there's the "big 3"
commercial products and there's the Samurai suite of tools.

If it's not appropriate to post the info to the list, could someone who
recently went through any kind of "bake-off" or testing of the tools, or
even just wants to say why they really like the scanner they chose or
maybe a warning to stay away from one with specific reason(s).



Thanks!

Patrick Laverty
Brown University

From javed at zsquad.com  Tue Nov  3 09:42:02 2009
From: javed at zsquad.com (Javed Ikbal)
Date: Tue, 03 Nov 2009 09:42:02 -0500
Subject: [Owasp-boston] anyone using or evaling web
	app	vulnerabilityscanner?
In-Reply-To: <09DED73804A95B42ACFEFF57AEA3B24E06FC3715@MAIL3.AD.Brown.Edu>
References: <20091102061519.1b34e4c3b93181cbb56b6df77bbedd57.a1ab164486.wbe@email.secureserver.net><41945506397C0C4886A8C5BFF089B5CA3AFA29C794@va-mailhub.cigital.com>	<40a166ad0911021138w1d81cb35x8f4daca9d9a54bd5@mail.gmail.com>
	<09DED73804A95B42ACFEFF57AEA3B24E06FC3715@MAIL3.AD.Brown.Edu>
Message-ID: <4AF0413A.5020507@zsquad.com>

Patrick:
I had replied to Jim privately, but given the interest in this topic
reposting that email to the list with a little additional information.

I have used most of the commercial solutions out there except Rapid7's
Nexpose.

Quick summary: For about 50% (or less, depending on the edition you are
getting) compared to WebInspect or AppScan, Acunetix will give you 90%
of the tests and functionalities.

- If you do not want to build this into the SDLC, Acunetix's
price/performance is tough to beat.
- If you have 25K to spend on this, I'd recommend AppScan or WebInspect,
depending on whether you already buy a lot from IBM or HP. They might
give huge discounts to an existing customer.

All products out there will give you a tremendous number of false positives

If your goal is to do much more than just run an automated tool and hand
over the reports to QA/Dev, also look at the Burp Suite pro edition. At
about $200/year, the price is amazing and while the product has some
rough edges, it can do a very good job.

Lastly, Sentinel from Whitehat is a hosted solution. Even if you want to
bring your scanning 100% inhouse, get a trial of that to measure other
products against.

Best regards

Javed
-------------------------------------------------------------------
Javed Ikbal, CISSP, CISM, CISA
Principal
www.zsquad.com | E: javed at zsquad.com
P: 617 780 9052 | F: 781 723 0590


Laverty, Patrick wrote:
> An app scanner is exactly what we need here at my school.  If people
> don't mind, I'd love to hear the pros and cons of each of the top
> products on the market.  I think I've been told there's the "big 3"
> commercial products and there's the Samurai suite of tools.
>
> If it's not appropriate to post the info to the list, could someone who
> recently went through any kind of "bake-off" or testing of the tools, or
> even just wants to say why they really like the scanner they chose or
> maybe a warning to stay away from one with specific reason(s).
>
>
>
> Thanks!
>
> Patrick Laverty
> Brown University
> _______________________________________________
> Owasp-boston mailing list
> Owasp-boston at lists.owasp.org
> https://lists.owasp.org/mailman/listinfo/owasp-boston
>
>   

-- 

Best regards

Javed
-------------------------------------------------------------------
Javed Ikbal, CISSP, CISM, CISA
Principal
www.zsquad.com | E: javed at zsquad.com
P: 617 780 9052 | F: 781 723 0590
-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091103/d4763a2a/attachment.html 

From Jim.Weiler at starwoodhotels.com  Tue Nov  3 11:10:23 2009
From: Jim.Weiler at starwoodhotels.com (Weiler, Jim)
Date: Tue, 3 Nov 2009 11:10:23 -0500
Subject: [Owasp-boston] OWASP Boston Tues Nov 17 meeting - 2 presentations
Message-ID: <0D8299A04960AE4C8906052835761AF103B952D2@STCEXCPMB02.corp.star>

http://www.owasp.org/index.php/Boston

 

Date - Tuesday, Nov 17

 

Time - 6:30 p.m.  Microsoft Waltham offices

 

Double Feature - Two presentations by OWASP members - 

 

Jim Weiler  - Web Application Vulnerability Scanners - What's out there?
I'll present my experience using and evaluating these tools, both
commercial and free, and summarize the conversations and emails I've had
with other users and evaluators. I'm assuming everyone knows what these
are, so we'll go right into architecture, technology differences,
reporting, configuration choices etc. I think we'll have plenty of
audience participation discussion too. I'll also report what I learned
at the OWASP AppSec conference in Washington DC.

 

 

Mush Hakhinian - Secure coding with no money down: unleashing the power
of open-source code analysis tools

 

Static code analysis is indispensible for uncovering coding errors
before they reach production. Two major obstacles - high price and steep
learning curve of the available commercial tools, hamper the adoption,
however. In the tool evaluation phase the 'ease of integration'
typically translates into how easy it is to kick-off the analysis from
existing build environment. Then, after all the excitement for the novel
tool and the sticker shock have passed, come the pains of making people
do things differently in their day-to-day work. Enter open-source Sonar
that makes process changes less painful by using familiar plug-ins and
blows away the price obstacle. 

Sonar is a code quality management platform that allows for pretty
robust static code analysis. In this presentation we will demonstrate
how to configure it to cover most of known vulnerabilities using open
source plug-ins and to track trends. By using Sonar and code analysis
plug-ins we are introducing automated code review to the development
process without spending a dime

 

 

Mush Hakhinian has been managing security initiatives for the past 16
years and is an active member of OWASP Boston Chapter. He leads the
application security practice at IntraLinks, a SaaS solution for secure
collaboration and communication inside and outside the firewall.

 

 

Location and Directions - 

 

Microsoft offices at the Waltham Weston Corporate Center, 201 Jones Rd.,
Sixth Floor Waltham, MA

 

>From Rt. 128 North take exit 26 toward Waltham, East up the hill on Rt.
20. From Rt 128 South take exit 26 but go around the rotary to get to 20
East to Waltham. Follow signs for Rt. 117 (left at the second light).
When you get to 117 turn left (West). You will cross back over Rt. 128.
Jones Rd. (look for the Waltham Weston Corporate Center sign) is the
second left, at a blinking yellow light, on Rt. 117 going west about 0.1
miles from Rt. 128 (I95). The office building is at the bottom of Jones
Rd. Best parking is to turn right just before the building and park in
the back. Knock on the door to get the security guard to open it. The
room is MPR C.

 

Pizza provided by IntraLinks

 

Jim Weiler   CISSP  CSSLP

Starwood Hotels and Resorts

Sr. Mgr. Information Security Risk Assessment

Office - 781 356 0067

Cell - 781 654 6048



This electronic message transmission contains information from the Company that may be proprietary, confidential and/or privileged. 
The information is intended only for the use of the individual(s) or entity named above.  If you are not the intended recipient, be 
aware that any disclosure, copying or distribution or use of the contents of this information is prohibited.  If you have received 
this electronic transmission in error, please notify the sender immediately by replying to the address listed in the "From:" field. 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091103/dcfb4330/attachment-0001.html 

From Patrick_Laverty at brown.edu  Wed Nov 11 14:59:54 2009
From: Patrick_Laverty at brown.edu (Laverty, Patrick)
Date: Wed, 11 Nov 2009 14:59:54 -0500
Subject: [Owasp-boston] Suhosin?
Message-ID: <09DED73804A95B42ACFEFF57AEA3B24E070D8757@MAIL3.AD.Brown.Edu>

I'm trying to find reviews on Suhosin for php hardening.  What are
peoples' thoughts here?  A must-have, especially for a decentralized
university web environment?  Any thoughts on it, pro or con are
appreciated.

 

Thanks!

 

Patrick

-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091111/af90413c/attachment.html 

From Patrick_Laverty at brown.edu  Thu Nov 12 09:31:31 2009
From: Patrick_Laverty at brown.edu (Laverty, Patrick)
Date: Thu, 12 Nov 2009 09:31:31 -0500
Subject: [Owasp-boston] Checking file types on upload
In-Reply-To: <4AF04A2D.500@codenomicon.com>
References: <4AF04A2D.500@codenomicon.com>
Message-ID: <09DED73804A95B42ACFEFF57AEA3B24E070D891F@MAIL3.AD.Brown.Edu>

Sorry for all the questions lately, but I'm wondering if someone has
come up with a reliable way to check actual file types when they get
uploaded to a server, preferably with PHP.  We've had some issues where
people uploaded php files with a .jpg or .gif extension, so they slipped
by for a while.

We are turning off php in upload directories, among other security
steps, but I just wanted to see if I could do more than just checking
the file extension.  Looking for that extra layer of security.

Thanks!

Patrick Laverty
Brown University

From javed at zsquad.com  Thu Nov 12 09:57:18 2009
From: javed at zsquad.com (Javed Ikbal)
Date: Thu, 12 Nov 2009 09:57:18 -0500
Subject: [Owasp-boston] Checking file types on upload
In-Reply-To: <09DED73804A95B42ACFEFF57AEA3B24E070D891F@MAIL3.AD.Brown.Edu>
References: <4AF04A2D.500@codenomicon.com>
	<09DED73804A95B42ACFEFF57AEA3B24E070D891F@MAIL3.AD.Brown.Edu>
Message-ID: <4AFC224E.5060300@zsquad.com>

The most common (and error-prone) method is to use the $_FILES array.
The browser supplies this information to the server, and IE is notorious
for using the extension to tell the server what the filetype is.

As it uses the filename extension, it is easy to fool (or make the mistake)

With PHP 5.3 or above, you can use fileinfo

http://www.php.net/manual/en/function.finfo-file.php

For older versions, fileinfo is a loadable module.

I guess you can run grep in the upload dirs to catch misnamed php files
that have already been upload, something like:

grep -R "<?" *.jpg *.gif *.png

Regards

Javed

Laverty, Patrick wrote:
> Sorry for all the questions lately, but I'm wondering if someone has
> come up with a reliable way to check actual file types when they get
> uploaded to a server, preferably with PHP.  We've had some issues where
> people uploaded php files with a .jpg or .gif extension, so they slipped
> by for a while.
>
> We are turning off php in upload directories, among other security
> steps, but I just wanted to see if I could do more than just checking
> the file extension.  Looking for that extra layer of security.
>
> Thanks!
>
> Patrick Laverty
> Brown University
> _______________________________________________
> Owasp-boston mailing list
> Owasp-boston at lists.owasp.org
> https://lists.owasp.org/mailman/listinfo/owasp-boston
>   


From scorlosquet at gmail.com  Thu Nov 12 10:15:33 2009
From: scorlosquet at gmail.com (Stephane Corlosquet)
Date: Thu, 12 Nov 2009 10:15:33 -0500
Subject: [Owasp-boston] Checking file types on upload
In-Reply-To: <4AFC224E.5060300@zsquad.com>
References: <4AF04A2D.500@codenomicon.com>
	<09DED73804A95B42ACFEFF57AEA3B24E070D891F@MAIL3.AD.Brown.Edu>
	<4AFC224E.5060300@zsquad.com>
Message-ID: <1452bf810911120715l56176cb7q690a6ad691d7137a@mail.gmail.com>

>
> I guess you can run grep in the upload dirs to catch misnamed php files
> that have already been upload, something like:
>
> grep -R "<?" *.jpg *.gif *.png
>

An image file could contain "<?" but that does not mean it's a PHP file.
Maybe you could run php --syntax-check on the command line to check the
syntax of a suspicious file.

Steph.

On Thu, Nov 12, 2009 at 9:57 AM, Javed Ikbal <javed at zsquad.com> wrote:

> The most common (and error-prone) method is to use the $_FILES array.
> The browser supplies this information to the server, and IE is notorious
> for using the extension to tell the server what the filetype is.
>
> As it uses the filename extension, it is easy to fool (or make the mistake)
>
> With PHP 5.3 or above, you can use fileinfo
>
> http://www.php.net/manual/en/function.finfo-file.php
>
> For older versions, fileinfo is a loadable module.
>
> I guess you can run grep in the upload dirs to catch misnamed php files
> that have already been upload, something like:
>
> grep -R "<?" *.jpg *.gif *.png
>
> Regards
>
> Javed
>
> Laverty, Patrick wrote:
> > Sorry for all the questions lately, but I'm wondering if someone has
> > come up with a reliable way to check actual file types when they get
> > uploaded to a server, preferably with PHP.  We've had some issues where
> > people uploaded php files with a .jpg or .gif extension, so they slipped
> > by for a while.
> >
> > We are turning off php in upload directories, among other security
> > steps, but I just wanted to see if I could do more than just checking
> > the file extension.  Looking for that extra layer of security.
> >
> > Thanks!
> >
> > Patrick Laverty
> > Brown University
> > _______________________________________________
> > Owasp-boston mailing list
> > Owasp-boston at lists.owasp.org
> > https://lists.owasp.org/mailman/listinfo/owasp-boston
> >
>
> _______________________________________________
> Owasp-boston mailing list
> Owasp-boston at lists.owasp.org
> https://lists.owasp.org/mailman/listinfo/owasp-boston
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091112/ae910ba2/attachment.html 

From javed at zsquad.com  Thu Nov 12 10:25:00 2009
From: javed at zsquad.com (Javed Ikbal)
Date: Thu, 12 Nov 2009 10:25:00 -0500
Subject: [Owasp-boston] Checking file types on upload
In-Reply-To: <1452bf810911120715l56176cb7q690a6ad691d7137a@mail.gmail.com>
References: <4AF04A2D.500@codenomicon.com>	<09DED73804A95B42ACFEFF57AEA3B24E070D891F@MAIL3.AD.Brown.Edu>	<4AFC224E.5060300@zsquad.com>
	<1452bf810911120715l56176cb7q690a6ad691d7137a@mail.gmail.com>
Message-ID: <4AFC28CC.7020505@zsquad.com>

Yes,  "<?" may exist in an image file.

But:
Given the upload dirs may have hundreds, if not thousands of files,
finding the suspicious files would be a chore--hence my recommendation.

grep will narrow down the list of suspects very quickly for the manual
examination, or the output of the grep could be piped to the php syntax
check as you suggested.

Javed

Stephane Corlosquet wrote:
>
>     I guess you can run grep in the upload dirs to catch misnamed php
>     files
>     that have already been upload, something like:
>
>     grep -R "<?" *.jpg *.gif *.png
>
>
> An image file could contain "<?" but that does not mean it's a PHP
> file. Maybe you could run php --syntax-check on the command line to
> check the syntax of a suspicious file.
>
> Steph.
>
> On Thu, Nov 12, 2009 at 9:57 AM, Javed Ikbal <javed at zsquad.com
> <mailto:javed at zsquad.com>> wrote:
>
>     The most common (and error-prone) method is to use the $_FILES array.
>     The browser supplies this information to the server, and IE is
>     notorious
>     for using the extension to tell the server what the filetype is.
>
>     As it uses the filename extension, it is easy to fool (or make the
>     mistake)
>
>     With PHP 5.3 or above, you can use fileinfo
>
>     http://www.php.net/manual/en/function.finfo-file.php
>
>     For older versions, fileinfo is a loadable module.
>
>     I guess you can run grep in the upload dirs to catch misnamed php
>     files
>     that have already been upload, something like:
>
>     grep -R "<?" *.jpg *.gif *.png
>
>     Regards
>
>     Javed
>
>     Laverty, Patrick wrote:
>     > Sorry for all the questions lately, but I'm wondering if someone has
>     > come up with a reliable way to check actual file types when they get
>     > uploaded to a server, preferably with PHP.  We've had some
>     issues where
>     > people uploaded php files with a .jpg or .gif extension, so they
>     slipped
>     > by for a while.
>     >
>     > We are turning off php in upload directories, among other security
>     > steps, but I just wanted to see if I could do more than just
>     checking
>     > the file extension.  Looking for that extra layer of security.
>     >
>     > Thanks!
>     >
>     > Patrick Laverty
>     > Brown University
>     > _______________________________________________
>     > Owasp-boston mailing list
>     > Owasp-boston at lists.owasp.org <mailto:Owasp-boston at lists.owasp.org>
>     > https://lists.owasp.org/mailman/listinfo/owasp-boston
>     >
>
>     _______________________________________________
>     Owasp-boston mailing list
>     Owasp-boston at lists.owasp.org <mailto:Owasp-boston at lists.owasp.org>
>     https://lists.owasp.org/mailman/listinfo/owasp-boston
>
>
> ------------------------------------------------------------------------
>
> _______________________________________________
> Owasp-boston mailing list
> Owasp-boston at lists.owasp.org
> https://lists.owasp.org/mailman/listinfo/owasp-boston
>   

-- 

Best regards

Javed
-------------------------------------------------------------------
Javed Ikbal, CISSP, CISM, CISA
Principal
www.zsquad.com | E: javed at zsquad.com
P: 617 780 9052 | F: 781 723 0590
-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091112/2d39f53a/attachment-0001.html 

From sprimost at cox.net  Thu Nov 12 10:34:21 2009
From: sprimost at cox.net (Steve Primost)
Date: Thu, 12 Nov 2009 10:34:21 -0500
Subject: [Owasp-boston] Checking file types on upload
In-Reply-To: <1452bf810911120715l56176cb7q690a6ad691d7137a@mail.gmail.com>
References: <4AF04A2D.500@codenomicon.com>	<09DED73804A95B42ACFEFF57AEA3B24E070D891F@MAIL3.AD.Brown.Edu>	<4AFC224E.5060300@zsquad.com>
	<1452bf810911120715l56176cb7q690a6ad691d7137a@mail.gmail.com>
Message-ID: <4AFC2AFD.3010406@cox.net>

An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091112/662778c7/attachment.html 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: sprimost.vcf
Type: text/x-vcard
Size: 366 bytes
Desc: not available
Url : https://lists.owasp.org/pipermail/owasp-boston/attachments/20091112/662778c7/attachment.vcf 

From paul_kozlov at harvard.edu  Thu Nov 12 10:39:10 2009
From: paul_kozlov at harvard.edu (Kozlov, Paul)
Date: Thu, 12 Nov 2009 10:39:10 -0500
Subject: [Owasp-boston] Checking file types on upload
In-Reply-To: <4AFC28CC.7020505@zsquad.com>
References: <4AF04A2D.500@codenomicon.com>	<09DED73804A95B42ACFEFF57AEA3B24E070D891F@MAIL3.AD.Brown.Edu>	<4AFC224E.5060300@zsquad.com><1452bf810911120715l56176cb7q690a6ad691d7137a@mail.gmail.com>
	<4AFC28CC.7020505@zsquad.com>
Message-ID: <C5E1527F129BF545A250B5AD0B0C26EF0566ACE8@ENTWEXMB0000000.university.harvard.edu>

On some Linux flavors you can use file command to test the file types.
It will recognize php scripts even if files do not have the right
extension. Here is a sample output I have on RHEL 4 system:

 

$ file testfile.php

testfile.php: PHP script text

$ mv  testfile.php testfile.jpg

$ file testfile.jpg

testfile.jpg: PHP script text

 

To automate that find + grep may be used as follows, and these files can
be singled out for manual examination or be deleted right away if you
wish.

 

$ find . -name '*.jpg' -exec file {} \; | grep PHP

./testfile.jpg: PHP script text

 

----------------------------------------------------

Paul Kozlov, CISSP

Operations Manager, IT Infrastructure Services

Harvard University UIS

________________________________

From: owasp-boston-bounces at lists.owasp.org
[mailto:owasp-boston-bounces at lists.owasp.org] On Behalf Of Javed Ikbal
Sent: Thursday, November 12, 2009 10:25 AM
To: owasp-boston at lists.owasp.org
Subject: Re: [Owasp-boston] Checking file types on upload

 

Yes,  "<?" may exist in an image file.

But:
Given the upload dirs may have hundreds, if not thousands of files,
finding the suspicious files would be a chore--hence my recommendation.

grep will narrow down the list of suspects very quickly for the manual
examination, or the output of the grep could be piped to the php syntax
check as you suggested.

Javed

Stephane Corlosquet wrote: 

	I guess you can run grep in the upload dirs to catch misnamed
php files
	that have already been upload, something like:
	
	grep -R "<?" *.jpg *.gif *.png


An image file could contain "<?" but that does not mean it's a PHP file.
Maybe you could run php --syntax-check on the command line to check the
syntax of a suspicious file.

Steph.

On Thu, Nov 12, 2009 at 9:57 AM, Javed Ikbal <javed at zsquad.com> wrote:

The most common (and error-prone) method is to use the $_FILES array.
The browser supplies this information to the server, and IE is notorious
for using the extension to tell the server what the filetype is.

As it uses the filename extension, it is easy to fool (or make the
mistake)

With PHP 5.3 or above, you can use fileinfo

http://www.php.net/manual/en/function.finfo-file.php

For older versions, fileinfo is a loadable module.

I guess you can run grep in the upload dirs to catch misnamed php files
that have already been upload, something like:

grep -R "<?" *.jpg *.gif *.png

Regards

Javed


Laverty, Patrick wrote:
> Sorry for all the questions lately, but I'm wondering if someone has
> come up with a reliable way to check actual file types when they get
> uploaded to a server, preferably with PHP.  We've had some issues
where
> people uploaded php files with a .jpg or .gif extension, so they
slipped
> by for a while.
>
> We are turning off php in upload directories, among other security
> steps, but I just wanted to see if I could do more than just checking
> the file extension.  Looking for that extra layer of security.
>
> Thanks!
>
> Patrick Laverty
> Brown University
> _______________________________________________
> Owasp-boston mailing list
> Owasp-boston at lists.owasp.org
> https://lists.owasp.org/mailman/listinfo/owasp-boston
>

_______________________________________________
Owasp-boston mailing list
Owasp-boston at lists.owasp.org
https://lists.owasp.org/mailman/listinfo/owasp-boston





 



________________________________



 
_______________________________________________
Owasp-boston mailing list
Owasp-boston at lists.owasp.org
https://lists.owasp.org/mailman/listinfo/owasp-boston
  

 

-- 

Best regards 

Javed
-------------------------------------------------------------------
Javed Ikbal, CISSP, CISM, CISA
Principal
www.zsquad.com | E: javed at zsquad.com
P: 617 780 9052 | F: 781 723 0590

-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091112/a4d5c3eb/attachment.html 

From cclark at isecpartners.com  Thu Nov 12 10:53:07 2009
From: cclark at isecpartners.com (Chris Clark)
Date: Thu, 12 Nov 2009 07:53:07 -0800
Subject: [Owasp-boston] Checking file types on upload
In-Reply-To: <4AFC2AFD.3010406@cox.net>
References: <4AF04A2D.500@codenomicon.com>
	<09DED73804A95B42ACFEFF57AEA3B24E070D891F@MAIL3.AD.Brown.Edu>
	<4AFC224E.5060300@zsquad.com>
	<1452bf810911120715l56176cb7q690a6ad691d7137a@mail.gmail.com>
	<4AFC2AFD.3010406@cox.net>
Message-ID: <7E3B942D6F9AE64EA28CE80B7283C1EC3152299DC9@exch01.isecpartners.com>

> For instance, a program that only reads GIF and JPG would render correctly (return code 0) and then we would export to the "intermediate" format necessary for conversion into a fax later in the process.> 

This is a good approach. Running uploaded images through an image processing step that removes EXIF data, resizes the image, or converts between formats will generally stop most malicious image attacks. Unfortunately, this process may affect image quality and the image processing tools could be vulnerable to buffer overflows and other code execution vulnerabilities.

We recommend running the image cleansing process on a dedicated machine or as a low-privilege service account. Additionally, as the original poster mentioned, disable PHP execution in the upload directory.

Chris


Chris Clark
Principal Security Consultant
iSEC Partners, Inc. - NYC 
cclark at isecpartners.com  - 206-963-2513
4FC4 B854 C4FE A6D5 3CFF 
1DFF FA31 71BB BFBD 33C0




From EZiots at Lifespan.org  Thu Nov 12 11:06:42 2009
From: EZiots at Lifespan.org (Ziots, Edward)
Date: Thu, 12 Nov 2009 11:06:42 -0500
Subject: [Owasp-boston] Spear Phishing attacks on the rise?
In-Reply-To: <C5E1527F129BF545A250B5AD0B0C26EF0566ACE8@ENTWEXMB0000000.university.harvard.edu>
References: <4AF04A2D.500@codenomicon.com>	<09DED73804A95B42ACFEFF57AEA3B24E070D891F@MAIL3.AD.Brown.Edu>	<4AFC224E.5060300@zsquad.com><1452bf810911120715l56176cb7q690a6ad691d7137a@mail.gmail.com><4AFC28CC.7020505@zsquad.com>
	<C5E1527F129BF545A250B5AD0B0C26EF0566ACE8@ENTWEXMB0000000.university.harvard.edu>
Message-ID: <CBD73348AD43344DAE3736A72831CDC90653D6FE@LSCOEXCH1.lsmaster.lifespan.org>

Just checking if anyone else is seeing a string of  phishing attacks
hitting the email waves. 

 

I just was privy to a targeted spear phishing attack this morning, with
account information trying to be harvested from Bank of America Account
holders, and the second was a fast-flux domain phishing attack, with the
following verbage, which takes you to a site that wants you to download
an obvious trojan horse exe file which they say is a self extracting pdf
file. 

 

Maybe some interesting things a-foot as the holidays are coming up it
seems. Like to see if anyone else is also seeing these type of things
roaming around,. 

 

Dear bank account holder,

The ACH transaction, recently initiated from your bank account (by you
or any other person), was rejected by the Electronic Payments
Association. Please review the transaction report by clicking the link
below:

Unauthorized ACH Transaction Report

Z

 

Edward Ziots

Network Engineer

Lifespan Organization

MCSE,MCSA,MCP+I, ME, CCA, Security +, Network +

eziots at lifespan.org

Phone:401-639-3505

-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091112/70090123/attachment.html 

From Jim.Weiler at starwoodhotels.com  Mon Nov 16 09:57:09 2009
From: Jim.Weiler at starwoodhotels.com (Weiler, Jim)
Date: Mon, 16 Nov 2009 09:57:09 -0500
Subject: [Owasp-boston] OWASP tomorrow mtg
Message-ID: <0D8299A04960AE4C8906052835761AF103BFD23E@STCEXCPMB02.corp.star>

Hi Folks,

After attending pen test training at OWASP AppSec 2009 last week, I
realized there are even more tools out there than I was aware of.  So
just to be clear, even though my topic description had the phrase
'what's out there', tomorrow I'm only going to talk about  web
application vulnerability scannering based on the 3 main commercial
tools I've been looking at and have talked to people about, Cenzic
Hailstorm; IBM AppScan and HP WebInspect. The discussion points will
apply to various other commercial and free tools, but there's not enough
time to cover them all. I'll do another presentation on the rest, maybe
in Jan.  

 

Also, remember the Mush Hakhinian will be presenting his experience with
the free source code quality scanning tool Sonar at tomorrow's meeting.
His company is also buying the pizza.

 

Jim Weiler   CISSP  CSSLP

Starwood Hotels and Resorts

Sr. Mgr. Information Security Risk Assessment

Office - 781 356 0067

Cell - 781 654 6048



This electronic message transmission contains information from the Company that may be proprietary, confidential and/or privileged. 
The information is intended only for the use of the individual(s) or entity named above.  If you are not the intended recipient, be 
aware that any disclosure, copying or distribution or use of the contents of this information is prohibited.  If you have received 
this electronic transmission in error, please notify the sender immediately by replying to the address listed in the "From:" field. 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091116/a8853f1a/attachment.html 

From Patrick_Laverty at brown.edu  Mon Nov 23 08:34:09 2009
From: Patrick_Laverty at brown.edu (Laverty, Patrick)
Date: Mon, 23 Nov 2009 08:34:09 -0500
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week
Message-ID: <09DED73804A95B42ACFEFF57AEA3B24E071F9850@MAIL3.AD.Brown.Edu>

And they're both about the same statement.

 

Jim stated that the scanners will "Only going to find 10 - 20% of vulns
- low hanging fruit"

 

My two questions are:

1.      How do we know that they will only find that number?  If we know
they're missing 80-90%, how did we find them to count those that were
missing?

2.      I've read that the most effective and thorough scanning is to
get 4-5 different scanners and use them all, as they will find some
different vulnerabilities.  Has anyone done this?  What did you find
when you look at all the data from multiple scanners and how varied are
they at finding different true positive results?

 

Thank you!

 

Patrick

 

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091123/bbb0bf74/attachment.html 

From smatsumoto at cigital.com  Mon Nov 23 09:59:59 2009
From: smatsumoto at cigital.com (Scott Matsumoto)
Date: Mon, 23 Nov 2009 09:59:59 -0500
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week
In-Reply-To: <09DED73804A95B42ACFEFF57AEA3B24E071F9850@MAIL3.AD.Brown.Edu>
References: <09DED73804A95B42ACFEFF57AEA3B24E071F9850@MAIL3.AD.Brown.Edu>
Message-ID: <41945506397C0C4886A8C5BFF089B5CA3AFA29C84B@va-mailhub.cigital.com>

Patrick,

In terms of your first question, I can answer from our experience doing assessments that mix manual and tools for both static and dynamic analysis that those percentages are roughly right.  I don't have quantitative numbers to back up that claim since our analysis isn't based solely on raw numbers.

In terms of employing multiple scanners, we use both AppScan and HP (SPI) and I don't see enough difference in coverage to run both.  I think that using different tools that are looking for different types of problems or using a mix of static and dynamic tools provides enough of a win to justify the extra cost (both dollar and human).

Using this mix of static and dynamic as well as manual and tool-based techniques is exactly what we do.  In the end, however, I find the vulnerabilities that have the highest business-related impact are those that we find manually.  I think it's because many of the defects that the tools miss involve information disclosure and the tools don't have enough intelligence to distinguish what data one is should or should not see.

________________________________
From: owasp-boston-bounces at lists.owasp.org [owasp-boston-bounces at lists.owasp.org] On Behalf Of Laverty, Patrick [Patrick_Laverty at brown.edu]
Sent: Monday, November 23, 2009 8:34 AM
To: owasp-boston at lists.owasp.org
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week

And they?re both about the same statement.

Jim stated that the scanners will ?Only going to find 10 ? 20% of vulns ? low hanging fruit?

My two questions are:

1.      How do we know that they will only find that number?  If we know they?re missing 80-90%, how did we find them to count those that were missing?

2.      I?ve read that the most effective and thorough scanning is to get 4-5 different scanners and use them all, as they will find some different vulnerabilities.  Has anyone done this?  What did you find when you look at all the data from multiple scanners and how varied are they at finding different true positive results?

Thank you!

Patrick




From dan.crowley at gmail.com  Mon Nov 23 10:33:16 2009
From: dan.crowley at gmail.com (Dan Crowley)
Date: Mon, 23 Nov 2009 10:33:16 -0500
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week
In-Reply-To: <09DED73804A95B42ACFEFF57AEA3B24E071F9850@MAIL3.AD.Brown.Edu>
References: <09DED73804A95B42ACFEFF57AEA3B24E071F9850@MAIL3.AD.Brown.Edu>
Message-ID: <bc0c5dbd0911230733u19ac50a6i95908a0a186753e4@mail.gmail.com>

The statistic is a bit odd, to be sure, but I think the intent was to convey
that 10-20% of TYPES of vulnerabilities can be found, and that automated
scanners are not a complete solution. Remember, 70% of statistics are
completely made up. ;)

Using multiple scanners can be useful since the weaknesses of one can be
complemented by the strengths of the others, but imagine that you have an
oddly named database backup file (complete with plaintext admin passwords)
sitting in a web-accessible, indexable directory. An automated scanner will
likely never find this, but with a good web spider and a keen eye, a pen
tester certainly will, and this is DEFINITELY a vulnerability.

I suggest using an intercepting proxy like OWASP's WebScarab or Burp Suite
or Paros when doing manual web app pen testing to complement your automated
scans.


On Nov 23, 2009 8:34 AM, "Laverty, Patrick" <Patrick_Laverty at brown.edu>
wrote:

 And they?re both about the same statement.



Jim stated that the scanners will ?Only going to find 10 ? 20% of vulns ?
low hanging fruit?



My two questions are:

1.      How do we know that they will only find that number?  If we know
they?re missing 80-90%, how did we find them to count those that were
missing?

2.      I?ve read that the most effective and thorough scanning is to get
4-5 different scanners and use them all, as they will find some different
vulnerabilities.  Has anyone done this?  What did you find when you look at
all the data from multiple scanners and how varied are they at finding
different true positive results?



Thank you!



Patrick







_______________________________________________
Owasp-boston mailing list
Owasp-boston at lists.owasp.org
https://lists.owasp.org/mailman/listinfo/owasp-boston
-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091123/2b9c5977/attachment.html 

From ceng at Veracode.com  Mon Nov 23 11:23:52 2009
From: ceng at Veracode.com (Chris Eng)
Date: Mon, 23 Nov 2009 11:23:52 -0500
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week
In-Reply-To: <bc0c5dbd0911230733u19ac50a6i95908a0a186753e4@mail.gmail.com>
References: <09DED73804A95B42ACFEFF57AEA3B24E071F9850@MAIL3.AD.Brown.Edu>
	<bc0c5dbd0911230733u19ac50a6i95908a0a186753e4@mail.gmail.com>
Message-ID: <5A9D3476391AD649B24ED4F9FDA2A8DD0219CBD723@orbital.Veracode.local>

I'll preface by saying that I wasn't at Jim's presentation. That said, I doubt that the intent was to say that only 10-20% of the TYPES of vulnerabilities can be found by automated scanners.  They are typically capable of finding much more than that, unless you are defining very narrow categories for different manifestations of business logic flaws.

Most scanners do look for backup files.  It won't understand the significance of what it found, but chances are it'll find the file.

The other point many people seem to miss when claiming that automated scanners only find 10-20% of vulnerabilities is that their methodology for making such claims is flawed.  They take manual penetration test results, then run a web scanner, and ask "how many of the manual results did the scanner find?" They don't always ask "how many of the automated results did the human find?"  Usually a human won't take the time (nor should they) to find 100 examples of XSS.  They'll find a few representative URLs, add them to the report, and move on.  Time is limited and you don't want to waste it on basic issues that a scanner can find, you want to find the bombshells that require a brain to discover.  The problem is, most of the time, when you give a dev 3 XSS flaws, they'll fix those 3 and say they're done, so it's important to give as many examples as possible.

The bottom line is that these manual vs. automated comparisons are kind of silly in the first place.  Automation gets you a lot, relative to its price.  Manual testing had better find a lot more stuff because it's significantly more expensive.  Use automation across the board, and augment with manual testing for critical assets.  Hit the most critical assets from every possible angle.



From: owasp-boston-bounces at lists.owasp.org [mailto:owasp-boston-bounces at lists.owasp.org] On Behalf Of Dan Crowley
Sent: Monday, November 23, 2009 10:33 AM
To: Laverty, Patrick; owasp-boston at lists.owasp.org
Subject: Re: [Owasp-boston] 2 Questions about Jim's Presentation last week


The statistic is a bit odd, to be sure, but I think the intent was to convey that 10-20% of TYPES of vulnerabilities can be found, and that automated scanners are not a complete solution. Remember, 70% of statistics are completely made up. ;)

Using multiple scanners can be useful since the weaknesses of one can be complemented by the strengths of the others, but imagine that you have an oddly named database backup file (complete with plaintext admin passwords) sitting in a web-accessible, indexable directory. An automated scanner will likely never find this, but with a good web spider and a keen eye, a pen tester certainly will, and this is DEFINITELY a vulnerability.

I suggest using an intercepting proxy like OWASP's WebScarab or Burp Suite or Paros when doing manual web app pen testing to complement your automated scans.


On Nov 23, 2009 8:34 AM, "Laverty, Patrick" <Patrick_Laverty at brown.edu<mailto:Patrick_Laverty at brown.edu>> wrote:
And they're both about the same statement.

Jim stated that the scanners will "Only going to find 10 - 20% of vulns - low hanging fruit"

My two questions are:

1.      How do we know that they will only find that number?  If we know they're missing 80-90%, how did we find them to count those that were missing?

2.      I've read that the most effective and thorough scanning is to get 4-5 different scanners and use them all, as they will find some different vulnerabilities.  Has anyone done this?  What did you find when you look at all the data from multiple scanners and how varied are they at finding different true positive results?

Thank you!

Patrick




_______________________________________________
Owasp-boston mailing list
Owasp-boston at lists.owasp.org<mailto:Owasp-boston at lists.owasp.org>
https://lists.owasp.org/mailman/listinfo/owasp-boston
-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091123/4991b1fa/attachment-0001.html 

From Jim.Weiler at starwoodhotels.com  Mon Nov 23 13:08:25 2009
From: Jim.Weiler at starwoodhotels.com (Weiler, Jim)
Date: Mon, 23 Nov 2009 13:08:25 -0500
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week
In-Reply-To: <09DED73804A95B42ACFEFF57AEA3B24E071F9850@MAIL3.AD.Brown.Edu>
References: <09DED73804A95B42ACFEFF57AEA3B24E071F9850@MAIL3.AD.Brown.Edu>
Message-ID: <0D8299A04960AE4C8906052835761AF103C6748B@STCEXCPMB02.corp.star>

I had the same question when I first heard these kinds of numbers, and
it's true that you can't know the exact number of vulnerabilities that
are really there, so you can't know what % of them any tool is finding.
I got the 10 - 20 % number from other forums, presentations, and talking
to people, and from all those sources it is still a guess based on the
amount of additional vulnerabilities people have found using other types
of tools and procedures, like source code scanning and penetration
testing. Also, the number of vulnerabilities found by a scanner will
vary depending on how much customization is done with the scanner
(recorded manual navigations, values to try for form fields etc). The 10
- 20% is assuming some basic customization.

 

I just read Chris Engs email - and as he said, those numbers are for the
number of vulnerabilities not the number of types of vulnerabilities.
Also as he said, you can run a scan and if it tries 100 variants for the
same vulnerable element (form field or url parameter etc) it might
report that as 100 vulnerabilities, even though it is a change in 1
piece of code that might fix it. You might run the same scan again but
configure it to limit the number of attack variants to 3, in which case
the number of vulnerabilities found would be 3.  Those two settings
would then have a big impact on the % found with scanning vrs something
else or vrs the assumed actual total vulnerabilities.

 

I think there are 2 points to get from those numbers, regardles of
whether the number is really 20% or 50% - 

1. they are the easiest vulnerabilities to exploit 

2. There are lots of other vulnerabilities that can only be found by
other means, mainly pen testing.

 

Jim Weiler   CISSP  CSSLP

Starwood Hotels and Resorts

Sr. Mgr. Information Security Risk Assessment

Office - 781 356 0067

Cell - 781 654 6048

From: owasp-boston-bounces at lists.owasp.org
[mailto:owasp-boston-bounces at lists.owasp.org] On Behalf Of Laverty,
Patrick
Sent: Monday, November 23, 2009 8:34 AM
To: owasp-boston at lists.owasp.org
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week

 

And they're both about the same statement.

 

Jim stated that the scanners will "Only going to find 10 - 20% of vulns
- low hanging fruit"

 

My two questions are:

1.      How do we know that they will only find that number?  If we know
they're missing 80-90%, how did we find them to count those that were
missing?

2.      I've read that the most effective and thorough scanning is to
get 4-5 different scanners and use them all, as they will find some
different vulnerabilities.  Has anyone done this?  What did you find
when you look at all the data from multiple scanners and how varied are
they at finding different true positive results?

 

Thank you!

 

Patrick

 

 

 



This electronic message transmission contains information from the Company that may be proprietary, confidential and/or privileged. 
The information is intended only for the use of the individual(s) or entity named above.  If you are not the intended recipient, be 
aware that any disclosure, copying or distribution or use of the contents of this information is prohibited.  If you have received 
this electronic transmission in error, please notify the sender immediately by replying to the address listed in the "From:" field. 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091123/a1f9963a/attachment.html 

From ceng at Veracode.com  Mon Nov 23 14:07:31 2009
From: ceng at Veracode.com (Chris Eng)
Date: Mon, 23 Nov 2009 14:07:31 -0500
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week
In-Reply-To: <0D8299A04960AE4C8906052835761AF103C6748B@STCEXCPMB02.corp.star>
References: <09DED73804A95B42ACFEFF57AEA3B24E071F9850@MAIL3.AD.Brown.Edu>
	<0D8299A04960AE4C8906052835761AF103C6748B@STCEXCPMB02.corp.star>
Message-ID: <5A9D3476391AD649B24ED4F9FDA2A8DD0219CBD812@orbital.Veracode.local>

> I just read Chris Engs email - and as he said, those numbers are for the
> number of vulnerabilities not the number of types of vulnerabilities. Also as
> he said, you can run a scan and if it tries 100 variants for the same
> vulnerable element (form field or url parameter etc) it might report that as
> 100 vulnerabilities, even though it is a change in 1 piece of code that might
> fix it. You might run the same scan again but configure it to limit the
> number of attack variants to 3, in which case the number of vulnerabilities
> found would be 3.  Those two settings would then have a big impact on the %
> found with scanning vrs something else or vrs the assumed actual total
> vulnerabilities.

Generally if there are multiple attack strings on a single parameter, it's reported as one vulnerability.  However, if there are multiple vulnerable parameters on a single URL (e.g. "?foo=ATTACK" and "?bar=ATTACK"), then those would be reported separately.  At least that's how we do it.  Are there scanners out there that count each variation on the attack string as a separate vulnerability? That would be pretty bogus for sure.




From Jim.Weiler at starwoodhotels.com  Mon Nov 23 14:56:07 2009
From: Jim.Weiler at starwoodhotels.com (Weiler, Jim)
Date: Mon, 23 Nov 2009 14:56:07 -0500
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week
In-Reply-To: <5A9D3476391AD649B24ED4F9FDA2A8DD0219CBD812@orbital.Veracode.local>
References: <09DED73804A95B42ACFEFF57AEA3B24E071F9850@MAIL3.AD.Brown.Edu><0D8299A04960AE4C8906052835761AF103C6748B@STCEXCPMB02.corp.star>
	<5A9D3476391AD649B24ED4F9FDA2A8DD0219CBD812@orbital.Veracode.local>
Message-ID: <0D8299A04960AE4C8906052835761AF103C67511@STCEXCPMB02.corp.star>

I think you're right for the basic summary or 'executive' stats of the 3
commercial scanners I was talking about - different variants for the
same vulnerable parameter do not add to the vulnerability count. 

AppScan standard (can't remember if the others do too - anybody know?)
lets you control the number of variants (attacks strings per parameter)
that are included in reports, and I had an automated analysis of the
reports set up, so if I didn't set the # of variants to 1, I'd get an
inflated count. That's what I was thinking of. 

Chris, what do you say to customers when they ask things like (I'm
assuming some of them ask this) 'what % of the vulnerabilities will your
(or some other) application vulnerability scanner find?'. 

Jim Weiler   CISSP  CSSLP
Starwood Hotels and Resorts
Sr. Mgr. Information Security Risk Assessment
Office - 781 356 0067
Cell - 781 654 6048
-----Original Message-----
From: owasp-boston-bounces at lists.owasp.org
[mailto:owasp-boston-bounces at lists.owasp.org] On Behalf Of Chris Eng
Sent: Monday, November 23, 2009 2:08 PM
To: Weiler, Jim; Laverty, Patrick; owasp-boston at lists.owasp.org
Subject: Re: [Owasp-boston] 2 Questions about Jim's Presentation last
week

> I just read Chris Engs email - and as he said, those numbers are for
the
> number of vulnerabilities not the number of types of vulnerabilities.
Also as
> he said, you can run a scan and if it tries 100 variants for the same
> vulnerable element (form field or url parameter etc) it might report
that as
> 100 vulnerabilities, even though it is a change in 1 piece of code
that might
> fix it. You might run the same scan again but configure it to limit
the
> number of attack variants to 3, in which case the number of
vulnerabilities
> found would be 3.  Those two settings would then have a big impact on
the %
> found with scanning vrs something else or vrs the assumed actual total
> vulnerabilities.

Generally if there are multiple attack strings on a single parameter,
it's reported as one vulnerability.  However, if there are multiple
vulnerable parameters on a single URL (e.g. "?foo=ATTACK" and
"?bar=ATTACK"), then those would be reported separately.  At least
that's how we do it.  Are there scanners out there that count each
variation on the attack string as a separate vulnerability? That would
be pretty bogus for sure.



_______________________________________________
Owasp-boston mailing list
Owasp-boston at lists.owasp.org
https://lists.owasp.org/mailman/listinfo/owasp-boston


This electronic message transmission contains information from the Company that may be proprietary, confidential and/or privileged. 
The information is intended only for the use of the individual(s) or entity named above.  If you are not the intended recipient, be 
aware that any disclosure, copying or distribution or use of the contents of this information is prohibited.  If you have received 
this electronic transmission in error, please notify the sender immediately by replying to the address listed in the "From:" field. 


From dharip at ptc.com  Mon Nov 23 15:25:20 2009
From: dharip at ptc.com (Harip, Devdatta)
Date: Mon, 23 Nov 2009 15:25:20 -0500
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week
In-Reply-To: <0D8299A04960AE4C8906052835761AF103C67511@STCEXCPMB02.corp.star>
References: <09DED73804A95B42ACFEFF57AEA3B24E071F9850@MAIL3.AD.Brown.Edu><0D8299A04960AE4C8906052835761AF103C6748B@STCEXCPMB02.corp.star><5A9D3476391AD649B24ED4F9FDA2A8DD0219CBD812@orbital.Veracode.local>
	<0D8299A04960AE4C8906052835761AF103C67511@STCEXCPMB02.corp.star>
Message-ID: <DE3BF7D023B1FB44A390EABB73048FF5021BBBCE@HQ-MAIL3.ptcnet.ptc.com>

There is an option in WebInspect for the final report to show only 1
vulnerability even if there are multiple attack strings on a single
parameter.

Thanks,
Devdatta

-----Original Message-----
From: owasp-boston-bounces at lists.owasp.org
[mailto:owasp-boston-bounces at lists.owasp.org] On Behalf Of Weiler, Jim
Sent: Monday, November 23, 2009 2:56 PM
To: Chris Eng; Laverty, Patrick; owasp-boston at lists.owasp.org
Subject: Re: [Owasp-boston] 2 Questions about Jim's Presentation last
week

I think you're right for the basic summary or 'executive' stats of the 3
commercial scanners I was talking about - different variants for the
same vulnerable parameter do not add to the vulnerability count. 

AppScan standard (can't remember if the others do too - anybody know?)
lets you control the number of variants (attacks strings per parameter)
that are included in reports, and I had an automated analysis of the
reports set up, so if I didn't set the # of variants to 1, I'd get an
inflated count. That's what I was thinking of. 

Chris, what do you say to customers when they ask things like (I'm
assuming some of them ask this) 'what % of the vulnerabilities will your
(or some other) application vulnerability scanner find?'. 

Jim Weiler   CISSP  CSSLP
Starwood Hotels and Resorts
Sr. Mgr. Information Security Risk Assessment
Office - 781 356 0067
Cell - 781 654 6048
-----Original Message-----
From: owasp-boston-bounces at lists.owasp.org
[mailto:owasp-boston-bounces at lists.owasp.org] On Behalf Of Chris Eng
Sent: Monday, November 23, 2009 2:08 PM
To: Weiler, Jim; Laverty, Patrick; owasp-boston at lists.owasp.org
Subject: Re: [Owasp-boston] 2 Questions about Jim's Presentation last
week

> I just read Chris Engs email - and as he said, those numbers are for
the
> number of vulnerabilities not the number of types of vulnerabilities.
Also as
> he said, you can run a scan and if it tries 100 variants for the same
> vulnerable element (form field or url parameter etc) it might report
that as
> 100 vulnerabilities, even though it is a change in 1 piece of code
that might
> fix it. You might run the same scan again but configure it to limit
the
> number of attack variants to 3, in which case the number of
vulnerabilities
> found would be 3.  Those two settings would then have a big impact on
the %
> found with scanning vrs something else or vrs the assumed actual total
> vulnerabilities.

Generally if there are multiple attack strings on a single parameter,
it's reported as one vulnerability.  However, if there are multiple
vulnerable parameters on a single URL (e.g. "?foo=ATTACK" and
"?bar=ATTACK"), then those would be reported separately.  At least
that's how we do it.  Are there scanners out there that count each
variation on the attack string as a separate vulnerability? That would
be pretty bogus for sure.



_______________________________________________
Owasp-boston mailing list
Owasp-boston at lists.owasp.org
https://lists.owasp.org/mailman/listinfo/owasp-boston


This electronic message transmission contains information from the
Company that may be proprietary, confidential and/or privileged. 
The information is intended only for the use of the individual(s) or
entity named above.  If you are not the intended recipient, be 
aware that any disclosure, copying or distribution or use of the
contents of this information is prohibited.  If you have received 
this electronic transmission in error, please notify the sender
immediately by replying to the address listed in the "From:" field. 

_______________________________________________
Owasp-boston mailing list
Owasp-boston at lists.owasp.org
https://lists.owasp.org/mailman/listinfo/owasp-boston

From james at architectbook.com  Mon Nov 23 15:45:33 2009
From: james at architectbook.com (james at architectbook.com)
Date: Mon, 23 Nov 2009 13:45:33 -0700
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week
Message-ID: <20091123134533.1b34e4c3b93181cbb56b6df77bbedd57.87ab08575b.wbe@email.secureserver.net>

An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091123/d0cd8760/attachment.html 

From smatsumoto at cigital.com  Mon Nov 23 15:57:15 2009
From: smatsumoto at cigital.com (Scott Matsumoto)
Date: Mon, 23 Nov 2009 15:57:15 -0500
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week
In-Reply-To: <20091123134533.1b34e4c3b93181cbb56b6df77bbedd57.87ab08575b.wbe@email.secureserver.net>
References: <20091123134533.1b34e4c3b93181cbb56b6df77bbedd57.87ab08575b.wbe@email.secureserver.net>
Message-ID: <41945506397C0C4886A8C5BFF089B5CA3AFA29C858@va-mailhub.cigital.com>

I don't know the fate of O2, so I cannot comment on it.

SAFES is an emerging standard to standardize tool findings, so, there's value in standardizing them.  I think it's very early for SAFES.

________________________________
From: james at architectbook.com [james at architectbook.com]
Sent: Monday, November 23, 2009 3:45 PM
To: Scott Matsumoto
Cc: owasp-boston at lists.owasp.org
Subject: RE: [Owasp-boston] 2 Questions about Jim's Presentation last week

OWASP Board member Dinis Cruz frequently discusses the O2 Platform. I am curious if anyone knows whether IBM will step up and rally behind it or let it die on the grapevine? Also would love to know if there is merit in having Appscan, WebInspect and other dynamic analysis tools emit a findings file in a standard format.

-------- Original Message --------
Subject: Re: [Owasp-boston] 2 Questions about Jim's Presentation last
week
From: Scott Matsumoto <smatsumoto at cigital.com>
Date: Mon, November 23, 2009 9:59 am
To: "Laverty, Patrick" <Patrick_Laverty at brown.edu>,
"owasp-boston at lists.owasp.org" <owasp-boston at lists.owasp.org>

Patrick,

In terms of your first question, I can answer from our experience doing assessments that mix manual and tools for both static and dynamic analysis that those percentages are roughly right. I don't have quantitative numbers to back up that claim since our analysis isn't based solely on raw numbers.

In terms of employing multiple scanners, we use both AppScan and HP (SPI) and I don't see enough difference in coverage to run both. I think that using different tools that are looking for different types of problems or using a mix of static and dynamic tools provides enough of a win to justify the extra cost (both dollar and human).

Using this mix of static and dynamic as well as manual and tool-based techniques is exactly what we do. In the end, however, I find the vulnerabilities that have the highest business-related impact are those that we find manually. I think it's because many of the defects that the tools miss involve information disclosure and the tools don't have enough intelligence to distinguish what data one is should or should not see.

________________________________
From: owasp-boston-bounces at lists.owasp.org [owasp-boston-bounces at lists.owasp.org] On Behalf Of Laverty, Patrick [Patrick_Laverty at brown.edu]
Sent: Monday, November 23, 2009 8:34 AM
To: owasp-boston at lists.owasp.org
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week

And they?re both about the same statement.

Jim stated that the scanners will ?Only going to find 10 ? 20% of vulns ? low hanging fruit?

My two questions are:

1. How do we know that they will only find that number? If we know they?re missing 80-90%, how did we find them to count those that were missing?

2. I?ve read that the most effective and thorough scanning is to get 4-5 different scanners and use them all, as they will find some different vulnerabilities. Has anyone done this? What did you find when you look at all the data from multiple scanners and how varied are they at finding different true positive results?

Thank you!

Patrick



_______________________________________________
Owasp-boston mailing list
Owasp-boston at lists.owasp.org
https://lists.owasp.org/mailman/listinfo/owasp-boston

From james at architectbook.com  Mon Nov 23 16:00:02 2009
From: james at architectbook.com (james at architectbook.com)
Date: Mon, 23 Nov 2009 14:00:02 -0700
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week
Message-ID: <20091123140002.1b34e4c3b93181cbb56b6df77bbedd57.66f5324ed1.wbe@email.secureserver.net>

An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091123/5037c5a1/attachment.html 

From smatsumoto at cigital.com  Mon Nov 23 16:53:12 2009
From: smatsumoto at cigital.com (Scott Matsumoto)
Date: Mon, 23 Nov 2009 16:53:12 -0500
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week
In-Reply-To: <20091123140002.1b34e4c3b93181cbb56b6df77bbedd57.66f5324ed1.wbe@email.secureserver.net>
References: <20091123140002.1b34e4c3b93181cbb56b6df77bbedd57.66f5324ed1.wbe@email.secureserver.net>
Message-ID: <41945506397C0C4886A8C5BFF089B5CA3AFA29C85B@va-mailhub.cigital.com>

One of guys at Cigital is organizing it with help from the feds and tool vendors.  A draft is due out after the first of the year.

________________________________
From: james at architectbook.com [james at architectbook.com]
Sent: Monday, November 23, 2009 4:00 PM
To: Scott Matsumoto
Cc: owasp-boston at lists.owasp.org
Subject: RE: [Owasp-boston] 2 Questions about Jim's Presentation last week

Do you have a URL that describes SAFES in more detail?

-------- Original Message --------
Subject: RE: [Owasp-boston] 2 Questions about Jim's Presentation last
week
From: Scott Matsumoto <smatsumoto at cigital.com>
Date: Mon, November 23, 2009 3:57 pm
To: "james at architectbook.com" <james at architectbook.com>
Cc: "owasp-boston at lists.owasp.org" <owasp-boston at lists.owasp.org>

I don't know the fate of O2, so I cannot comment on it.

SAFES is an emerging standard to standardize tool findings, so, there's value in standardizing them. I think it's very early for SAFES.

________________________________
From: james at architectbook.com [james at architectbook.com]
Sent: Monday, November 23, 2009 3:45 PM
To: Scott Matsumoto
Cc: owasp-boston at lists.owasp.org
Subject: RE: [Owasp-boston] 2 Questions about Jim's Presentation last week

OWASP Board member Dinis Cruz frequently discusses the O2 Platform. I am curious if anyone knows whether IBM will step up and rally behind it or let it die on the grapevine? Also would love to know if there is merit in having Appscan, WebInspect and other dynamic analysis tools emit a findings file in a standard format.

-------- Original Message --------
Subject: Re: [Owasp-boston] 2 Questions about Jim's Presentation last
week
From: Scott Matsumoto <smatsumoto at cigital.com>
Date: Mon, November 23, 2009 9:59 am
To: "Laverty, Patrick" <Patrick_Laverty at brown.edu>,
"owasp-boston at lists.owasp.org" <owasp-boston at lists.owasp.org>

Patrick,

In terms of your first question, I can answer from our experience doing assessments that mix manual and tools for both static and dynamic analysis that those percentages are roughly right. I don't have quantitative numbers to back up that claim since our analysis isn't based solely on raw numbers.

In terms of employing multiple scanners, we use both AppScan and HP (SPI) and I don't see enough difference in coverage to run both. I think that using different tools that are looking for different types of problems or using a mix of static and dynamic tools provides enough of a win to justify the extra cost (both dollar and human).

Using this mix of static and dynamic as well as manual and tool-based techniques is exactly what we do. In the end, however, I find the vulnerabilities that have the highest business-related impact are those that we find manually. I think it's because many of the defects that the tools miss involve information disclosure and the tools don't have enough intelligence to distinguish what data one is should or should not see.

________________________________
From: owasp-boston-bounces at lists.owasp.org [owasp-boston-bounces at lists.owasp.org] On Behalf Of Laverty, Patrick [Patrick_Laverty at brown.edu]
Sent: Monday, November 23, 2009 8:34 AM
To: owasp-boston at lists.owasp.org
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week

And they?re both about the same statement.

Jim stated that the scanners will ?Only going to find 10 ? 20% of vulns ? low hanging fruit?

My two questions are:

1. How do we know that they will only find that number? If we know they?re missing 80-90%, how did we find them to count those that were missing?

2. I?ve read that the most effective and thorough scanning is to get 4-5 different scanners and use them all, as they will find some different vulnerabilities. Has anyone done this? What did you find when you look at all the data from multiple scanners and how varied are they at finding different true positive results?

Thank you!

Patrick



_______________________________________________
Owasp-boston mailing list
Owasp-boston at lists.owasp.org
https://lists.owasp.org/mailman/listinfo/owasp-boston

From smatsumoto at cigital.com  Mon Nov 23 17:15:24 2009
From: smatsumoto at cigital.com (Scott Matsumoto)
Date: Mon, 23 Nov 2009 17:15:24 -0500
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week
In-Reply-To: <70566cd40911231410kcbbb8e8g6856bdc786fb535c@mail.gmail.com>
References: <20091123134533.1b34e4c3b93181cbb56b6df77bbedd57.87ab08575b.wbe@email.secureserver.net>
	<41945506397C0C4886A8C5BFF089B5CA3AFA29C858@va-mailhub.cigital.com>,
	<70566cd40911231410kcbbb8e8g6856bdc786fb535c@mail.gmail.com>
Message-ID: <41945506397C0C4886A8C5BFF089B5CA3AFA29C862@va-mailhub.cigital.com>

Neil,

I am only relaying information that work is going on.  My personal opinion is that unless there is some unbiased, 3rd partly (like a NIST) that's validating the standard, it standard doesn't really achieve any level of interop.
________________________________
From: neil.smithline at gmail.com [neil.smithline at gmail.com] On Behalf Of Neil Smithline [owasp.org at smithline.net]
Sent: Monday, November 23, 2009 5:10 PM
To: Scott Matsumoto
Cc: james at architectbook.com; owasp-boston at lists.owasp.org
Subject: Re: [Owasp-boston] 2 Questions about Jim's Presentation last week

No offense Scott, but I would guess that the number of useful standards is several orders of magnitude smaller than the number of standards. I would add several orders of magnitude to that if we compare useful standards to incomplete standards.

IMO, the general question of whether standardizing a data format is useful depends on who you ask.

Microsoft spent years saying "no". Now they sometimes say "yes" (eg: .docx format) and sometimes say "yes, as long as we control the standard" (eg: some WS standards).

Ask RMS, OWASP.org or Dinis Cruz (of O2), and the answer will be "yes".

Clearly MS's responses are based on business issues. Nearly every time MS said "no", the EBM consortium (Everyone But Microsoft) argued strongly for a standard and frequently created some of those many standards that nobody uses.

The open-sourcerers argue yes largely on philosophical reasons.

>From a strictly technical viewpoint, I think that open (ie: publicly available and modifiable) standards  frequently help. The trick is not to stifle progress by being forced to adhere to a standard that is not capable of meeting your requirements.

Hence, open, extensible standards seem the big win. For example, X.509 and SAML both provide a means to support a core functionality, come with common extensions, and allow extensions that can be ignored if a tool does not know how to handle them. Another example is how many Servlet containers have configuration extensions beyond those that are specified in the standard web.xml file.

Perhaps "open, extensible standards supporting graceful degradation" sums up my thoughts. I argue "open", in part on religious reasons, but more because closed standards hinder innovation. Not just for static analysis, but for data, API and communication formats of many types.

The only question is if static analysis, in particular security-focused static analysis is at a point where it is mature enough to be standardized. I think the answer is unquestionably "yes", provided it is an open, extensible standard that supports graceful degradation.


Neil Smithline
OneStopAppSecurity.com
781-754-7628
Chat [http://www.images.wisestamp.com/gtalk.png] Google Talk: neil_smithline at gmail.com<mailto:neil_smithline at gmail.com> [http://www.images.wisestamp.com/yahoo.png] Y! messenger: smithln
Contact Me [http://www.images.wisestamp.com/linkedin.png] <http://www.linkedin.com/profile?viewProfile=&key=2519386&trk=tab_pro>  [http://www.images.wisestamp.com/facebook.png] <http://www.facebook.com/home.php?#/profile.php?ref=profile&id=546173087>  [http://www.images.wisestamp.com/twitter.png] <http://twitter.com/emandab1>


--- @ WiseStamp Signature<http://www.wisestamp.com/email-install>. Get it now<http://www.wisestamp.com/email-install>


On Mon, Nov 23, 2009 at 15:57, Scott Matsumoto <smatsumoto at cigital.com<mailto:smatsumoto at cigital.com>> wrote:
I don't know the fate of O2, so I cannot comment on it.

SAFES is an emerging standard to standardize tool findings, so, there's value in standardizing them.  I think it's very early for SAFES.

________________________________
From: james at architectbook.com<mailto:james at architectbook.com> [james at architectbook.com<mailto:james at architectbook.com>]
Sent: Monday, November 23, 2009 3:45 PM
To: Scott Matsumoto
Cc: owasp-boston at lists.owasp.org<mailto:owasp-boston at lists.owasp.org>
Subject: RE: [Owasp-boston] 2 Questions about Jim's Presentation last week

OWASP Board member Dinis Cruz frequently discusses the O2 Platform. I am curious if anyone knows whether IBM will step up and rally behind it or let it die on the grapevine? Also would love to know if there is merit in having Appscan, WebInspect and other dynamic analysis tools emit a findings file in a standard format.

-------- Original Message --------
Subject: Re: [Owasp-boston] 2 Questions about Jim's Presentation last
week
From: Scott Matsumoto <smatsumoto at cigital.com<mailto:smatsumoto at cigital.com>>
Date: Mon, November 23, 2009 9:59 am
To: "Laverty, Patrick" <Patrick_Laverty at brown.edu<mailto:Patrick_Laverty at brown.edu>>,
"owasp-boston at lists.owasp.org<mailto:owasp-boston at lists.owasp.org>" <owasp-boston at lists.owasp.org<mailto:owasp-boston at lists.owasp.org>>

Patrick,

In terms of your first question, I can answer from our experience doing assessments that mix manual and tools for both static and dynamic analysis that those percentages are roughly right. I don't have quantitative numbers to back up that claim since our analysis isn't based solely on raw numbers.

In terms of employing multiple scanners, we use both AppScan and HP (SPI) and I don't see enough difference in coverage to run both. I think that using different tools that are looking for different types of problems or using a mix of static and dynamic tools provides enough of a win to justify the extra cost (both dollar and human).

Using this mix of static and dynamic as well as manual and tool-based techniques is exactly what we do. In the end, however, I find the vulnerabilities that have the highest business-related impact are those that we find manually. I think it's because many of the defects that the tools miss involve information disclosure and the tools don't have enough intelligence to distinguish what data one is should or should not see.

________________________________
From: owasp-boston-bounces at lists.owasp.org<mailto:owasp-boston-bounces at lists.owasp.org> [owasp-boston-bounces at lists.owasp.org<mailto:owasp-boston-bounces at lists.owasp.org>] On Behalf Of Laverty, Patrick [Patrick_Laverty at brown.edu<mailto:Patrick_Laverty at brown.edu>]
Sent: Monday, November 23, 2009 8:34 AM
To: owasp-boston at lists.owasp.org<mailto:owasp-boston at lists.owasp.org>
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week

And they?re both about the same statement.

Jim stated that the scanners will ?Only going to find 10 ? 20% of vulns ? low hanging fruit?

My two questions are:

1. How do we know that they will only find that number? If we know they?re missing 80-90%, how did we find them to count those that were missing?

2. I?ve read that the most effective and thorough scanning is to get 4-5 different scanners and use them all, as they will find some different vulnerabilities. Has anyone done this? What did you find when you look at all the data from multiple scanners and how varied are they at finding different true positive results?

Thank you!

Patrick



_______________________________________________
Owasp-boston mailing list
Owasp-boston at lists.owasp.org<mailto:Owasp-boston at lists.owasp.org>
https://lists.owasp.org/mailman/listinfo/owasp-boston
_______________________________________________
Owasp-boston mailing list
Owasp-boston at lists.owasp.org<mailto:Owasp-boston at lists.owasp.org>
https://lists.owasp.org/mailman/listinfo/owasp-boston



From hallam at gmail.com  Mon Nov 23 17:22:00 2009
From: hallam at gmail.com (Phillip Hallam-Baker)
Date: Mon, 23 Nov 2009 17:22:00 -0500
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week
In-Reply-To: <DE3BF7D023B1FB44A390EABB73048FF5021BBBCE@HQ-MAIL3.ptcnet.ptc.com>
References: <09DED73804A95B42ACFEFF57AEA3B24E071F9850@MAIL3.AD.Brown.Edu>
	<0D8299A04960AE4C8906052835761AF103C6748B@STCEXCPMB02.corp.star>
	<5A9D3476391AD649B24ED4F9FDA2A8DD0219CBD812@orbital.Veracode.local>
	<0D8299A04960AE4C8906052835761AF103C67511@STCEXCPMB02.corp.star>
	<DE3BF7D023B1FB44A390EABB73048FF5021BBBCE@HQ-MAIL3.ptcnet.ptc.com>
Message-ID: <a123a5d60911231422u46429b92l2f3176b04aa7626c@mail.gmail.com>

10% may be enough, it all depends on what you are trying to do with the scanner.

First off, what types of vulnerability can you expect a scanner to
detect? By definition, a scanner is only going to detect known
vulnerabilities since if the scanner can detect the problem it becomes
a known problem. So the question is what percentage of known issues do
the scanners detect? What percentage of vulnerabilities are part of
their corpus?

I am much more interested to know how the scanner is being used, is it
being used as the primary defense against vulnerabilities or is it
being used to measure the effectiveness of procedural controls?

For example, lets imagine that Alice and Bob are using a scanner.
Alice slaps her Web site together using tools she found on the net
written by a group of Russian programmers out in St Petersburg. She
runs the scanner over the site until she has identified and closed all
the vulnerabilities.

Bob starts off by choosing Web site tools that have a reputation for
being solid. He writes a coding standards guide and institutes a code
review process so that no code written by his team goes onto the site
without being seen by at least two pairs of eyes. He makes sure that
all his systems are patched before he runs the scanner. He then
reviews each item identified by the scanner and considers why the
procedural controls failed. He then multiplies the number of
vulnerabilities discovered by ten to provide an estimate of the number
of residual vulnerabilities.


Clearly Bob is performing a lot more work up front, but can expect to
be rewarded by a site that requires a lot less effort to maintain,
besides being a lot more secure.

About ten years ago, I developed a security assessment methodology
called ART (Assets, Risks, Threats) which I have used to analyze the
security of protocols such as SAML during design but most people
considered an infeasible amount of effort to go to in order to secure
an instance of a system. I have spent some time today looking through
the NIST FISMA guidelines and, well they have a six stage process
instead of three and their manual is going to be about the size of the
health care bill before they are finished.

Scanners are useful, but what we should be using them for is to
demonstrate that certain approaches to Web site development have more
hidden costs than others.

From ceng at Veracode.com  Mon Nov 23 17:36:47 2009
From: ceng at Veracode.com (Chris Eng)
Date: Mon, 23 Nov 2009 17:36:47 -0500
Subject: [Owasp-boston] 2 Questions about Jim's Presentation last week
In-Reply-To: <0D8299A04960AE4C8906052835761AF103C67511@STCEXCPMB02.corp.star>
References: <09DED73804A95B42ACFEFF57AEA3B24E071F9850@MAIL3.AD.Brown.Edu><0D8299A04960AE4C8906052835761AF103C6748B@STCEXCPMB02.corp.star>
	<5A9D3476391AD649B24ED4F9FDA2A8DD0219CBD812@orbital.Veracode.local>
	<0D8299A04960AE4C8906052835761AF103C67511@STCEXCPMB02.corp.star>
Message-ID: <5A9D3476391AD649B24ED4F9FDA2A8DD0219CBD985@orbital.Veracode.local>

> Chris, what do you say to customers when they ask things like (I'm
> assuming some of them ask this) 'what % of the vulnerabilities will your
> (or some other) application vulnerability scanner find?'.

I don't like to give a number -- not even a ballpark -- because it's so likely to be misinterpreted.  Instead, I'll talk about the OWASP Top Ten or the CWE/SANS Top 25, and discuss our coverage in each area.  It varies by customer depending on where their pain points are, which regulations they are being measured against, etc.

I'll also give examples of vulnerabilities that can only be found by a human, to be sure that whoever is asking the question has reasonable expectations.  If we're talking about static analysis, I'll talk about types of flaws that can only be detected via static analysis and the known tradeoffs (better coverage but more noise).  I try not to overstate what any type of automation can do, but rather describe how each type of analysis fits into a secure development process.





From ianevans at comcast.net  Mon Nov 23 18:39:23 2009
From: ianevans at comcast.net (Ian Evans)
Date: Mon, 23 Nov 2009 18:39:23 -0500
Subject: [Owasp-boston] unsubscribe
Message-ID: <B6C633A5-67E6-4D55-9F5D-8863F5F429E4@comcast.net>

unsubscribe

From Jim.Weiler at starwoodhotels.com  Mon Nov 30 10:50:17 2009
From: Jim.Weiler at starwoodhotels.com (Weiler, Jim)
Date: Mon, 30 Nov 2009 10:50:17 -0500
Subject: [Owasp-boston] Boston OWASP - No mtg this week
Message-ID: <0D8299A04960AE4C8906052835761AF103CE6F4B@STCEXCPMB02.corp.star>

We may have a speaker on the 17th - waiting to see what their final
travel schedule is.

 

Jim Weiler   CISSP  CSSLP

Starwood Hotels and Resorts

Sr. Mgr. Information Security Risk Assessment

Office - 781 356 0067

Cell - 781 654 6048



This electronic message transmission contains information from the Company that may be proprietary, confidential and/or privileged. 
The information is intended only for the use of the individual(s) or entity named above.  If you are not the intended recipient, be 
aware that any disclosure, copying or distribution or use of the contents of this information is prohibited.  If you have received 
this electronic transmission in error, please notify the sender immediately by replying to the address listed in the "From:" field. 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://lists.owasp.org/pipermail/owasp-boston/attachments/20091130/b3063f72/attachment.html 

