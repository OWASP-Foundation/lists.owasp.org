From justin.ferguson at owasp.org  Mon Oct 23 21:16:57 2017
From: justin.ferguson at owasp.org (Justin Ferguson)
Date: Mon, 23 Oct 2017 21:16:57 +0000
Subject: [Owasp-kansascity] NO MEETING - OCTOBER 2016
Message-ID: <CAD6LLGivD=j1U5d4Sbfn_TmQULh2VwwjMbQL8DTJBJkVwtVXdg@mail.gmail.com>

All - There will not be an OWASP Meeting for October.  Apologies as usual
for the late notice.  I've been trying to work around some travel for the
new job, and things just didn't work out.

We have some exciting things planned for upcoming meeting, and I'm trying
to line up a new meeting day earlier in the month, so that we can
potentially meet in November and December without things being right up
against holidays.  Expect a more detailed announcement in the very near
future!

JF
-- 
-- 
Justin Ferguson
OWASP-Kansas City Chapter Leader
justin.ferguson at owasp.org
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.owasp.org/pipermail/owasp-kansascity/attachments/20171023/f80d36d2/attachment.html>

From caughron at gmail.com  Mon Oct 30 19:01:48 2017
From: caughron at gmail.com (Mat Caughron)
Date: Mon, 30 Oct 2017 19:01:48 +0000
Subject: [Owasp-kansascity] code review metrics
Message-ID: <CAN4hdVXxH171i+=VPmwc6f=0e9zKqoUCXcsKpExbcfvvaScOag@mail.gmail.com>

Greetings KC chapter,

I have been comparing static analysis software and found the following to
be insightful, so passing it along...

https://www.owasp.org/index.php/Code_Review_Metrics



Mat



Secure Development MetricsDefect Density

The average occurrence of programming faults per Lines of Code (LOC). This
gives a high level view of the code quality but not much more. Fault
density on its own does not give rise to a pragmatic metric. Defect density
would cover minor issues as well as major security flaws in the code; all
are treated the same way. Security of code can not be judged accurately
using defect density alone.
Lines of Code (LOC)

The count of the executable lines of code. Commented-out code or spaces
don't count. This is another metric in an attempt to quantify the size of
the code. This gives a rough estimate but is not particularly scientific.
Some circles of thinking believe that the estimation of an application size
by virtue of LOC is professional malpractice!
Function Point

The estimation of software size by measuring functionality. The combination
of a number of statements which perform a specific task, independent of
programming language used or development methodology.
Risk Density

Similar to defect density, but discovered issues are rated by risk (high,
medium & low). In doing this we can give insight into the quality of the
code being developed via a [*X Risk / LoC*] or [*Y Risk / Function Point*]
value. (X&Y being high, medium or low risks) as defined by your internal
application development policies and standards.

Example:
4 High Risk Defects per 1000 (Lines of Code)
2 Medium Risk Defects per 3 Function Points
Path complexity/complexity-to-defect/cyclomatic complexity

Cyclomatic complexity can help establish risk and stability estimations on
an item of code, such as a class or method or even a complete system. It
was defined by Thomas McCabe in the 70's and it easy to calsulate and
apply, hence its usefulness.

CC = Number of decisions +1

A decision could be considered commands such as:

If....else
switch
case
catch
while
do
and so on.....

As the decision count increases, so does the complexity. Complex code leads
to less stability and maintainability.

The more complex the code, the higher risk of defects. One could establish
thresholds for Cyclomatic complexity:

0-10: Stable code. Acceptable complexity
11-15: Medium Risk. More complex
16-20: High Risk code. Too many decisions for a unit of code.
Review Process MetricsInspection Rate

This metric can be used to get a rough idea of the required duration to
perform a code review. The inspection rate is the rate of coverage a code
reviewer can cover per unit of time. From experience, a rate of 250 lines
per hour would be a baseline. This rate should not be used as part of a
measure of review quality, but simply to determine duration of the task.
Defect Detection Rate

This metric measures the defects found per unit of time. Again, can be used
to measure performance of the code review team, but not to be used as a
quality measure. Defect detection rate would normally increase as the
inspection rate (above) decreases.
Code Coverage

Measured as a % of LoC of function points, the code coverage is the
proportion of the code reviewed. In the case of manual review we would aim
for close to 100%, unlike automated testing wherein 80-90% is considered
good. In order to ensure that the code coverage standards are met, some
organizations might implement a safety check during the build process, so
the build will fail if there are any piece of code that has not been tested
or if it the coverage is under the desired percentage. The higher the
percentage of code coverage, the better to ensure quality and prevent logic
errors.
Defect Correction Rate

The amount of time used to correct detected defects. This metric can be
used to optimise a project plan within the SDLC. Average values can be
measured over time, producing a measure of effort which must be taken into
account in the planning phase.
Reinspection Defect Rate

The rate at which upon re-inspection of the code more defects exist, some
defects still exist, or other defects manifest through an attempt to
address previously discovered defects.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.owasp.org/pipermail/owasp-kansascity/attachments/20171030/aff8898f/attachment.html>

